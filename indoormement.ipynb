{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "c:\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./MovementAAL/dataset/MovementAAL_RSS_1.csv')\n",
    "df2 = pd.read_csv('./MovementAAL/dataset/MovementAAL_RSS_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.90476</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.28571</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.38095</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.14286</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.28571</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.47619</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.90476         -0.48       0.28571          0.30\n",
       "1      -0.57143         -0.32       0.14286          0.30\n",
       "2      -0.38095         -0.28      -0.14286          0.35\n",
       "3      -0.28571         -0.20      -0.47619          0.35\n",
       "4      -0.14286         -0.20       0.14286         -0.20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.76190</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.85714</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.57143         -0.20       0.71429          0.50\n",
       "1      -0.76190         -0.48       0.76190         -0.25\n",
       "2      -0.85714         -0.60       0.85714          0.55\n",
       "3      -0.76190         -0.40       0.71429          0.60\n",
       "4      -0.76190         -0.84       0.85714          0.45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MovementAAL/dataset/MovementAAL_RSS_1.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_2.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_3.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_4.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_5.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_6.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_7.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_8.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_9.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_10.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_11.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_12.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_13.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_14.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_15.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_16.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_17.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_18.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_19.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_20.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_21.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_22.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_23.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_24.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_25.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_26.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_27.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_28.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_29.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_30.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_31.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_32.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_33.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_34.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_35.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_36.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_37.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_38.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_39.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_40.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_41.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_42.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_43.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_44.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_45.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_46.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_47.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_48.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_49.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_50.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_51.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_52.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_53.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_54.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_55.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_56.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_57.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_58.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_59.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_60.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_61.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_62.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_63.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_64.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_65.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_66.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_67.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_68.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_69.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_70.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_71.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_72.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_73.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_74.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_75.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_76.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_77.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_78.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_79.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_80.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_81.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_82.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_83.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_84.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_85.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_86.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_87.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_88.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_89.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_90.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_91.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_92.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_93.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_94.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_95.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_96.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_97.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_98.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_99.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_100.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_101.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_102.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_103.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_104.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_105.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_106.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_107.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_108.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_109.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_110.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_111.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_112.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_113.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_114.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_115.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_116.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_117.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_118.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_119.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_120.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_121.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_122.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_123.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_124.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_125.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_126.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_127.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_128.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_129.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_130.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_131.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_132.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_133.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_134.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_135.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_136.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_137.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_138.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_139.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_140.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_141.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_142.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_143.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_144.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_145.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_146.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_147.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_148.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_149.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_150.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_151.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_152.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_153.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_154.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_155.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_156.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_157.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_158.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_159.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_160.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_161.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_162.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_163.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_164.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_165.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_166.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_167.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_168.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_169.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_170.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_171.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_172.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_173.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_174.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_175.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_176.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_177.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_178.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_179.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_180.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_181.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_182.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_183.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_184.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_185.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_186.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_187.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_188.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_189.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_190.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_191.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_192.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_193.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_194.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_195.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_196.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_197.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_198.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_199.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_200.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_201.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_202.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_203.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_204.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_205.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_206.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_207.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_208.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_209.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_210.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_211.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_212.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_213.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_214.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_215.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_216.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_217.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_218.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_219.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_220.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_221.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_222.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_223.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MovementAAL/dataset/MovementAAL_RSS_224.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_225.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_226.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_227.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_228.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_229.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_230.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_231.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_232.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_233.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_234.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_235.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_236.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_237.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_238.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_239.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_240.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_241.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_242.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_243.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_244.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_245.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_246.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_247.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_248.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_249.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_250.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_251.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_252.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_253.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_254.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_255.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_256.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_257.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_258.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_259.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_260.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_261.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_262.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_263.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_264.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_265.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_266.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_267.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_268.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_269.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_270.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_271.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_272.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_273.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_274.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_275.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_276.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_277.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_278.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_279.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_280.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_281.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_282.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_283.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_284.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_285.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_286.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_287.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_288.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_289.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_290.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_291.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_292.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_293.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_294.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_295.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_296.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_297.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_298.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_299.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_300.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_301.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_302.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_303.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_304.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_305.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_306.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_307.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_308.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_309.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_310.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_311.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_312.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_313.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_314.csv\n"
     ]
    }
   ],
   "source": [
    "path = './MovementAAL/dataset/MovementAAL_RSS_'\n",
    "sequences = list()\n",
    "for i in range(1,315):\n",
    "    file_path = path + str(i) + '.csv'\n",
    "    print(file_path)\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    values = df.values\n",
    "    sequences.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('./MovementAAL/dataset/MovementAAL_target.csv')\n",
    "targets = targets.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.72727 , -1.      , -0.066667,  0.06383 ],\n",
       "       [-0.45455 , -0.6     ,  0.066667,  0.06383 ],\n",
       "       [-0.27273 , -0.77778 , -0.11111 ,  0.06383 ],\n",
       "       [-0.36364 , -0.82222 , -0.86667 ,  0.06383 ],\n",
       "       [-0.36364 , -0.73333 ,  0.33333 ,  0.06383 ],\n",
       "       [-0.36364 , -0.33333 ,  0.066667,  0.53191 ],\n",
       "       [-0.36364 , -0.6     ,  0.2     ,  0.65957 ],\n",
       "       [-0.36364 , -0.46667 , -0.022222,  0.61702 ],\n",
       "       [-0.36364 , -0.6     ,  0.15556 ,  0.65957 ],\n",
       "       [-0.36364 , -0.82222 ,  0.33333 ,  0.53191 ],\n",
       "       [-0.68182 , -0.33333 ,  0.33333 ,  0.65957 ],\n",
       "       [-0.68182 , -0.24444 ,  0.066667,  0.65957 ],\n",
       "       [-0.68182 , -0.6     ,  0.33333 ,  0.40426 ],\n",
       "       [-0.68182 , -0.46667 ,  0.33333 ,  0.65957 ],\n",
       "       [-0.68182 , -0.42222 ,  0.33333 ,  0.65957 ],\n",
       "       [-0.81818 , -0.37778 , -0.15556 ,  0.74468 ],\n",
       "       [-0.81818 , -0.55556 ,  0.46667 ,  0.74468 ],\n",
       "       [-0.63636 , -0.68889 ,  0.64444 ,  0.57447 ],\n",
       "       [-0.63636 , -1.      ,  0.33333 ,  0.57447 ],\n",
       "       [-0.59091 , -0.46667 ,  0.68889 ,  0.65957 ],\n",
       "       [-0.59091 , -0.55556 ,  0.2     ,  0.65957 ],\n",
       "       [-0.45455 , -0.95556 ,  0.64444 ,  0.61702 ],\n",
       "       [-0.45455 , -0.95556 ,  0.33333 ,  0.61702 ],\n",
       "       [-0.72727 , -0.51111 ,  0.77778 ,  0.61702 ],\n",
       "       [-0.72727 , -0.46667 , -0.2     ,  0.61702 ],\n",
       "       [-0.72727 , -0.46667 ,  0.46667 ,  0.40426 ],\n",
       "       [-0.22727 , -0.46667 ,  0.33333 ,  0.78723 ],\n",
       "       [-0.27273 , -0.46667 ,  0.022222,  0.78723 ],\n",
       "       [-0.31818 , -0.82222 ,  0.46667 ,  0.78723 ],\n",
       "       [-0.13636 , -0.64444 ,  0.46667 ,  0.78723 ],\n",
       "       [-0.13636 , -0.68889 ,  0.33333 ,  0.78723 ],\n",
       "       [-0.13636 , -0.6     ,  0.46667 ,  0.74468 ],\n",
       "       [-0.13636 , -0.37778 ,  0.64444 ,  0.74468 ],\n",
       "       [-0.27273 , -0.33333 ,  0.37778 ,  0.65957 ],\n",
       "       [-0.18182 , -0.33333 ,  0.64444 ,  0.65957 ],\n",
       "       [-0.090909, -0.46667 ,  0.68889 ,  0.65957 ],\n",
       "       [-0.13636 , -0.46667 ,  0.066667,  0.65957 ],\n",
       "       [-0.31818 , -0.55556 ,  0.64444 ,  0.65957 ],\n",
       "       [-0.31818 , -0.46667 ,  0.33333 ,  0.65957 ],\n",
       "       [-0.22727 , -0.6     ,  0.46667 ,  1.      ],\n",
       "       [-0.5     , -0.6     ,  0.73333 ,  1.      ],\n",
       "       [-0.5     , -0.46667 ,  0.73333 ,  1.      ],\n",
       "       [-0.5     , -0.42222 ,  0.73333 ,  0.3617  ],\n",
       "       [-0.40909 , -0.33333 ,  0.55556 , -0.31915 ],\n",
       "       [-0.45455 , -0.46667 ,  0.46667 , -0.31915 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sequences[313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = pd.read_csv('./MovementAAL/groups/MovementAAL_DatasetGroup.csv', header=0)\n",
    "groups = groups.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    314.000000\n",
       "mean      42.028662\n",
       "std       16.185303\n",
       "min       19.000000\n",
       "25%       26.000000\n",
       "50%       41.000000\n",
       "75%       56.000000\n",
       "max      129.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sequences = []\n",
    "for one_seq in sequences:\n",
    "    len_sequences.append(len(one_seq))\n",
    "pd.Series(len_sequences).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the sequence with the values in last row to max length\n",
    "to_pad = 129\n",
    "new_seq = []\n",
    "for one_seq in sequences:\n",
    "    len_one_seq = len(one_seq)\n",
    "    last_val = one_seq[-1]\n",
    "    n = to_pad - len_one_seq\n",
    "   \n",
    "    to_concat = np.repeat(one_seq[-1], n).reshape(4, n).transpose()\n",
    "    new_one_seq = np.concatenate([one_seq, to_concat])\n",
    "    new_seq.append(new_one_seq)\n",
    "final_seq = np.stack(new_seq)\n",
    "\n",
    "#truncate the sequence to length 60\n",
    "from keras.preprocessing import sequence\n",
    "seq_len = 60\n",
    "final_seq=sequence.pad_sequences(final_seq, maxlen=seq_len, padding='post', dtype='float', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 60, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90476 , -0.48    ,  0.28571 ,  0.3     ],\n",
       "       [-0.57143 , -0.32    ,  0.14286 ,  0.3     ],\n",
       "       [-0.38095 , -0.28    , -0.14286 ,  0.35    ],\n",
       "       [-0.28571 , -0.2     , -0.47619 ,  0.35    ],\n",
       "       [-0.14286 , -0.2     ,  0.14286 , -0.2     ],\n",
       "       [-0.14286 , -0.2     ,  0.047619,  0.      ],\n",
       "       [-0.14286 , -0.16    , -0.38095 ,  0.2     ],\n",
       "       [-0.14286 , -0.04    , -0.61905 , -0.2     ],\n",
       "       [-0.095238, -0.08    ,  0.14286 , -0.55    ],\n",
       "       [-0.047619,  0.04    , -0.095238,  0.05    ],\n",
       "       [-0.19048 , -0.04    ,  0.095238,  0.4     ],\n",
       "       [-0.095238, -0.04    , -0.14286 ,  0.35    ],\n",
       "       [-0.33333 , -0.08    , -0.28571 , -0.2     ],\n",
       "       [-0.2381  ,  0.04    ,  0.14286 ,  0.35    ],\n",
       "       [ 0.      ,  0.08    ,  0.14286 ,  0.05    ],\n",
       "       [-0.095238,  0.04    ,  0.095238,  0.1     ],\n",
       "       [-0.14286 , -0.2     ,  0.14286 ,  0.5     ],\n",
       "       [-0.19048 ,  0.04    , -0.42857 ,  0.3     ],\n",
       "       [-0.14286 , -0.08    , -0.2381  ,  0.15    ],\n",
       "       [-0.33333 ,  0.16    , -0.14286 , -0.8     ],\n",
       "       [-0.42857 ,  0.16    , -0.28571 , -0.1     ],\n",
       "       [-0.71429 ,  0.16    , -0.28571 ,  0.2     ],\n",
       "       [-0.095238, -0.08    ,  0.095238,  0.35    ],\n",
       "       [-0.28571 ,  0.04    ,  0.14286 ,  0.2     ],\n",
       "       [ 0.      ,  0.04    ,  0.14286 ,  0.1     ],\n",
       "       [ 0.      ,  0.04    , -0.047619, -0.05    ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [final_seq[i] for i in range(len(groups)) if (groups[i]==2)]\n",
    "validation = [final_seq[i] for i in range(len(groups)) if groups[i]==1]\n",
    "test = [final_seq[i] for i in range(len(groups)) if groups[i]==3]\n",
    "\n",
    "train_target = [targets[i] for i in range(len(groups)) if (groups[i]==2)]\n",
    "validation_target = [targets[i] for i in range(len(groups)) if groups[i]==1]\n",
    "test_target = [targets[i] for i in range(len(groups)) if groups[i]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "validation = np.array(validation)\n",
    "test = np.array(test)\n",
    "\n",
    "train_target = np.array(train_target)\n",
    "train_target = (train_target+1)/2\n",
    "\n",
    "validation_target = np.array(validation_target)\n",
    "validation_target = (validation_target+1)/2\n",
    "\n",
    "test_target = np.array(test_target)\n",
    "test_target = (test_target+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 60, 4), (104, 60, 4), (104, 60, 4))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY1: Building time series classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len, 4)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               267264    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 267,521\n",
      "Trainable params: 267,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106 samples, validate on 104 samples\n",
      "Epoch 1/200\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7073 - accuracy: 0.4717 - val_loss: 0.7106 - val_accuracy: 0.4519\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.45192, saving model to best_model.pkl\n",
      "Epoch 2/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7942 - accuracy: 0.5849 - val_loss: 1.1361 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.45192 to 0.49038, saving model to best_model.pkl\n",
      "Epoch 3/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8665 - accuracy: 0.5000 - val_loss: 0.8760 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.49038 to 0.50000, saving model to best_model.pkl\n",
      "Epoch 4/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7810 - accuracy: 0.5566 - val_loss: 0.6699 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.50000 to 0.54808, saving model to best_model.pkl\n",
      "Epoch 5/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6905 - accuracy: 0.5566 - val_loss: 0.7151 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.54808 to 0.55769, saving model to best_model.pkl\n",
      "Epoch 6/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6745 - accuracy: 0.5566 - val_loss: 0.6899 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.55769\n",
      "Epoch 7/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.6631 - accuracy: 0.6509 - val_loss: 0.7301 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.55769\n",
      "Epoch 8/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.7028 - accuracy: 0.5755 - val_loss: 0.7210 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.55769\n",
      "Epoch 9/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6842 - accuracy: 0.5283 - val_loss: 0.7053 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.55769\n",
      "Epoch 10/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5189 - val_loss: 0.7561 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.55769\n",
      "Epoch 11/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7933 - accuracy: 0.3679 - val_loss: 0.7170 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.55769\n",
      "Epoch 12/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7145 - accuracy: 0.4528 - val_loss: 0.7273 - val_accuracy: 0.5096\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.55769\n",
      "Epoch 13/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6713 - accuracy: 0.6415 - val_loss: 0.7758 - val_accuracy: 0.4615\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.55769\n",
      "Epoch 14/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6675 - accuracy: 0.5755 - val_loss: 0.7943 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.55769\n",
      "Epoch 15/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.6914 - accuracy: 0.5566 - val_loss: 0.7499 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.55769\n",
      "Epoch 16/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6374 - accuracy: 0.5755 - val_loss: 0.7440 - val_accuracy: 0.4615\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.55769\n",
      "Epoch 17/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6295 - accuracy: 0.5943 - val_loss: 0.7343 - val_accuracy: 0.5096\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.55769\n",
      "Epoch 18/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6582 - accuracy: 0.5377 - val_loss: 0.7745 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.55769\n",
      "Epoch 19/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5920 - accuracy: 0.7075 - val_loss: 0.7936 - val_accuracy: 0.4712\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.55769\n",
      "Epoch 20/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.5517 - accuracy: 0.7358 - val_loss: 0.7733 - val_accuracy: 0.4808\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.55769\n",
      "Epoch 21/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5350 - accuracy: 0.7264 - val_loss: 0.7552 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.55769\n",
      "Epoch 22/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5330 - accuracy: 0.7453 - val_loss: 0.7313 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.55769\n",
      "Epoch 23/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.5260 - accuracy: 0.7358 - val_loss: 0.8156 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.55769\n",
      "Epoch 24/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.5022 - accuracy: 0.7358 - val_loss: 0.8823 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.55769\n",
      "Epoch 25/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.4837 - accuracy: 0.7453 - val_loss: 0.9500 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.55769\n",
      "Epoch 26/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4806 - accuracy: 0.7453 - val_loss: 0.9196 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.55769\n",
      "Epoch 27/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5007 - accuracy: 0.7170 - val_loss: 0.9207 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.55769\n",
      "Epoch 28/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4748 - accuracy: 0.7736 - val_loss: 0.9119 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.55769\n",
      "Epoch 29/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4678 - accuracy: 0.7830 - val_loss: 0.9634 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.55769\n",
      "Epoch 30/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4239 - accuracy: 0.7830 - val_loss: 0.9090 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.55769\n",
      "Epoch 31/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4297 - accuracy: 0.8019 - val_loss: 0.9405 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.55769\n",
      "Epoch 32/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4267 - accuracy: 0.8491 - val_loss: 1.0060 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.55769\n",
      "Epoch 33/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4156 - accuracy: 0.8396 - val_loss: 1.0477 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.55769\n",
      "Epoch 34/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2957 - accuracy: 0.9057 - val_loss: 0.9697 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.55769\n",
      "Epoch 35/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3247 - accuracy: 0.8774 - val_loss: 1.1768 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.55769\n",
      "Epoch 36/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3622 - accuracy: 0.8491 - val_loss: 1.4480 - val_accuracy: 0.4808\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.55769\n",
      "Epoch 37/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.3517 - accuracy: 0.8679 - val_loss: 1.1332 - val_accuracy: 0.5096\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.55769\n",
      "Epoch 38/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.3420 - accuracy: 0.8491 - val_loss: 0.9977 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00038: val_accuracy improved from 0.55769 to 0.56731, saving model to best_model.pkl\n",
      "Epoch 39/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.3351 - accuracy: 0.8679 - val_loss: 1.0909 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.56731\n",
      "Epoch 40/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3440 - accuracy: 0.8679 - val_loss: 1.1305 - val_accuracy: 0.5481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.56731\n",
      "Epoch 41/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3335 - accuracy: 0.8679 - val_loss: 1.3201 - val_accuracy: 0.4808\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.56731\n",
      "Epoch 42/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.3469 - accuracy: 0.8585 - val_loss: 1.1991 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.56731\n",
      "Epoch 43/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3808 - accuracy: 0.8491 - val_loss: 1.2019 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.56731\n",
      "Epoch 44/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3607 - accuracy: 0.8491 - val_loss: 1.4772 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.56731\n",
      "Epoch 45/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3860 - accuracy: 0.8302 - val_loss: 1.5124 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.56731\n",
      "Epoch 46/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.3736 - accuracy: 0.8396 - val_loss: 0.9919 - val_accuracy: 0.5288\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.56731\n",
      "Epoch 47/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.3334 - accuracy: 0.8491 - val_loss: 0.8464 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.56731\n",
      "Epoch 48/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.3621 - accuracy: 0.8302 - val_loss: 1.0105 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.56731\n",
      "Epoch 49/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3311 - accuracy: 0.8396 - val_loss: 1.2564 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.56731\n",
      "Epoch 50/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3284 - accuracy: 0.8585 - val_loss: 1.1369 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.56731\n",
      "Epoch 51/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3016 - accuracy: 0.8774 - val_loss: 1.0492 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.56731\n",
      "Epoch 52/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3050 - accuracy: 0.8585 - val_loss: 1.0999 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.56731\n",
      "Epoch 53/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.2852 - accuracy: 0.8774 - val_loss: 1.2498 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.56731\n",
      "Epoch 54/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3165 - accuracy: 0.8585 - val_loss: 1.1949 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.56731\n",
      "Epoch 55/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.2980 - accuracy: 0.8585 - val_loss: 1.3387 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.56731\n",
      "Epoch 56/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3166 - accuracy: 0.8491 - val_loss: 1.1426 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.56731\n",
      "Epoch 57/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.2917 - accuracy: 0.8774 - val_loss: 1.0023 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00057: val_accuracy improved from 0.56731 to 0.57692, saving model to best_model.pkl\n",
      "Epoch 58/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2862 - accuracy: 0.8774 - val_loss: 1.1398 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.57692\n",
      "Epoch 59/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.2776 - accuracy: 0.8491 - val_loss: 1.2275 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.57692\n",
      "Epoch 60/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3204 - accuracy: 0.8491 - val_loss: 1.0863 - val_accuracy: 0.5096\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.57692\n",
      "Epoch 61/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4616 - accuracy: 0.7830 - val_loss: 0.9399 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.57692\n",
      "Epoch 62/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5430 - accuracy: 0.6604 - val_loss: 0.7026 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00062: val_accuracy improved from 0.57692 to 0.62500, saving model to best_model.pkl\n",
      "Epoch 63/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5912 - accuracy: 0.6792 - val_loss: 0.7007 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.62500\n",
      "Epoch 64/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5957 - accuracy: 0.6226 - val_loss: 0.7492 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.62500\n",
      "Epoch 65/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.5833 - accuracy: 0.6887 - val_loss: 0.8611 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.62500\n",
      "Epoch 66/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5253 - accuracy: 0.7264 - val_loss: 0.8918 - val_accuracy: 0.4808\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.62500\n",
      "Epoch 67/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5076 - accuracy: 0.7358 - val_loss: 0.7752 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.62500\n",
      "Epoch 68/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4779 - accuracy: 0.7358 - val_loss: 0.8162 - val_accuracy: 0.4615\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.62500\n",
      "Epoch 69/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5288 - accuracy: 0.7642 - val_loss: 0.7586 - val_accuracy: 0.5288\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.62500\n",
      "Epoch 70/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5245 - accuracy: 0.7075 - val_loss: 0.8039 - val_accuracy: 0.5288\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.62500\n",
      "Epoch 71/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5381 - accuracy: 0.7358 - val_loss: 0.7934 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.62500\n",
      "Epoch 72/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5340 - accuracy: 0.7264 - val_loss: 0.7962 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.62500\n",
      "Epoch 73/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5415 - accuracy: 0.7642 - val_loss: 0.8063 - val_accuracy: 0.4519\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.62500\n",
      "Epoch 74/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5550 - accuracy: 0.7547 - val_loss: 0.8099 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.62500\n",
      "Epoch 75/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5413 - accuracy: 0.7736 - val_loss: 0.8198 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.62500\n",
      "Epoch 76/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5389 - accuracy: 0.7642 - val_loss: 0.8266 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.62500\n",
      "Epoch 77/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5348 - accuracy: 0.7736 - val_loss: 0.7994 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.62500\n",
      "Epoch 78/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5195 - accuracy: 0.7642 - val_loss: 0.8057 - val_accuracy: 0.5288\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.62500\n",
      "Epoch 79/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5141 - accuracy: 0.7547 - val_loss: 0.7964 - val_accuracy: 0.4712\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.62500\n",
      "Epoch 80/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.5157 - accuracy: 0.7547 - val_loss: 0.7584 - val_accuracy: 0.5096\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.62500\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5101 - accuracy: 0.7736 - val_loss: 0.7789 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.62500\n",
      "Epoch 82/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5043 - accuracy: 0.7642 - val_loss: 0.7860 - val_accuracy: 0.4808\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.62500\n",
      "Epoch 83/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5196 - accuracy: 0.7642 - val_loss: 0.8379 - val_accuracy: 0.5096\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.62500\n",
      "Epoch 84/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.5208 - accuracy: 0.7264 - val_loss: 0.7797 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.62500\n",
      "Epoch 85/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5106 - accuracy: 0.7830 - val_loss: 0.7628 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.62500\n",
      "Epoch 86/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5010 - accuracy: 0.7830 - val_loss: 0.8090 - val_accuracy: 0.5288\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.62500\n",
      "Epoch 87/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4868 - accuracy: 0.7830 - val_loss: 0.8402 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.62500\n",
      "Epoch 88/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4955 - accuracy: 0.7925 - val_loss: 0.8029 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.62500\n",
      "Epoch 89/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4697 - accuracy: 0.8019 - val_loss: 0.8048 - val_accuracy: 0.4808\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.62500\n",
      "Epoch 90/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.5013 - accuracy: 0.7642 - val_loss: 0.8002 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.62500\n",
      "Epoch 91/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4743 - accuracy: 0.7830 - val_loss: 0.7971 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.62500\n",
      "Epoch 92/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4892 - accuracy: 0.8302 - val_loss: 0.8179 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.62500\n",
      "Epoch 93/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4816 - accuracy: 0.8113 - val_loss: 0.8463 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.62500\n",
      "Epoch 94/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.4938 - accuracy: 0.7642 - val_loss: 0.8113 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.62500\n",
      "Epoch 95/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4718 - accuracy: 0.7925 - val_loss: 0.7808 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.62500\n",
      "Epoch 96/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4676 - accuracy: 0.8019 - val_loss: 0.7915 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.62500\n",
      "Epoch 97/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4545 - accuracy: 0.8113 - val_loss: 0.8279 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.62500\n",
      "Epoch 98/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4801 - accuracy: 0.8019 - val_loss: 0.7866 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.62500\n",
      "Epoch 99/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4846 - accuracy: 0.7830 - val_loss: 0.7956 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.62500\n",
      "Epoch 100/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4899 - accuracy: 0.7547 - val_loss: 0.7648 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.62500\n",
      "Epoch 101/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4644 - accuracy: 0.7830 - val_loss: 0.7701 - val_accuracy: 0.5962\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.62500\n",
      "Epoch 102/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4768 - accuracy: 0.8019 - val_loss: 0.7414 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.62500\n",
      "Epoch 103/200\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.4781 - accuracy: 0.8113 - val_loss: 0.7129 - val_accuracy: 0.6346\n",
      "\n",
      "Epoch 00103: val_accuracy improved from 0.62500 to 0.63462, saving model to best_model.pkl\n",
      "Epoch 104/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4551 - accuracy: 0.8302 - val_loss: 0.7637 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.63462\n",
      "Epoch 105/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4548 - accuracy: 0.8113 - val_loss: 0.8298 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.63462\n",
      "Epoch 106/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4569 - accuracy: 0.7736 - val_loss: 0.7982 - val_accuracy: 0.5288\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.63462\n",
      "Epoch 107/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4565 - accuracy: 0.7830 - val_loss: 0.7736 - val_accuracy: 0.6346\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.63462\n",
      "Epoch 108/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4437 - accuracy: 0.8491 - val_loss: 0.7831 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.63462\n",
      "Epoch 109/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4407 - accuracy: 0.8208 - val_loss: 0.7696 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.63462\n",
      "Epoch 110/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4459 - accuracy: 0.8113 - val_loss: 0.7551 - val_accuracy: 0.5962\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.63462\n",
      "Epoch 111/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4501 - accuracy: 0.8113 - val_loss: 0.7408 - val_accuracy: 0.6058\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.63462\n",
      "Epoch 112/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4485 - accuracy: 0.8208 - val_loss: 0.7474 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.63462\n",
      "Epoch 113/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4328 - accuracy: 0.8113 - val_loss: 0.8484 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.63462\n",
      "Epoch 114/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4401 - accuracy: 0.8019 - val_loss: 0.8778 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.63462\n",
      "Epoch 115/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4591 - accuracy: 0.7925 - val_loss: 0.8154 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.63462\n",
      "Epoch 116/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4269 - accuracy: 0.7925 - val_loss: 0.8932 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.63462\n",
      "Epoch 117/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4260 - accuracy: 0.8208 - val_loss: 0.9471 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.63462\n",
      "Epoch 118/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4231 - accuracy: 0.8491 - val_loss: 0.8518 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.63462\n",
      "Epoch 119/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4227 - accuracy: 0.7830 - val_loss: 0.8061 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.63462\n",
      "Epoch 120/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4129 - accuracy: 0.8302 - val_loss: 0.8614 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.63462\n",
      "Epoch 121/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4169 - accuracy: 0.8208 - val_loss: 0.8244 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.63462\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4106 - accuracy: 0.8491 - val_loss: 0.7940 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.63462\n",
      "Epoch 123/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4079 - accuracy: 0.8302 - val_loss: 0.8669 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.63462\n",
      "Epoch 124/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4080 - accuracy: 0.8302 - val_loss: 0.9977 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.63462\n",
      "Epoch 125/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4069 - accuracy: 0.8396 - val_loss: 0.9363 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.63462\n",
      "Epoch 126/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4253 - accuracy: 0.7925 - val_loss: 0.8643 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.63462\n",
      "Epoch 127/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4010 - accuracy: 0.8113 - val_loss: 0.8847 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.63462\n",
      "Epoch 128/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3995 - accuracy: 0.8491 - val_loss: 0.8491 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.63462\n",
      "Epoch 129/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4078 - accuracy: 0.8396 - val_loss: 0.8264 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.63462\n",
      "Epoch 130/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4207 - accuracy: 0.8113 - val_loss: 0.7900 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.63462\n",
      "Epoch 131/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4161 - accuracy: 0.8019 - val_loss: 0.8641 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.63462\n",
      "Epoch 132/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4091 - accuracy: 0.8113 - val_loss: 0.9289 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.63462\n",
      "Epoch 133/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4135 - accuracy: 0.8019 - val_loss: 0.8955 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.63462\n",
      "Epoch 134/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3984 - accuracy: 0.8396 - val_loss: 0.8467 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.63462\n",
      "Epoch 135/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3946 - accuracy: 0.8491 - val_loss: 0.8656 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.63462\n",
      "Epoch 136/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4055 - accuracy: 0.8302 - val_loss: 0.8741 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.63462\n",
      "Epoch 137/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4039 - accuracy: 0.8208 - val_loss: 0.9310 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.63462\n",
      "Epoch 138/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4001 - accuracy: 0.8396 - val_loss: 0.8814 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.63462\n",
      "Epoch 139/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4115 - accuracy: 0.8113 - val_loss: 0.8186 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.63462\n",
      "Epoch 140/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4049 - accuracy: 0.8302 - val_loss: 0.9004 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.63462\n",
      "Epoch 141/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3898 - accuracy: 0.8396 - val_loss: 0.8746 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.63462\n",
      "Epoch 142/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4088 - accuracy: 0.8113 - val_loss: 0.7839 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.63462\n",
      "Epoch 143/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3812 - accuracy: 0.8302 - val_loss: 0.8454 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.63462\n",
      "Epoch 144/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3905 - accuracy: 0.8019 - val_loss: 0.8657 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.63462\n",
      "Epoch 145/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3852 - accuracy: 0.8302 - val_loss: 0.8235 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.63462\n",
      "Epoch 146/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3844 - accuracy: 0.8302 - val_loss: 0.7781 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.63462\n",
      "Epoch 147/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3775 - accuracy: 0.8491 - val_loss: 0.7961 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.63462\n",
      "Epoch 148/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3715 - accuracy: 0.8585 - val_loss: 0.8608 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.63462\n",
      "Epoch 149/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3692 - accuracy: 0.8396 - val_loss: 0.9341 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.63462\n",
      "Epoch 150/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3780 - accuracy: 0.8302 - val_loss: 0.9349 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.63462\n",
      "Epoch 151/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3753 - accuracy: 0.8396 - val_loss: 0.8332 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.63462\n",
      "Epoch 152/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3611 - accuracy: 0.8491 - val_loss: 0.7916 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.63462\n",
      "Epoch 153/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3615 - accuracy: 0.8585 - val_loss: 0.8893 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.63462\n",
      "Epoch 154/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3742 - accuracy: 0.8208 - val_loss: 0.8569 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.63462\n",
      "Epoch 155/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3559 - accuracy: 0.8774 - val_loss: 0.8153 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.63462\n",
      "Epoch 156/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3461 - accuracy: 0.8585 - val_loss: 0.8305 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.63462\n",
      "Epoch 157/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3667 - accuracy: 0.8774 - val_loss: 0.7772 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.63462\n",
      "Epoch 158/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3416 - accuracy: 0.8774 - val_loss: 0.7508 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.63462\n",
      "Epoch 159/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3680 - accuracy: 0.8585 - val_loss: 0.9057 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.63462\n",
      "Epoch 160/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4016 - accuracy: 0.8208 - val_loss: 0.7789 - val_accuracy: 0.5962\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.63462\n",
      "Epoch 161/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3922 - accuracy: 0.8396 - val_loss: 0.7344 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.63462\n",
      "Epoch 162/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3807 - accuracy: 0.8396 - val_loss: 0.8386 - val_accuracy: 0.5962\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.63462\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3793 - accuracy: 0.8396 - val_loss: 0.8759 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.63462\n",
      "Epoch 164/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3691 - accuracy: 0.8208 - val_loss: 0.7844 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.63462\n",
      "Epoch 165/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3631 - accuracy: 0.8585 - val_loss: 0.8257 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.63462\n",
      "Epoch 166/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3854 - accuracy: 0.8302 - val_loss: 0.7927 - val_accuracy: 0.5962\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.63462\n",
      "Epoch 167/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3541 - accuracy: 0.8491 - val_loss: 0.7348 - val_accuracy: 0.5962\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.63462\n",
      "Epoch 168/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3377 - accuracy: 0.8679 - val_loss: 0.7530 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.63462\n",
      "Epoch 169/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3256 - accuracy: 0.8774 - val_loss: 0.7343 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.63462\n",
      "Epoch 170/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3271 - accuracy: 0.8679 - val_loss: 0.8273 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.63462\n",
      "Epoch 171/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3387 - accuracy: 0.8491 - val_loss: 0.8195 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.63462\n",
      "Epoch 172/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3304 - accuracy: 0.8585 - val_loss: 0.8835 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.63462\n",
      "Epoch 173/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3908 - accuracy: 0.8302 - val_loss: 0.8159 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.63462\n",
      "Epoch 174/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3487 - accuracy: 0.8774 - val_loss: 0.7345 - val_accuracy: 0.6058\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.63462\n",
      "Epoch 175/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3187 - accuracy: 0.8774 - val_loss: 0.8214 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.63462\n",
      "Epoch 176/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3238 - accuracy: 0.8774 - val_loss: 0.7833 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.63462\n",
      "Epoch 177/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3038 - accuracy: 0.8774 - val_loss: 0.7542 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.63462\n",
      "Epoch 178/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3277 - accuracy: 0.8774 - val_loss: 0.7649 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.63462\n",
      "Epoch 179/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3041 - accuracy: 0.8868 - val_loss: 0.7699 - val_accuracy: 0.6058\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.63462\n",
      "Epoch 180/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2944 - accuracy: 0.8774 - val_loss: 0.7791 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.63462\n",
      "Epoch 181/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.2859 - accuracy: 0.8868 - val_loss: 0.7252 - val_accuracy: 0.6538\n",
      "\n",
      "Epoch 00181: val_accuracy improved from 0.63462 to 0.65385, saving model to best_model.pkl\n",
      "Epoch 182/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3131 - accuracy: 0.8679 - val_loss: 1.0858 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.65385\n",
      "Epoch 183/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.5745 - accuracy: 0.7453 - val_loss: 0.8752 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.65385\n",
      "Epoch 184/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4222 - accuracy: 0.8208 - val_loss: 0.7372 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.65385\n",
      "Epoch 185/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.4042 - accuracy: 0.8208 - val_loss: 0.7527 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.65385\n",
      "Epoch 186/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.3669 - accuracy: 0.8491 - val_loss: 0.8314 - val_accuracy: 0.5577\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.65385\n",
      "Epoch 187/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3659 - accuracy: 0.8585 - val_loss: 0.8830 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.65385\n",
      "Epoch 188/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3201 - accuracy: 0.8679 - val_loss: 0.7754 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.65385\n",
      "Epoch 189/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3281 - accuracy: 0.8774 - val_loss: 0.7397 - val_accuracy: 0.5962\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.65385\n",
      "Epoch 190/200\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.3232 - accuracy: 0.8962 - val_loss: 0.9433 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.65385\n",
      "Epoch 191/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.3017 - accuracy: 0.8868 - val_loss: 0.9359 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.65385\n",
      "Epoch 192/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2920 - accuracy: 0.8774 - val_loss: 0.8976 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.65385\n",
      "Epoch 193/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2891 - accuracy: 0.8962 - val_loss: 0.8663 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.65385\n",
      "Epoch 194/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.2945 - accuracy: 0.9151 - val_loss: 0.8928 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.65385\n",
      "Epoch 195/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2904 - accuracy: 0.8868 - val_loss: 1.0332 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.65385\n",
      "Epoch 196/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2919 - accuracy: 0.8868 - val_loss: 0.9813 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.65385\n",
      "Epoch 197/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2747 - accuracy: 0.9151 - val_loss: 0.8566 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.65385\n",
      "Epoch 198/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2883 - accuracy: 0.9245 - val_loss: 0.8831 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.65385\n",
      "Epoch 199/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2747 - accuracy: 0.9057 - val_loss: 1.0022 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.65385\n",
      "Epoch 200/200\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.2727 - accuracy: 0.9151 - val_loss: 1.0490 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.65385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29cb874ef60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "chk = ModelCheckpoint('best_model.pkl', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(train, train_target, epochs=200, batch_size=32, callbacks=[chk], validation_data=(validation,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7884615384615384"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the model and checking accuracy on the test data\n",
    "model = load_model('best_model.pkl')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_preds = model.predict_classes(test)\n",
    "accuracy_score(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39, 11],\n",
       "       [11, 43]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY2: USE LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(seq_len, n_features)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100)               42000     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,101\n",
      "Trainable params: 42,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106 samples, validate on 104 samples\n",
      "Epoch 1/600\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.6317 - val_loss: 0.8468\n",
      "Epoch 2/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7519 - val_loss: 0.7766\n",
      "Epoch 3/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.6983 - val_loss: 0.7167\n",
      "Epoch 4/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.6856 - val_loss: 0.7397\n",
      "Epoch 5/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.6796 - val_loss: 0.9180\n",
      "Epoch 6/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7493 - val_loss: 0.8302\n",
      "Epoch 7/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.6716 - val_loss: 0.7387\n",
      "Epoch 8/600\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.6136 - val_loss: 0.7689\n",
      "Epoch 9/600\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7411 - val_loss: 1.4152\n",
      "Epoch 10/600\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 1.4710 - val_loss: 0.6752\n",
      "Epoch 11/600\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 4.1528 - val_loss: 1.4821\n",
      "Epoch 12/600\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 4.1605 - val_loss: 0.8762\n",
      "Epoch 13/600\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 2.4399 - val_loss: 0.8293\n",
      "Epoch 14/600\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.6614 - val_loss: 0.7846\n",
      "Epoch 15/600\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.6026 - val_loss: 2.3788\n",
      "Epoch 16/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5253 - val_loss: 2.9503\n",
      "Epoch 17/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.4425 - val_loss: 2.1740\n",
      "Epoch 18/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5563 - val_loss: 3.5079\n",
      "Epoch 19/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.4493 - val_loss: 3.7758\n",
      "Epoch 20/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5593 - val_loss: 3.7729\n",
      "Epoch 21/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5474 - val_loss: 3.7654\n",
      "Epoch 22/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5646 - val_loss: 3.6437\n",
      "Epoch 23/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5064 - val_loss: 3.2110\n",
      "Epoch 24/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8889 - val_loss: 3.5536\n",
      "Epoch 25/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.4512 - val_loss: 4.7563\n",
      "Epoch 26/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 2.7726 - val_loss: 3.6163\n",
      "Epoch 27/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.4756 - val_loss: 4.6667\n",
      "Epoch 28/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7373 - val_loss: 3.9517\n",
      "Epoch 29/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5345 - val_loss: 4.7909\n",
      "Epoch 30/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5670 - val_loss: 4.6367\n",
      "Epoch 31/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.5380 - val_loss: 4.6520\n",
      "Epoch 32/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 4.4704 - val_loss: 4.3063\n",
      "Epoch 33/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 2.7219 - val_loss: 3.4573\n",
      "Epoch 34/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.5371 - val_loss: 4.2538\n",
      "Epoch 35/600\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 2.8488 - val_loss: 4.9275\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29cc9dd00f0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, \n",
    "          train_target, \n",
    "          epochs=600, \n",
    "          batch_size=5, \n",
    "          callbacks=[early_stop], \n",
    "          validation_data=(validation,validation_target),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29cc9e4dac8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23, 27],\n",
       "       [32, 22]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Lambda, Input, Dropout, Flatten, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps=60\n",
    "n_features=4\n",
    "n_outputs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "x = Input((n_timesteps, n_features))\n",
    "\n",
    "# LSTM\n",
    "lstm_1 = LSTM(150, activation='relu', input_shape=(n_timesteps, n_features))(x)\n",
    "lstm_2 = Dropout(0.5)(lstm_1)\n",
    "lstm_3 = Dense(150, activation='relu')(lstm_2)\n",
    "lstm_4 = Dense(n_outputs, activation='softmax', name='lstm_out')(lstm_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(x, \n",
    "           lstm_4,\n",
    "           name=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60, 4)             0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 150)               93000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "lstm_out (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 115,801\n",
      "Trainable params: 115,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_delta_val = 0.01\n",
    "lr_cb = ReduceLROnPlateau(monitor = 'val_auc', mode='max', \n",
    "                          factor = 0.5, min_delta = min_delta_val, patience = 3, verbose = 1)\n",
    "es_cb = EarlyStopping(monitor = 'val_auc', mode='max', \n",
    "                      min_delta=min_delta_val, patience = 10, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "default_callbacks = [lr_cb, es_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106 samples, validate on 104 samples\n",
      "Epoch 1/200\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_auc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "c:\\python36\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 3/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 4/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 5/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 6/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 7/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 8/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 9/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 10/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 13/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 15/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 16/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 17/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 18/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 19/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 20/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 21/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 22/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 23/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 24/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 25/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 26/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 27/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 28/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 29/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 30/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 31/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 32/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 33/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 34/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 35/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 36/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 37/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 38/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 39/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 40/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 41/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 42/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 43/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 44/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 45/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 46/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 47/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 48/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 49/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 50/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 51/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 52/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 53/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 54/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 56/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 60/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 61/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 62/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 63/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 64/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 65/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 66/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 67/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 68/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 69/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 70/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 71/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 72/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 73/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 74/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 75/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 76/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 77/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 78/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 79/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 80/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 81/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 82/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 83/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 84/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 85/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 86/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 87/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 88/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 89/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 90/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 91/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 92/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 93/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 94/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 95/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 96/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 97/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 98/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 99/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 100/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 101/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 102/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 103/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 104/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 105/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 106/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 107/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 108/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 109/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 110/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 111/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 112/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 113/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 114/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 115/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 117/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 118/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 119/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 120/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 121/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 122/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 123/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 124/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 125/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 126/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 127/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 128/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 129/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 130/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 131/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 132/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 133/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 134/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 135/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 136/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 137/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 138/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 139/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 140/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 141/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 142/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 143/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 144/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 145/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 146/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 147/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 148/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 149/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 150/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 151/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 152/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 153/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 154/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 155/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 156/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 157/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 158/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 159/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 160/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 161/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 162/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 163/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 164/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 165/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 166/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 167/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 168/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 169/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 170/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 171/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 172/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 174/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 175/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 176/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 177/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 178/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 179/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 180/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 181/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 182/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 183/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 184/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 185/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 186/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 187/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 188/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 189/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 190/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 191/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 192/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 193/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 194/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 195/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 196/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 197/200\n",
      "106/106 [==============================] - 0s 2ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 198/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 199/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n",
      "Epoch 200/200\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 7.8113 - accuracy: 0.4906 - val_loss: 7.6666 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29cc853b8d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=200\n",
    "batch_size=1000\n",
    "validation_split_on_training = 0.2\n",
    "# train model\n",
    "model.fit(train,\n",
    "          train_target, epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation, validation_target),\n",
    "          #validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "          callbacks=default_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127,\n",
       "  7.666618824005127],\n",
       " 'val_accuracy': [0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5],\n",
       " 'loss': [7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811271667480469,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811271667480469,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811271667480469,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785,\n",
       "  7.811272621154785],\n",
       " 'accuracy': [0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605,\n",
       "  0.49056605],\n",
       " 'lr': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29ccee9b9b0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
