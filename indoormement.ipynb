{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./MovementAAL/dataset/MovementAAL_RSS_1.csv')\n",
    "df2 = pd.read_csv('./MovementAAL/dataset/MovementAAL_RSS_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.90476</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.28571</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.38095</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.14286</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.28571</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.47619</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.90476         -0.48       0.28571          0.30\n",
       "1      -0.57143         -0.32       0.14286          0.30\n",
       "2      -0.38095         -0.28      -0.14286          0.35\n",
       "3      -0.28571         -0.20      -0.47619          0.35\n",
       "4      -0.14286         -0.20       0.14286         -0.20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.76190</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.85714</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.57143         -0.20       0.71429          0.50\n",
       "1      -0.76190         -0.48       0.76190         -0.25\n",
       "2      -0.85714         -0.60       0.85714          0.55\n",
       "3      -0.76190         -0.40       0.71429          0.60\n",
       "4      -0.76190         -0.84       0.85714          0.45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MovementAAL/dataset/MovementAAL_RSS_1.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_2.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_3.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_4.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_5.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_6.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_7.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_8.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_9.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_10.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_11.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_12.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_13.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_14.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_15.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_16.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_17.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_18.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_19.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_20.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_21.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_22.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_23.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_24.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_25.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_26.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_27.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_28.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_29.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_30.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_31.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_32.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_33.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_34.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_35.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_36.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_37.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_38.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_39.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_40.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_41.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_42.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_43.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_44.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_45.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_46.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_47.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_48.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_49.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_50.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_51.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_52.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_53.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_54.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_55.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_56.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_57.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_58.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_59.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_60.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_61.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_62.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_63.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_64.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_65.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_66.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_67.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_68.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_69.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_70.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_71.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_72.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_73.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_74.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_75.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_76.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_77.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_78.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_79.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_80.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_81.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_82.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_83.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_84.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_85.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_86.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_87.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_88.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_89.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_90.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_91.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_92.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_93.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_94.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_95.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_96.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_97.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_98.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_99.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_100.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_101.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_102.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_103.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_104.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_105.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_106.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_107.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_108.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_109.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_110.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_111.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_112.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_113.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_114.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_115.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_116.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_117.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_118.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_119.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_120.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_121.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_122.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_123.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_124.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_125.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_126.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_127.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_128.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_129.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_130.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_131.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_132.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_133.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_134.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_135.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_136.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_137.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_138.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_139.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_140.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_141.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_142.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_143.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_144.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_145.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_146.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_147.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_148.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_149.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_150.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_151.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_152.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_153.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_154.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_155.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_156.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_157.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_158.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_159.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_160.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_161.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_162.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_163.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_164.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_165.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_166.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_167.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_168.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_169.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_170.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_171.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_172.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_173.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_174.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_175.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_176.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_177.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_178.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_179.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_180.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_181.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_182.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_183.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_184.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MovementAAL/dataset/MovementAAL_RSS_185.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_186.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_187.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_188.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_189.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_190.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_191.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_192.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_193.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_194.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_195.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_196.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_197.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_198.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_199.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_200.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_201.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_202.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_203.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_204.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_205.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_206.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_207.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_208.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_209.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_210.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_211.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_212.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_213.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_214.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_215.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_216.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_217.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_218.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_219.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_220.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_221.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_222.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_223.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_224.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_225.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_226.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_227.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_228.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_229.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_230.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_231.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_232.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_233.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_234.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_235.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_236.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_237.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_238.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_239.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_240.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_241.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_242.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_243.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_244.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_245.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_246.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_247.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_248.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_249.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_250.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_251.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_252.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_253.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_254.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_255.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_256.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_257.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_258.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_259.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_260.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_261.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_262.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_263.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_264.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_265.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_266.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_267.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_268.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_269.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_270.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_271.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_272.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_273.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_274.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_275.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_276.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_277.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_278.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_279.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_280.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_281.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_282.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_283.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_284.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_285.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_286.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_287.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_288.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_289.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_290.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_291.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_292.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_293.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_294.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_295.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_296.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_297.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_298.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_299.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_300.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_301.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_302.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_303.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_304.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_305.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_306.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_307.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_308.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_309.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_310.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_311.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_312.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_313.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_314.csv\n"
     ]
    }
   ],
   "source": [
    "path = './MovementAAL/dataset/MovementAAL_RSS_'\n",
    "sequences = list()\n",
    "for i in range(1,315):\n",
    "    file_path = path + str(i) + '.csv'\n",
    "    print(file_path)\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    values = df.values\n",
    "    sequences.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('./MovementAAL/dataset/MovementAAL_target.csv')\n",
    "targets = targets.values[:,1]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences[313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = pd.read_csv('./MovementAAL/groups/MovementAAL_DatasetGroup.csv', header=0)\n",
    "groups = groups.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    314.000000\n",
       "mean      42.028662\n",
       "std       16.185303\n",
       "min       19.000000\n",
       "25%       26.000000\n",
       "50%       41.000000\n",
       "75%       56.000000\n",
       "max      129.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sequences = []\n",
    "for one_seq in sequences:\n",
    "    len_sequences.append(len(one_seq))\n",
    "pd.Series(len_sequences).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the sequence with the values in last row to max length\n",
    "to_pad = 129\n",
    "new_seq = []\n",
    "for one_seq in sequences:\n",
    "    len_one_seq = len(one_seq)\n",
    "    last_val = one_seq[-1]\n",
    "    n = to_pad - len_one_seq\n",
    "   \n",
    "    to_concat = np.repeat(one_seq[-1], n).reshape(4, n).transpose()\n",
    "    new_one_seq = np.concatenate([one_seq, to_concat])\n",
    "    new_seq.append(new_one_seq)\n",
    "final_seq = np.stack(new_seq)\n",
    "\n",
    "#truncate the sequence to length 60\n",
    "from keras.preprocessing import sequence\n",
    "seq_len = 60\n",
    "final_seq=sequence.pad_sequences(final_seq, maxlen=seq_len, padding='post', dtype='float', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 60, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [final_seq[i] for i in range(len(groups)) if (groups[i]==2)]\n",
    "validation = [final_seq[i] for i in range(len(groups)) if groups[i]==1]\n",
    "test = [final_seq[i] for i in range(len(groups)) if groups[i]==3]\n",
    "\n",
    "train_target = [targets[i] for i in range(len(groups)) if (groups[i]==2)]\n",
    "validation_target = [targets[i] for i in range(len(groups)) if groups[i]==1]\n",
    "test_target = [targets[i] for i in range(len(groups)) if groups[i]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "validation = np.array(validation)\n",
    "test = np.array(test)\n",
    "\n",
    "train_target = np.array(train_target)\n",
    "train_target = (train_target+1)/2\n",
    "\n",
    "validation_target = np.array(validation_target)\n",
    "validation_target = (validation_target+1)/2\n",
    "\n",
    "test_target = np.array(test_target)\n",
    "test_target = (test_target+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 60, 4), (104, 60, 4), (104, 60, 4))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY1: Building time series classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len, 4)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               267264    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 293,065\n",
      "Trainable params: 293,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.4811\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.71154, saving model to best_model\n",
      "WARNING:tensorflow:From c:\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.6982 - accuracy: 0.4811 - val_loss: 0.6505 - val_accuracy: 0.7115\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8830 - accuracy: 0.4906\n",
      "Epoch 00002: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 0.8830 - accuracy: 0.4906 - val_loss: 0.6499 - val_accuracy: 0.6154\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7618 - accuracy: 0.5472\n",
      "Epoch 00003: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.7618 - accuracy: 0.5472 - val_loss: 0.6203 - val_accuracy: 0.6346\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7753 - accuracy: 0.4811\n",
      "Epoch 00004: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.7753 - accuracy: 0.4811 - val_loss: 0.6998 - val_accuracy: 0.5769\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7564 - accuracy: 0.5189\n",
      "Epoch 00005: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 0.7564 - accuracy: 0.5189 - val_loss: 0.7081 - val_accuracy: 0.4808\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.5189\n",
      "Epoch 00006: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 195ms/step - loss: 0.7078 - accuracy: 0.5189 - val_loss: 0.7223 - val_accuracy: 0.4231\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6961 - accuracy: 0.5189\n",
      "Epoch 00007: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6961 - accuracy: 0.5189 - val_loss: 0.6925 - val_accuracy: 0.5673\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.5000\n",
      "Epoch 00008: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6920 - accuracy: 0.5000 - val_loss: 0.7143 - val_accuracy: 0.5096\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7106 - accuracy: 0.4245\n",
      "Epoch 00009: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 0.7106 - accuracy: 0.4245 - val_loss: 0.6944 - val_accuracy: 0.5288\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6947 - accuracy: 0.5283\n",
      "Epoch 00010: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 157ms/step - loss: 0.6947 - accuracy: 0.5283 - val_loss: 0.6981 - val_accuracy: 0.4327\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.5189\n",
      "Epoch 00011: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.6902 - accuracy: 0.5189 - val_loss: 0.7049 - val_accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.5377\n",
      "Epoch 00012: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.6953 - accuracy: 0.5377 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7024 - accuracy: 0.4340\n",
      "Epoch 00013: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.7024 - accuracy: 0.4340 - val_loss: 0.6889 - val_accuracy: 0.5385\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.5189\n",
      "Epoch 00014: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 156ms/step - loss: 0.6869 - accuracy: 0.5189 - val_loss: 0.6907 - val_accuracy: 0.4519\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.5943\n",
      "Epoch 00015: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 0.6775 - accuracy: 0.5943 - val_loss: 0.6981 - val_accuracy: 0.5096\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.5943\n",
      "Epoch 00016: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.6793 - accuracy: 0.5943 - val_loss: 0.7151 - val_accuracy: 0.5096\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6694 - accuracy: 0.5943\n",
      "Epoch 00017: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 163ms/step - loss: 0.6694 - accuracy: 0.5943 - val_loss: 0.7222 - val_accuracy: 0.5000\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.5943\n",
      "Epoch 00018: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 147ms/step - loss: 0.6695 - accuracy: 0.5943 - val_loss: 0.7661 - val_accuracy: 0.5000\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.5755\n",
      "Epoch 00019: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.6679 - accuracy: 0.5755 - val_loss: 0.7346 - val_accuracy: 0.5673\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.6415\n",
      "Epoch 00020: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.6311 - accuracy: 0.6415 - val_loss: 0.6835 - val_accuracy: 0.5096\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6289 - accuracy: 0.5943\n",
      "Epoch 00021: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.6289 - accuracy: 0.5943 - val_loss: 0.8546 - val_accuracy: 0.4135\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.5094\n",
      "Epoch 00022: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.7657 - accuracy: 0.5094 - val_loss: 0.7335 - val_accuracy: 0.5096\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.5755\n",
      "Epoch 00023: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 156ms/step - loss: 0.6786 - accuracy: 0.5755 - val_loss: 0.6662 - val_accuracy: 0.5673\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7132 - accuracy: 0.4906\n",
      "Epoch 00024: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 0.7132 - accuracy: 0.4906 - val_loss: 0.7098 - val_accuracy: 0.5000\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7212 - accuracy: 0.5189\n",
      "Epoch 00025: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 163ms/step - loss: 0.7212 - accuracy: 0.5189 - val_loss: 0.6958 - val_accuracy: 0.4712\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7121 - accuracy: 0.4717\n",
      "Epoch 00026: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.7121 - accuracy: 0.4717 - val_loss: 0.6854 - val_accuracy: 0.4904\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6819 - accuracy: 0.5566\n",
      "Epoch 00027: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 0.6819 - accuracy: 0.5566 - val_loss: 0.6879 - val_accuracy: 0.5288\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.5566\n",
      "Epoch 00028: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.6824 - accuracy: 0.5566 - val_loss: 0.6960 - val_accuracy: 0.5288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.5566\n",
      "Epoch 00029: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.6707 - accuracy: 0.5566 - val_loss: 0.7063 - val_accuracy: 0.5481\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.5472\n",
      "Epoch 00030: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.6892 - accuracy: 0.5472 - val_loss: 0.7102 - val_accuracy: 0.5385\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.5283\n",
      "Epoch 00031: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 322ms/step - loss: 0.6878 - accuracy: 0.5283 - val_loss: 0.7038 - val_accuracy: 0.5385\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.5377\n",
      "Epoch 00032: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.6901 - accuracy: 0.5377 - val_loss: 0.6878 - val_accuracy: 0.5288\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7298 - accuracy: 0.5094\n",
      "Epoch 00033: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.7298 - accuracy: 0.5094 - val_loss: 0.7079 - val_accuracy: 0.5000\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.4906\n",
      "Epoch 00034: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 182ms/step - loss: 0.7015 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6808 - accuracy: 0.5660\n",
      "Epoch 00035: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6808 - accuracy: 0.5660 - val_loss: 0.6924 - val_accuracy: 0.5288\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.6038\n",
      "Epoch 00036: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.6834 - accuracy: 0.6038 - val_loss: 0.6964 - val_accuracy: 0.5000\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.6132\n",
      "Epoch 00037: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.6793 - accuracy: 0.6132 - val_loss: 0.7028 - val_accuracy: 0.4615\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.5660\n",
      "Epoch 00038: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 156ms/step - loss: 0.6786 - accuracy: 0.5660 - val_loss: 0.7040 - val_accuracy: 0.5000\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.6226\n",
      "Epoch 00039: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 156ms/step - loss: 0.6674 - accuracy: 0.6226 - val_loss: 0.7109 - val_accuracy: 0.5096\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.6981\n",
      "Epoch 00040: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.6355 - accuracy: 0.6981 - val_loss: 0.7188 - val_accuracy: 0.5096\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.6509\n",
      "Epoch 00041: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 0.6222 - accuracy: 0.6509 - val_loss: 0.7305 - val_accuracy: 0.5000\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7358\n",
      "Epoch 00042: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.5545 - accuracy: 0.7358 - val_loss: 0.8939 - val_accuracy: 0.5385\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.7170\n",
      "Epoch 00043: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.5477 - accuracy: 0.7170 - val_loss: 1.1880 - val_accuracy: 0.5096\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.6509\n",
      "Epoch 00044: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 162ms/step - loss: 0.7078 - accuracy: 0.6509 - val_loss: 0.6873 - val_accuracy: 0.5962\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5942 - accuracy: 0.6226\n",
      "Epoch 00045: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.5942 - accuracy: 0.6226 - val_loss: 1.0495 - val_accuracy: 0.4423\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7109 - accuracy: 0.6321\n",
      "Epoch 00046: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 0.7109 - accuracy: 0.6321 - val_loss: 0.7076 - val_accuracy: 0.5481\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6286 - accuracy: 0.6509\n",
      "Epoch 00047: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.6286 - accuracy: 0.6509 - val_loss: 0.7460 - val_accuracy: 0.5000\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6725 - accuracy: 0.5472\n",
      "Epoch 00048: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.6725 - accuracy: 0.5472 - val_loss: 0.7810 - val_accuracy: 0.5000\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6387 - accuracy: 0.6604\n",
      "Epoch 00049: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.6387 - accuracy: 0.6604 - val_loss: 0.7645 - val_accuracy: 0.5288\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.6415\n",
      "Epoch 00050: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.6315 - accuracy: 0.6415 - val_loss: 0.7545 - val_accuracy: 0.5096\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.6415\n",
      "Epoch 00051: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.6371 - accuracy: 0.6415 - val_loss: 0.7268 - val_accuracy: 0.4904\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7153 - accuracy: 0.5189\n",
      "Epoch 00052: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.7153 - accuracy: 0.5189 - val_loss: 0.7151 - val_accuracy: 0.4808\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6619 - accuracy: 0.5943\n",
      "Epoch 00053: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.6619 - accuracy: 0.5943 - val_loss: 0.6963 - val_accuracy: 0.5769\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6237 - accuracy: 0.6698\n",
      "Epoch 00054: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.6237 - accuracy: 0.6698 - val_loss: 0.7078 - val_accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.6321\n",
      "Epoch 00055: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.6192 - accuracy: 0.6321 - val_loss: 0.7329 - val_accuracy: 0.4904\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5951 - accuracy: 0.6981\n",
      "Epoch 00056: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.5951 - accuracy: 0.6981 - val_loss: 0.7445 - val_accuracy: 0.5385\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.7264\n",
      "Epoch 00057: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.5438 - accuracy: 0.7264 - val_loss: 0.7978 - val_accuracy: 0.5096\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.7075\n",
      "Epoch 00058: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 0.5375 - accuracy: 0.7075 - val_loss: 1.5340 - val_accuracy: 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.2836 - accuracy: 0.5094\n",
      "Epoch 00059: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 1.2836 - accuracy: 0.5094 - val_loss: 0.7650 - val_accuracy: 0.5481\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7564 - accuracy: 0.5189\n",
      "Epoch 00060: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.7564 - accuracy: 0.5189 - val_loss: 0.7384 - val_accuracy: 0.4615\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6691 - accuracy: 0.5377\n",
      "Epoch 00061: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.6691 - accuracy: 0.5377 - val_loss: 0.7131 - val_accuracy: 0.4135\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6779 - accuracy: 0.5377\n",
      "Epoch 00062: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6779 - accuracy: 0.5377 - val_loss: 0.6811 - val_accuracy: 0.5000\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6760 - accuracy: 0.5472\n",
      "Epoch 00063: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.6760 - accuracy: 0.5472 - val_loss: 0.6720 - val_accuracy: 0.6346\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.5000\n",
      "Epoch 00064: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.6956 - accuracy: 0.5000 - val_loss: 0.6891 - val_accuracy: 0.5192\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.5377\n",
      "Epoch 00065: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.6902 - accuracy: 0.5377 - val_loss: 0.6943 - val_accuracy: 0.4904\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.5377\n",
      "Epoch 00066: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.6810 - accuracy: 0.5377 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.5755\n",
      "Epoch 00067: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 154ms/step - loss: 0.6890 - accuracy: 0.5755 - val_loss: 0.6928 - val_accuracy: 0.5769\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6791 - accuracy: 0.5566\n",
      "Epoch 00068: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.6791 - accuracy: 0.5566 - val_loss: 0.6913 - val_accuracy: 0.5962\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.6132\n",
      "Epoch 00069: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.6799 - accuracy: 0.6132 - val_loss: 0.6806 - val_accuracy: 0.5673\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6714 - accuracy: 0.6132\n",
      "Epoch 00070: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.6714 - accuracy: 0.6132 - val_loss: 0.6717 - val_accuracy: 0.5962\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6621 - accuracy: 0.6792\n",
      "Epoch 00071: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 151ms/step - loss: 0.6621 - accuracy: 0.6792 - val_loss: 0.6753 - val_accuracy: 0.5385\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6601 - accuracy: 0.6132\n",
      "Epoch 00072: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6601 - accuracy: 0.6132 - val_loss: 0.6927 - val_accuracy: 0.6346\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.5283\n",
      "Epoch 00073: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 153ms/step - loss: 0.6983 - accuracy: 0.5283 - val_loss: 0.6702 - val_accuracy: 0.6827\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6387 - accuracy: 0.6226\n",
      "Epoch 00074: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.6387 - accuracy: 0.6226 - val_loss: 0.6703 - val_accuracy: 0.5673\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6534 - accuracy: 0.6415\n",
      "Epoch 00075: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.6534 - accuracy: 0.6415 - val_loss: 0.6872 - val_accuracy: 0.4808\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6439 - accuracy: 0.6038\n",
      "Epoch 00076: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.6439 - accuracy: 0.6038 - val_loss: 0.6669 - val_accuracy: 0.5481\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6170 - accuracy: 0.6415\n",
      "Epoch 00077: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 162ms/step - loss: 0.6170 - accuracy: 0.6415 - val_loss: 0.6688 - val_accuracy: 0.6346\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6391 - accuracy: 0.6698\n",
      "Epoch 00078: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.6391 - accuracy: 0.6698 - val_loss: 0.6411 - val_accuracy: 0.6731\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5903 - accuracy: 0.6981\n",
      "Epoch 00079: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 0s 121ms/step - loss: 0.5903 - accuracy: 0.6981 - val_loss: 0.7285 - val_accuracy: 0.5481\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7264\n",
      "Epoch 00080: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.5372 - accuracy: 0.7264 - val_loss: 0.6352 - val_accuracy: 0.6731\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5319 - accuracy: 0.6792\n",
      "Epoch 00081: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.5319 - accuracy: 0.6792 - val_loss: 1.3124 - val_accuracy: 0.5481\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.7925\n",
      "Epoch 00082: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.5010 - accuracy: 0.7925 - val_loss: 0.7175 - val_accuracy: 0.6538\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.7170\n",
      "Epoch 00083: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.5174 - accuracy: 0.7170 - val_loss: 0.8856 - val_accuracy: 0.5577\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6189 - accuracy: 0.6698\n",
      "Epoch 00084: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 153ms/step - loss: 0.6189 - accuracy: 0.6698 - val_loss: 0.8152 - val_accuracy: 0.5769\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5126 - accuracy: 0.7358\n",
      "Epoch 00085: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.5126 - accuracy: 0.7358 - val_loss: 0.8273 - val_accuracy: 0.5769\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.6698\n",
      "Epoch 00086: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.5780 - accuracy: 0.6698 - val_loss: 0.7819 - val_accuracy: 0.6154\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.7736\n",
      "Epoch 00087: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.4815 - accuracy: 0.7736 - val_loss: 0.8300 - val_accuracy: 0.5577\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8208\n",
      "Epoch 00088: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.4252 - accuracy: 0.8208 - val_loss: 0.9580 - val_accuracy: 0.5673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.8113\n",
      "Epoch 00089: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.3808 - accuracy: 0.8113 - val_loss: 1.0466 - val_accuracy: 0.5962\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8208\n",
      "Epoch 00090: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.3495 - accuracy: 0.8208 - val_loss: 1.2378 - val_accuracy: 0.5673\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8828 - accuracy: 0.6887\n",
      "Epoch 00091: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.8828 - accuracy: 0.6887 - val_loss: 1.0328 - val_accuracy: 0.5192\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7042 - accuracy: 0.6132\n",
      "Epoch 00092: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.7042 - accuracy: 0.6132 - val_loss: 0.6506 - val_accuracy: 0.6635\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.6132\n",
      "Epoch 00093: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.6134 - accuracy: 0.6132 - val_loss: 0.6538 - val_accuracy: 0.7115\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.6887\n",
      "Epoch 00094: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.5824 - accuracy: 0.6887 - val_loss: 0.6731 - val_accuracy: 0.6731\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.6887\n",
      "Epoch 00095: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 0.5862 - accuracy: 0.6887 - val_loss: 0.6782 - val_accuracy: 0.5769\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.7264\n",
      "Epoch 00096: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 162ms/step - loss: 0.5550 - accuracy: 0.7264 - val_loss: 0.6749 - val_accuracy: 0.5288\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.7925\n",
      "Epoch 00097: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 0.5270 - accuracy: 0.7925 - val_loss: 0.6600 - val_accuracy: 0.5673\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.7736\n",
      "Epoch 00098: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.4898 - accuracy: 0.7736 - val_loss: 0.6521 - val_accuracy: 0.5769\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4967 - accuracy: 0.8113\n",
      "Epoch 00099: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 158ms/step - loss: 0.4967 - accuracy: 0.8113 - val_loss: 0.7048 - val_accuracy: 0.5481\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.7736\n",
      "Epoch 00100: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 0.4655 - accuracy: 0.7736 - val_loss: 0.8245 - val_accuracy: 0.5192\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.8396\n",
      "Epoch 00101: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.3926 - accuracy: 0.8396 - val_loss: 0.7056 - val_accuracy: 0.5962\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.7925\n",
      "Epoch 00102: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.4427 - accuracy: 0.7925 - val_loss: 0.9387 - val_accuracy: 0.5096\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.7925\n",
      "Epoch 00103: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 152ms/step - loss: 0.4146 - accuracy: 0.7925 - val_loss: 0.9213 - val_accuracy: 0.5000\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8208\n",
      "Epoch 00104: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.3841 - accuracy: 0.8208 - val_loss: 0.7886 - val_accuracy: 0.5481\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9057\n",
      "Epoch 00105: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.2680 - accuracy: 0.9057 - val_loss: 1.0016 - val_accuracy: 0.5096\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.8868\n",
      "Epoch 00106: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 152ms/step - loss: 0.4347 - accuracy: 0.8868 - val_loss: 1.2227 - val_accuracy: 0.6154\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.8774\n",
      "Epoch 00107: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 153ms/step - loss: 0.4634 - accuracy: 0.8774 - val_loss: 1.1197 - val_accuracy: 0.5673\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.8396\n",
      "Epoch 00108: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.3857 - accuracy: 0.8396 - val_loss: 1.2130 - val_accuracy: 0.5577\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8585\n",
      "Epoch 00109: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.3951 - accuracy: 0.8585 - val_loss: 0.8923 - val_accuracy: 0.6154\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4864 - accuracy: 0.8302\n",
      "Epoch 00110: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.4864 - accuracy: 0.8302 - val_loss: 0.8601 - val_accuracy: 0.5385\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4767 - accuracy: 0.8113\n",
      "Epoch 00111: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.4767 - accuracy: 0.8113 - val_loss: 0.8064 - val_accuracy: 0.5096\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6204 - accuracy: 0.7264\n",
      "Epoch 00112: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.6204 - accuracy: 0.7264 - val_loss: 0.6885 - val_accuracy: 0.5481\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.6698\n",
      "Epoch 00113: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6418 - accuracy: 0.6698 - val_loss: 0.7080 - val_accuracy: 0.5000\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6593 - accuracy: 0.5755\n",
      "Epoch 00114: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.6593 - accuracy: 0.5755 - val_loss: 0.7286 - val_accuracy: 0.4712\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6413 - accuracy: 0.6321\n",
      "Epoch 00115: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.6413 - accuracy: 0.6321 - val_loss: 0.8166 - val_accuracy: 0.4808\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.6509\n",
      "Epoch 00116: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.6083 - accuracy: 0.6509 - val_loss: 0.8376 - val_accuracy: 0.4423\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.5755\n",
      "Epoch 00117: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.6811 - accuracy: 0.5755 - val_loss: 0.7776 - val_accuracy: 0.5000\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7041 - accuracy: 0.4623\n",
      "Epoch 00118: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.7041 - accuracy: 0.4623 - val_loss: 0.7564 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.4906\n",
      "Epoch 00119: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.6972 - accuracy: 0.4906 - val_loss: 0.7410 - val_accuracy: 0.5000\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.5000\n",
      "Epoch 00120: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.6981 - accuracy: 0.5000 - val_loss: 0.7296 - val_accuracy: 0.5096\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.6038\n",
      "Epoch 00121: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.6785 - accuracy: 0.6038 - val_loss: 0.7151 - val_accuracy: 0.5000\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.5660\n",
      "Epoch 00122: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6841 - accuracy: 0.5660 - val_loss: 0.7121 - val_accuracy: 0.5481\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.4717\n",
      "Epoch 00123: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.6972 - accuracy: 0.4717 - val_loss: 0.7200 - val_accuracy: 0.6346\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.5472\n",
      "Epoch 00124: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.7014 - accuracy: 0.5472 - val_loss: 0.7167 - val_accuracy: 0.5673\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.5377\n",
      "Epoch 00125: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6858 - accuracy: 0.5377 - val_loss: 0.7300 - val_accuracy: 0.5000\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.5849\n",
      "Epoch 00126: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 0.6860 - accuracy: 0.5849 - val_loss: 0.7250 - val_accuracy: 0.5000\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.5377\n",
      "Epoch 00127: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.6834 - accuracy: 0.5377 - val_loss: 0.7658 - val_accuracy: 0.5000\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.5660\n",
      "Epoch 00128: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.6370 - accuracy: 0.5660 - val_loss: 0.7803 - val_accuracy: 0.5096\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.7736\n",
      "Epoch 00129: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.5272 - accuracy: 0.7736 - val_loss: 0.8046 - val_accuracy: 0.5481\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4445 - accuracy: 0.8019\n",
      "Epoch 00130: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.4445 - accuracy: 0.8019 - val_loss: 0.8729 - val_accuracy: 0.5192\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9180 - accuracy: 0.5849\n",
      "Epoch 00131: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 162ms/step - loss: 0.9180 - accuracy: 0.5849 - val_loss: 0.7507 - val_accuracy: 0.6538\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7624 - accuracy: 0.5000\n",
      "Epoch 00132: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.7624 - accuracy: 0.5000 - val_loss: 0.6960 - val_accuracy: 0.5000\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 00133: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 0.6927 - accuracy: 0.5000 - val_loss: 0.7246 - val_accuracy: 0.5000\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7353 - accuracy: 0.3962\n",
      "Epoch 00134: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 163ms/step - loss: 0.7353 - accuracy: 0.3962 - val_loss: 0.7224 - val_accuracy: 0.5769\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.5377\n",
      "Epoch 00135: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.6898 - accuracy: 0.5377 - val_loss: 0.7344 - val_accuracy: 0.4423\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.5094\n",
      "Epoch 00136: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.6942 - accuracy: 0.5094 - val_loss: 0.7497 - val_accuracy: 0.4808\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6442 - accuracy: 0.5943\n",
      "Epoch 00137: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 159ms/step - loss: 0.6442 - accuracy: 0.5943 - val_loss: 0.8964 - val_accuracy: 0.3654\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.6226\n",
      "Epoch 00138: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 0.6195 - accuracy: 0.6226 - val_loss: 0.9817 - val_accuracy: 0.4038\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.6226\n",
      "Epoch 00139: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 0.5999 - accuracy: 0.6226 - val_loss: 1.2254 - val_accuracy: 0.4327\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5671 - accuracy: 0.5849\n",
      "Epoch 00140: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.5671 - accuracy: 0.5849 - val_loss: 1.4349 - val_accuracy: 0.5096\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.7453\n",
      "Epoch 00141: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.5037 - accuracy: 0.7453 - val_loss: 1.7713 - val_accuracy: 0.4904\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.7264\n",
      "Epoch 00142: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 151ms/step - loss: 0.5396 - accuracy: 0.7264 - val_loss: 1.6227 - val_accuracy: 0.4423\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5222 - accuracy: 0.7170\n",
      "Epoch 00143: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 162ms/step - loss: 0.5222 - accuracy: 0.7170 - val_loss: 1.5376 - val_accuracy: 0.5192\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.7264\n",
      "Epoch 00144: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.5877 - accuracy: 0.7264 - val_loss: 1.4581 - val_accuracy: 0.4615\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.7925\n",
      "Epoch 00145: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.4791 - accuracy: 0.7925 - val_loss: 1.5861 - val_accuracy: 0.4808\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.7547\n",
      "Epoch 00146: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.5084 - accuracy: 0.7547 - val_loss: 1.5652 - val_accuracy: 0.5000\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.7642\n",
      "Epoch 00147: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.5174 - accuracy: 0.7642 - val_loss: 1.5571 - val_accuracy: 0.5288\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.8113\n",
      "Epoch 00148: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.4482 - accuracy: 0.8113 - val_loss: 1.5708 - val_accuracy: 0.5096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.8302\n",
      "Epoch 00149: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.4178 - accuracy: 0.8302 - val_loss: 1.6844 - val_accuracy: 0.4712\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.8019\n",
      "Epoch 00150: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.4749 - accuracy: 0.8019 - val_loss: 1.7181 - val_accuracy: 0.3846\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8019\n",
      "Epoch 00151: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 0.4252 - accuracy: 0.8019 - val_loss: 1.2389 - val_accuracy: 0.4712\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5943 - accuracy: 0.6981\n",
      "Epoch 00152: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.5943 - accuracy: 0.6981 - val_loss: 0.7338 - val_accuracy: 0.6635\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.7736\n",
      "Epoch 00153: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.4909 - accuracy: 0.7736 - val_loss: 0.7224 - val_accuracy: 0.6346\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.7642\n",
      "Epoch 00154: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 159ms/step - loss: 0.4508 - accuracy: 0.7642 - val_loss: 0.7621 - val_accuracy: 0.5577\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.8208\n",
      "Epoch 00155: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 0.4225 - accuracy: 0.8208 - val_loss: 0.8176 - val_accuracy: 0.5962\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.7453\n",
      "Epoch 00156: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.5006 - accuracy: 0.7453 - val_loss: 0.7558 - val_accuracy: 0.5769\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4709 - accuracy: 0.7547\n",
      "Epoch 00157: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.4709 - accuracy: 0.7547 - val_loss: 0.7507 - val_accuracy: 0.5577\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.7358\n",
      "Epoch 00158: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.5422 - accuracy: 0.7358 - val_loss: 0.8919 - val_accuracy: 0.4904\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4975 - accuracy: 0.7358\n",
      "Epoch 00159: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.4975 - accuracy: 0.7358 - val_loss: 1.1688 - val_accuracy: 0.5385\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6416 - accuracy: 0.6887\n",
      "Epoch 00160: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.6416 - accuracy: 0.6887 - val_loss: 1.4443 - val_accuracy: 0.4904\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.7075\n",
      "Epoch 00161: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.5287 - accuracy: 0.7075 - val_loss: 1.6035 - val_accuracy: 0.4904\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4640 - accuracy: 0.7736\n",
      "Epoch 00162: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.4640 - accuracy: 0.7736 - val_loss: 1.5541 - val_accuracy: 0.5096\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.7736\n",
      "Epoch 00163: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.4931 - accuracy: 0.7736 - val_loss: 1.6020 - val_accuracy: 0.5000\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.8302\n",
      "Epoch 00164: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.4366 - accuracy: 0.8302 - val_loss: 1.6842 - val_accuracy: 0.4904\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8679\n",
      "Epoch 00165: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.3654 - accuracy: 0.8679 - val_loss: 1.6037 - val_accuracy: 0.5096\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.8679\n",
      "Epoch 00166: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.2957 - accuracy: 0.8679 - val_loss: 1.4073 - val_accuracy: 0.5962\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.8585\n",
      "Epoch 00167: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.3668 - accuracy: 0.8585 - val_loss: 1.4803 - val_accuracy: 0.5385\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.8774\n",
      "Epoch 00168: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.4013 - accuracy: 0.8774 - val_loss: 2.2207 - val_accuracy: 0.6154\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.6792\n",
      "Epoch 00169: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.7850 - accuracy: 0.6792 - val_loss: 1.3364 - val_accuracy: 0.6058\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.7547\n",
      "Epoch 00170: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.5322 - accuracy: 0.7547 - val_loss: 0.7539 - val_accuracy: 0.5865\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.8774\n",
      "Epoch 00171: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 0.3613 - accuracy: 0.8774 - val_loss: 1.0375 - val_accuracy: 0.5673\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3915 - accuracy: 0.8491\n",
      "Epoch 00172: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 0.3915 - accuracy: 0.8491 - val_loss: 0.8413 - val_accuracy: 0.5769\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.8962\n",
      "Epoch 00173: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.3843 - accuracy: 0.8962 - val_loss: 0.9385 - val_accuracy: 0.5865\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.9245\n",
      "Epoch 00174: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.2584 - accuracy: 0.9245 - val_loss: 1.0130 - val_accuracy: 0.5962\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.8585\n",
      "Epoch 00175: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.3940 - accuracy: 0.8585 - val_loss: 0.9950 - val_accuracy: 0.6827\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3967 - accuracy: 0.8679\n",
      "Epoch 00176: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.3967 - accuracy: 0.8679 - val_loss: 1.2907 - val_accuracy: 0.6154\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.9151\n",
      "Epoch 00177: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.3009 - accuracy: 0.9151 - val_loss: 1.7662 - val_accuracy: 0.6442\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3709 - accuracy: 0.8962\n",
      "Epoch 00178: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.3709 - accuracy: 0.8962 - val_loss: 1.4773 - val_accuracy: 0.6346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.9057\n",
      "Epoch 00179: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.3099 - accuracy: 0.9057 - val_loss: 1.1431 - val_accuracy: 0.6346\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9151\n",
      "Epoch 00180: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.2424 - accuracy: 0.9151 - val_loss: 1.0050 - val_accuracy: 0.6250\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9057\n",
      "Epoch 00181: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.2509 - accuracy: 0.9057 - val_loss: 1.1690 - val_accuracy: 0.5673\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9340\n",
      "Epoch 00182: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.2267 - accuracy: 0.9340 - val_loss: 1.3404 - val_accuracy: 0.6058\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9245\n",
      "Epoch 00183: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.2381 - accuracy: 0.9245 - val_loss: 1.8451 - val_accuracy: 0.5577\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9340\n",
      "Epoch 00184: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 0.1677 - accuracy: 0.9340 - val_loss: 2.4791 - val_accuracy: 0.5000\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.8962\n",
      "Epoch 00185: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.2157 - accuracy: 0.8962 - val_loss: 2.1381 - val_accuracy: 0.6346\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9151\n",
      "Epoch 00186: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.1864 - accuracy: 0.9151 - val_loss: 2.2982 - val_accuracy: 0.6154\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4691 - accuracy: 0.9057\n",
      "Epoch 00187: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.4691 - accuracy: 0.9057 - val_loss: 1.7532 - val_accuracy: 0.6058\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9245\n",
      "Epoch 00188: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.2897 - accuracy: 0.9245 - val_loss: 1.2442 - val_accuracy: 0.6058\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.8868\n",
      "Epoch 00189: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 0.3291 - accuracy: 0.8868 - val_loss: 0.9574 - val_accuracy: 0.5962\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8491\n",
      "Epoch 00190: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 0.3401 - accuracy: 0.8491 - val_loss: 0.8848 - val_accuracy: 0.6154\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.8774\n",
      "Epoch 00191: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.3360 - accuracy: 0.8774 - val_loss: 0.9957 - val_accuracy: 0.6154\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.8585\n",
      "Epoch 00192: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.3440 - accuracy: 0.8585 - val_loss: 1.2718 - val_accuracy: 0.5481\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9151\n",
      "Epoch 00193: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.2435 - accuracy: 0.9151 - val_loss: 1.5741 - val_accuracy: 0.5288\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9245\n",
      "Epoch 00194: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.2276 - accuracy: 0.9245 - val_loss: 1.7344 - val_accuracy: 0.5865\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9151\n",
      "Epoch 00195: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.1645 - accuracy: 0.9151 - val_loss: 1.9008 - val_accuracy: 0.6635\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9434\n",
      "Epoch 00196: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.1851 - accuracy: 0.9434 - val_loss: 1.9705 - val_accuracy: 0.6154\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9340\n",
      "Epoch 00197: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 183ms/step - loss: 0.1713 - accuracy: 0.9340 - val_loss: 2.1145 - val_accuracy: 0.5673\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9340\n",
      "Epoch 00198: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.1890 - accuracy: 0.9340 - val_loss: 2.1352 - val_accuracy: 0.5769\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9811\n",
      "Epoch 00199: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.0877 - accuracy: 0.9811 - val_loss: 2.0229 - val_accuracy: 0.5865\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9623\n",
      "Epoch 00200: val_accuracy did not improve from 0.71154\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 0.1571 - accuracy: 0.9623 - val_loss: 1.8937 - val_accuracy: 0.6154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ccabce2e80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "chk = ModelCheckpoint('best_model', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(train, train_target, epochs=200, batch_size=32, callbacks=[chk], validation_data=(validation,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-17f8931156b5>:5: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47115384615384615"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the model and checking accuracy on the test data\n",
    "model = load_model('best_model')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_preds = model.predict_classes(test)\n",
    "accuracy_score(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 46],\n",
       "       [ 9, 45]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"TRY1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-5773ecd32795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    947\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ax\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"left_ax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;31m# no non-numeric frames or series allowed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no numeric data to plot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no numeric data to plot"
     ]
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 256)               267264    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 293,065\n",
      "Trainable params: 293,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len=60\n",
    "n_features=4\n",
    "n_outputs=1\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 0.7603 - accuracy: 0.5000 - val_loss: 0.6735 - val_accuracy: 0.6346\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.7259 - accuracy: 0.5377 - val_loss: 0.6931 - val_accuracy: 0.5096\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.6494 - accuracy: 0.5849 - val_loss: 0.8909 - val_accuracy: 0.6346\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6434 - accuracy: 0.6415 - val_loss: 0.6686 - val_accuracy: 0.5481\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.6363 - accuracy: 0.5755 - val_loss: 0.6444 - val_accuracy: 0.5192\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6338 - accuracy: 0.5943 - val_loss: 0.6486 - val_accuracy: 0.6250\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6273 - accuracy: 0.7075 - val_loss: 0.5715 - val_accuracy: 0.7115\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.6837 - accuracy: 0.5189 - val_loss: 0.6482 - val_accuracy: 0.6442\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.7247 - accuracy: 0.5660 - val_loss: 0.6456 - val_accuracy: 0.5769\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6173 - accuracy: 0.6604 - val_loss: 0.8251 - val_accuracy: 0.5000\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6308 - accuracy: 0.5849 - val_loss: 0.6530 - val_accuracy: 0.5962\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6359 - accuracy: 0.6132 - val_loss: 0.8268 - val_accuracy: 0.4327\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.8529 - accuracy: 0.5283 - val_loss: 0.7187 - val_accuracy: 0.5000\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.7134 - accuracy: 0.5849 - val_loss: 0.7371 - val_accuracy: 0.5288\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.7101 - accuracy: 0.5377 - val_loss: 0.7625 - val_accuracy: 0.4327\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.7058 - accuracy: 0.5283 - val_loss: 0.7999 - val_accuracy: 0.5000\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6845 - accuracy: 0.5472 - val_loss: 0.7018 - val_accuracy: 0.5192\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6204 - accuracy: 0.6226 - val_loss: 0.8723 - val_accuracy: 0.4327\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6138 - accuracy: 0.6038 - val_loss: 0.7891 - val_accuracy: 0.4712\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.6122 - accuracy: 0.5849 - val_loss: 1.1936 - val_accuracy: 0.4327\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.5918 - accuracy: 0.6415 - val_loss: 0.8113 - val_accuracy: 0.5481\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.6099 - accuracy: 0.6321 - val_loss: 1.0548 - val_accuracy: 0.4423\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.6128 - accuracy: 0.5566 - val_loss: 0.9321 - val_accuracy: 0.5000\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.5763 - accuracy: 0.6509 - val_loss: 0.9126 - val_accuracy: 0.4519\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.5768 - accuracy: 0.5755 - val_loss: 1.1495 - val_accuracy: 0.4231\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.5773 - accuracy: 0.6038 - val_loss: 0.8920 - val_accuracy: 0.5577\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6718 - accuracy: 0.6226 - val_loss: 0.8295 - val_accuracy: 0.5577\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.5627 - accuracy: 0.6698 - val_loss: 1.0753 - val_accuracy: 0.4519\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.5930 - accuracy: 0.5472 - val_loss: 1.1233 - val_accuracy: 0.4615\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.5657 - accuracy: 0.6604 - val_loss: 0.8655 - val_accuracy: 0.5577\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.5834 - accuracy: 0.6604 - val_loss: 0.8824 - val_accuracy: 0.5673\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.5482 - accuracy: 0.6415 - val_loss: 0.9566 - val_accuracy: 0.5385\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.5549 - accuracy: 0.7170 - val_loss: 1.1060 - val_accuracy: 0.5192\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5748 - accuracy: 0.7075 - val_loss: 1.0018 - val_accuracy: 0.4615\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6047 - accuracy: 0.6226 - val_loss: 0.8348 - val_accuracy: 0.5385\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6415 - accuracy: 0.5755 - val_loss: 0.9565 - val_accuracy: 0.5673\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6104 - accuracy: 0.6038 - val_loss: 0.8677 - val_accuracy: 0.4423\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6054 - accuracy: 0.6132 - val_loss: 0.7256 - val_accuracy: 0.5673\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6205 - accuracy: 0.6132 - val_loss: 0.7412 - val_accuracy: 0.5481\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.6003 - accuracy: 0.6132 - val_loss: 0.7449 - val_accuracy: 0.5385\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.5971 - accuracy: 0.6132 - val_loss: 0.7528 - val_accuracy: 0.5192\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5988 - accuracy: 0.5849 - val_loss: 0.7514 - val_accuracy: 0.5096\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.6080 - accuracy: 0.6038 - val_loss: 0.7679 - val_accuracy: 0.5000\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6287 - accuracy: 0.6415 - val_loss: 0.8169 - val_accuracy: 0.5000\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.6156 - accuracy: 0.6321 - val_loss: 0.7829 - val_accuracy: 0.5096\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6340 - accuracy: 0.6132 - val_loss: 0.7858 - val_accuracy: 0.5000\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6282 - accuracy: 0.5943 - val_loss: 0.7402 - val_accuracy: 0.5865\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6361 - accuracy: 0.6321 - val_loss: 0.7838 - val_accuracy: 0.5000\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.6369 - accuracy: 0.5755 - val_loss: 0.7612 - val_accuracy: 0.4615\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.6478 - accuracy: 0.5660 - val_loss: 0.7534 - val_accuracy: 0.4615\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.6313 - accuracy: 0.5660 - val_loss: 0.7514 - val_accuracy: 0.5385\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6393 - accuracy: 0.6038 - val_loss: 0.7835 - val_accuracy: 0.5000\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.6523 - accuracy: 0.5377 - val_loss: 0.7162 - val_accuracy: 0.5577\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6425 - accuracy: 0.5472 - val_loss: 0.7125 - val_accuracy: 0.5288\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6368 - accuracy: 0.6132 - val_loss: 0.7353 - val_accuracy: 0.4808\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.5967 - accuracy: 0.7075 - val_loss: 0.7973 - val_accuracy: 0.5577\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.6013 - accuracy: 0.6415 - val_loss: 0.7421 - val_accuracy: 0.5481\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.5985 - accuracy: 0.6887 - val_loss: 0.7245 - val_accuracy: 0.5577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 0.5912 - accuracy: 0.6509 - val_loss: 0.6991 - val_accuracy: 0.6154\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.5523 - accuracy: 0.7075 - val_loss: 0.7217 - val_accuracy: 0.5481\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5357 - accuracy: 0.7642 - val_loss: 0.7909 - val_accuracy: 0.6058\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.5079 - accuracy: 0.7453 - val_loss: 0.8001 - val_accuracy: 0.5577\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 0.4929 - accuracy: 0.7547 - val_loss: 1.0266 - val_accuracy: 0.5769\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 0.5143 - accuracy: 0.6981 - val_loss: 1.4147 - val_accuracy: 0.5769\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.7353 - accuracy: 0.7358 - val_loss: 1.1578 - val_accuracy: 0.5096\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 0.6416 - accuracy: 0.7264 - val_loss: 0.7999 - val_accuracy: 0.5385\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.4685 - accuracy: 0.7830 - val_loss: 0.6845 - val_accuracy: 0.6731\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.4974 - accuracy: 0.7925 - val_loss: 0.7287 - val_accuracy: 0.6058\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.5609 - accuracy: 0.7358 - val_loss: 0.7012 - val_accuracy: 0.5865\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.5376 - accuracy: 0.6887 - val_loss: 0.8280 - val_accuracy: 0.5769\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.3936 - accuracy: 0.8491 - val_loss: 0.8774 - val_accuracy: 0.5769\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.5397 - accuracy: 0.7830 - val_loss: 0.7919 - val_accuracy: 0.5673\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.4124 - accuracy: 0.8302 - val_loss: 0.7376 - val_accuracy: 0.6442\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 0.3921 - accuracy: 0.8396 - val_loss: 0.9886 - val_accuracy: 0.5096\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.5454 - accuracy: 0.7547 - val_loss: 0.8197 - val_accuracy: 0.5577\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.5085 - accuracy: 0.8019 - val_loss: 0.9075 - val_accuracy: 0.5385\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 0.5604 - accuracy: 0.7642 - val_loss: 0.7969 - val_accuracy: 0.5385\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.4538 - accuracy: 0.8302 - val_loss: 0.7716 - val_accuracy: 0.5481\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3876 - accuracy: 0.8491 - val_loss: 0.8801 - val_accuracy: 0.5385\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.4003 - accuracy: 0.8774 - val_loss: 0.8818 - val_accuracy: 0.5577\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 0.5153 - accuracy: 0.7736 - val_loss: 0.9380 - val_accuracy: 0.5000\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.5826 - accuracy: 0.6981 - val_loss: 0.9954 - val_accuracy: 0.5096\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.5840 - accuracy: 0.7547 - val_loss: 0.7701 - val_accuracy: 0.5385\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.5368 - accuracy: 0.7736 - val_loss: 0.7778 - val_accuracy: 0.5385\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.8383 - accuracy: 0.4811 - val_loss: 0.7391 - val_accuracy: 0.5000\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.7412 - accuracy: 0.4811 - val_loss: 0.6957 - val_accuracy: 0.5000\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 0.7340 - accuracy: 0.4245 - val_loss: 0.6925 - val_accuracy: 0.5192\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.7087 - accuracy: 0.4906 - val_loss: 0.6981 - val_accuracy: 0.5000\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6950 - accuracy: 0.5377 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6915 - accuracy: 0.5283 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.7012 - accuracy: 0.4717 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.7016 - accuracy: 0.4340 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6971 - accuracy: 0.4717 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.7145 - accuracy: 0.4528 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6929 - accuracy: 0.4906 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6959 - accuracy: 0.4906 - val_loss: 0.6962 - val_accuracy: 0.5000\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.7038 - accuracy: 0.4906 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6919 - accuracy: 0.5189 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6917 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6957 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6944 - accuracy: 0.4340 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6937 - accuracy: 0.4340 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6939 - accuracy: 0.4623 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6931 - accuracy: 0.4623 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6927 - accuracy: 0.4811 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6979 - accuracy: 0.5189 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.7316 - accuracy: 0.5283 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6938 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6939 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6951 - accuracy: 0.4906 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.6923 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.6938 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6943 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6924 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6925 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6954 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6921 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6920 - accuracy: 0.5189 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6926 - accuracy: 0.5189 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6949 - accuracy: 0.4717 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6950 - accuracy: 0.4811 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6930 - accuracy: 0.5189 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 166/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6921 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6912 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6958 - accuracy: 0.5094 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6965 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6919 - accuracy: 0.5094 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.6989 - accuracy: 0.5094 - val_loss: 0.6955 - val_accuracy: 0.5000\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6955 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6964 - accuracy: 0.4151 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6943 - accuracy: 0.4340 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6929 - accuracy: 0.5189 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6927 - accuracy: 0.5189 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6937 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.6938 - accuracy: 0.5189 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6972 - accuracy: 0.4717 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6879 - accuracy: 0.5566 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6965 - accuracy: 0.5189 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.7114 - accuracy: 0.4623 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6895 - accuracy: 0.5660 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 248/500\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6938 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6928 - accuracy: 0.5189 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6948 - accuracy: 0.4906 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6957 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 287/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6918 - accuracy: 0.5189 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.4906 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6947 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6926 - accuracy: 0.5189 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6989 - accuracy: 0.4906 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6938 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6925 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6938 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6961 - accuracy: 0.4717 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6928 - accuracy: 0.5189 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6926 - accuracy: 0.5283 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6947 - accuracy: 0.4623 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6936 - accuracy: 0.4717 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6935 - accuracy: 0.4811 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 330/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6936 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6939 - accuracy: 0.4906 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6937 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6935 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6935 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6935 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6939 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6933 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6937 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6935 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6937 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6935 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6937 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6934 - accuracy: 0.3396 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6935 - accuracy: 0.4811 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6933 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6933 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6934 - accuracy: 0.4811 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6933 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6930 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 0.6942 - accuracy: 0.3396 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6933 - accuracy: 0.4528 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6933 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6937 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6933 - accuracy: 0.4906 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6934 - accuracy: 0.4811 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6937 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.3962 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5189 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 401/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.6925 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 412/500\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6939 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 2s 394ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 494/500\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 0.6935 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "0.48076921701431274\n"
     ]
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 1, 500, 20\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(train, train_target, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_data=(validation, validation_target))\n",
    "# evaluate model\n",
    "_,accuracy = model.evaluate(test, test_target, batch_size=batch_size, verbose=0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY_3: USE LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(seq_len, n_features)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,101\n",
      "Trainable params: 42,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "22/22 [==============================] - 2s 83ms/step - loss: 1.4975 - val_loss: 0.8056\n",
      "Epoch 2/600\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.6861 - val_loss: 0.7270\n",
      "Epoch 3/600\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.6436 - val_loss: 0.6854\n",
      "Epoch 4/600\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.6454 - val_loss: 0.8064\n",
      "Epoch 5/600\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.6235 - val_loss: 0.7226\n",
      "Epoch 6/600\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.6187 - val_loss: 1.0206\n",
      "Epoch 7/600\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.5124 - val_loss: 1.6943\n",
      "Epoch 8/600\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 1.2527 - val_loss: 4.4533\n",
      "Epoch 9/600\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 2.8725 - val_loss: 6.8319\n",
      "Epoch 10/600\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 3.5578 - val_loss: 5.5707\n",
      "Epoch 11/600\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 3.7797 - val_loss: 4.2031\n",
      "Epoch 12/600\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 3.7874 - val_loss: 4.3416\n",
      "Epoch 13/600\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 3.7591 - val_loss: 4.7861\n",
      "Epoch 14/600\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 3.7604 - val_loss: 4.3124\n",
      "Epoch 15/600\n",
      "22/22 [==============================] - 1s 55ms/step - loss: 4.2012 - val_loss: 4.8711\n",
      "Epoch 16/600\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 4.7104 - val_loss: 4.4913\n",
      "Epoch 17/600\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 4.0413 - val_loss: 4.8481\n",
      "Epoch 18/600\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.9083 - val_loss: 4.7749\n",
      "Epoch 19/600\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 3.8795 - val_loss: 4.7226\n",
      "Epoch 20/600\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9897 - val_loss: 5.0559\n",
      "Epoch 21/600\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 3.7923 - val_loss: 5.2853\n",
      "Epoch 22/600\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.7564 - val_loss: 5.1852\n",
      "Epoch 23/600\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.9022 - val_loss: 6.4369\n",
      "Epoch 24/600\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.7614 - val_loss: 4.6522\n",
      "Epoch 25/600\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7534 - val_loss: 5.1203\n",
      "Epoch 26/600\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 3.7498 - val_loss: 6.8855\n",
      "Epoch 27/600\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1827 - val_loss: 8.6175\n",
      "Epoch 28/600\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6122 - val_loss: 7.6993\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27d60bf6eb0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, \n",
    "          train_target, \n",
    "          epochs=600, \n",
    "          batch_size=5, \n",
    "          callbacks=[early_stop], \n",
    "          validation_data=(validation,validation_target),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 35],\n",
       "       [ 1, 53]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Lambda, Input, Dropout, Flatten, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps=60\n",
    "n_features=4\n",
    "n_outputs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "x = Input((n_timesteps, n_features))\n",
    "\n",
    "# LSTM\n",
    "lstm_1 = LSTM(150, activation='relu', input_shape=(n_timesteps, n_features))(x)\n",
    "lstm_2 = Dropout(0.5)(lstm_1)\n",
    "lstm_3 = Dense(150, activation='relu')(lstm_2)\n",
    "lstm_4 = Dense(n_outputs, activation='softmax', name='lstm_out')(lstm_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(x, \n",
    "           lstm_4,\n",
    "           name=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_delta_val = 0.01\n",
    "lr_cb = ReduceLROnPlateau(monitor = 'val_auc', mode='max', \n",
    "                          factor = 0.5, min_delta = min_delta_val, patience = 3, verbose = 1)\n",
    "es_cb = EarlyStopping(monitor = 'val_auc', mode='max', \n",
    "                      min_delta=min_delta_val, patience = 10, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "default_callbacks = [lr_cb, es_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200\n",
    "batch_size=1000\n",
    "validation_split_on_training = 0.2\n",
    "# train model\n",
    "model.fit(train,\n",
    "          train_target, epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation, validation_target),\n",
    "          #validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "          callbacks=default_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 60, 4), (104, 60, 4))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len, 4)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 0.7819 - accuracy: 0.4717 - val_loss: 0.6909 - val_accuracy: 0.5962\n",
      "Epoch 2/600\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.7192 - accuracy: 0.4434 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 3/600\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.7716 - accuracy: 0.5283 - val_loss: 0.7008 - val_accuracy: 0.5000\n",
      "Epoch 4/600\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.8342 - accuracy: 0.3962 - val_loss: 0.6853 - val_accuracy: 0.5577\n",
      "Epoch 5/600\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.7420 - accuracy: 0.5283 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 6/600\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.7042 - accuracy: 0.5472 - val_loss: 0.7036 - val_accuracy: 0.5000\n",
      "Epoch 7/600\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.7999 - accuracy: 0.4340 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
      "Epoch 8/600\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.7283 - accuracy: 0.5094 - val_loss: 0.7004 - val_accuracy: 0.5000\n",
      "Epoch 9/600\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.7615 - accuracy: 0.4623 - val_loss: 0.6948 - val_accuracy: 0.5000\n",
      "Epoch 10/600\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.7079 - accuracy: 0.4811 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 11/600\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.6923 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 12/600\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.7071 - accuracy: 0.4623 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 13/600\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.7081 - accuracy: 0.4623 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 14/600\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.7009 - accuracy: 0.5189 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 15/600\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.7170 - accuracy: 0.4340 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 16/600\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.7095 - accuracy: 0.4434 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 17/600\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 0.6780 - accuracy: 0.5377 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 18/600\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.6932 - accuracy: 0.4623 - val_loss: 0.6993 - val_accuracy: 0.5000\n",
      "Epoch 19/600\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.7038 - accuracy: 0.5660 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 20/600\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.7246 - accuracy: 0.4623 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 21/600\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.6906 - accuracy: 0.5189 - val_loss: 0.6982 - val_accuracy: 0.5000\n",
      "Epoch 22/600\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.7088 - accuracy: 0.4340 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 23/600\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 0.7392 - accuracy: 0.4623 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 24/600\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ccd5967490>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "chk = ModelCheckpoint('best_model', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(train, train_target, epochs=600, batch_size=32, callbacks=[early_stop], validation_data=(validation,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0],\n",
       "       [54,  0]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test=test1.reshape(1,60,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(single_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
