{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./MovementAAL/dataset/MovementAAL_RSS_1.csv')\n",
    "df2 = pd.read_csv('./MovementAAL/dataset/MovementAAL_RSS_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.90476</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.28571</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.38095</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.14286</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.28571</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.47619</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.90476         -0.48       0.28571          0.30\n",
       "1      -0.57143         -0.32       0.14286          0.30\n",
       "2      -0.38095         -0.28      -0.14286          0.35\n",
       "3      -0.28571         -0.20      -0.47619          0.35\n",
       "4      -0.14286         -0.20       0.14286         -0.20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.76190</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.85714</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.57143         -0.20       0.71429          0.50\n",
       "1      -0.76190         -0.48       0.76190         -0.25\n",
       "2      -0.85714         -0.60       0.85714          0.55\n",
       "3      -0.76190         -0.40       0.71429          0.60\n",
       "4      -0.76190         -0.84       0.85714          0.45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MovementAAL/dataset/MovementAAL_RSS_1.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_2.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_3.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_4.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_5.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_6.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_7.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_8.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_9.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_10.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_11.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_12.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_13.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_14.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_15.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_16.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_17.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_18.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_19.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_20.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_21.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_22.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_23.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_24.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_25.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_26.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_27.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_28.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_29.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_30.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_31.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_32.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_33.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_34.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_35.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_36.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_37.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_38.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_39.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_40.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_41.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_42.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_43.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_44.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_45.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_46.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_47.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_48.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_49.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_50.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_51.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_52.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_53.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_54.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_55.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_56.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_57.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_58.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_59.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_60.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_61.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_62.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_63.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_64.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_65.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_66.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_67.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_68.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_69.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_70.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_71.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_72.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_73.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_74.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_75.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_76.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_77.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_78.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_79.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_80.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_81.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_82.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_83.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_84.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_85.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_86.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_87.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_88.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_89.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_90.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_91.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_92.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_93.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_94.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_95.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_96.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_97.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_98.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_99.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_100.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_101.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_102.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_103.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_104.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_105.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_106.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_107.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_108.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_109.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_110.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_111.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_112.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_113.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_114.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_115.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_116.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_117.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_118.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_119.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_120.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_121.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_122.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_123.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_124.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_125.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_126.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_127.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_128.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_129.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_130.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_131.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_132.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_133.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_134.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_135.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_136.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_137.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_138.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_139.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_140.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_141.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_142.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_143.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_144.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_145.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_146.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_147.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_148.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_149.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_150.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_151.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_152.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_153.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_154.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_155.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_156.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_157.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_158.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_159.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_160.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_161.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_162.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_163.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_164.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_165.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_166.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_167.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_168.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_169.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_170.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_171.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_172.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_173.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_174.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_175.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_176.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_177.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_178.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_179.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_180.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_181.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_182.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_183.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_184.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_185.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_186.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_187.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_188.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_189.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_190.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_191.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_192.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_193.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_194.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_195.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_196.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_197.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_198.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_199.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_200.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_201.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_202.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_203.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_204.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_205.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_206.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_207.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_208.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_209.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_210.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_211.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_212.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_213.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_214.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_215.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_216.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_217.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_218.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_219.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_220.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_221.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_222.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_223.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_224.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_225.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_226.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_227.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_228.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_229.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_230.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_231.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_232.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_233.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_234.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_235.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_236.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_237.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_238.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MovementAAL/dataset/MovementAAL_RSS_239.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_240.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_241.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_242.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_243.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_244.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_245.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_246.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_247.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_248.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_249.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_250.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_251.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_252.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_253.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_254.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_255.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_256.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_257.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_258.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_259.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_260.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_261.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_262.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_263.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_264.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_265.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_266.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_267.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_268.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_269.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_270.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_271.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_272.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_273.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_274.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_275.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_276.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_277.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_278.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_279.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_280.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_281.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_282.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_283.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_284.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_285.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_286.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_287.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_288.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_289.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_290.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_291.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_292.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_293.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_294.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_295.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_296.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_297.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_298.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_299.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_300.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_301.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_302.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_303.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_304.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_305.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_306.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_307.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_308.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_309.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_310.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_311.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_312.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_313.csv\n",
      "./MovementAAL/dataset/MovementAAL_RSS_314.csv\n"
     ]
    }
   ],
   "source": [
    "path = './MovementAAL/dataset/MovementAAL_RSS_'\n",
    "sequences = list()\n",
    "for i in range(1,315):\n",
    "    file_path = path + str(i) + '.csv'\n",
    "    print(file_path)\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    values = df.values\n",
    "    sequences.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('./MovementAAL/dataset/MovementAAL_target.csv')\n",
    "targets = targets.values[:,1]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences[313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = pd.read_csv('./MovementAAL/groups/MovementAAL_DatasetGroup.csv', header=0)\n",
    "groups = groups.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    314.000000\n",
       "mean      42.028662\n",
       "std       16.185303\n",
       "min       19.000000\n",
       "25%       26.000000\n",
       "50%       41.000000\n",
       "75%       56.000000\n",
       "max      129.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sequences = []\n",
    "for one_seq in sequences:\n",
    "    len_sequences.append(len(one_seq))\n",
    "pd.Series(len_sequences).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the sequence with the values in last row to max length\n",
    "to_pad = 129\n",
    "new_seq = []\n",
    "for one_seq in sequences:\n",
    "    len_one_seq = len(one_seq)\n",
    "    last_val = one_seq[-1]\n",
    "    n = to_pad - len_one_seq\n",
    "   \n",
    "    to_concat = np.repeat(one_seq[-1], n).reshape(4, n).transpose()\n",
    "    new_one_seq = np.concatenate([one_seq, to_concat])\n",
    "    new_seq.append(new_one_seq)\n",
    "final_seq = np.stack(new_seq)\n",
    "\n",
    "#truncate the sequence to length 60\n",
    "from keras.preprocessing import sequence\n",
    "seq_len = 60\n",
    "final_seq=sequence.pad_sequences(final_seq, maxlen=seq_len, padding='post', dtype='float', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 60, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [final_seq[i] for i in range(len(groups)) if (groups[i]==2)]\n",
    "validation = [final_seq[i] for i in range(len(groups)) if groups[i]==1]\n",
    "test = [final_seq[i] for i in range(len(groups)) if groups[i]==3]\n",
    "\n",
    "train_target = [targets[i] for i in range(len(groups)) if (groups[i]==2)]\n",
    "validation_target = [targets[i] for i in range(len(groups)) if groups[i]==1]\n",
    "test_target = [targets[i] for i in range(len(groups)) if groups[i]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "validation = np.array(validation)\n",
    "test = np.array(test)\n",
    "\n",
    "train_target = np.array(train_target)\n",
    "train_target = (train_target+1)/2\n",
    "\n",
    "validation_target = np.array(validation_target)\n",
    "validation_target = (validation_target+1)/2\n",
    "\n",
    "test_target = np.array(test_target)\n",
    "test_target = (test_target+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 60, 4), (104, 60, 4), (104, 60, 4))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY1: Building time series classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len, 4)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               267264    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 293,065\n",
      "Trainable params: 293,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7320 - accuracy: 0.5283\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.47115, saving model to best_model\n",
      "WARNING:tensorflow:From c:\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.7320 - accuracy: 0.5283 - val_loss: 2.0097 - val_accuracy: 0.4712\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.1590 - accuracy: 0.5094\n",
      "Epoch 00002: val_accuracy did not improve from 0.47115\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1.1590 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.4615\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6965 - accuracy: 0.4717\n",
      "Epoch 00003: val_accuracy improved from 0.47115 to 0.55769, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.6965 - accuracy: 0.4717 - val_loss: 0.6873 - val_accuracy: 0.5577\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.5660\n",
      "Epoch 00004: val_accuracy did not improve from 0.55769\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.6822 - accuracy: 0.5660 - val_loss: 0.6921 - val_accuracy: 0.5000\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6993 - accuracy: 0.4717\n",
      "Epoch 00005: val_accuracy did not improve from 0.55769\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6993 - accuracy: 0.4717 - val_loss: 0.6960 - val_accuracy: 0.5000\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7206 - accuracy: 0.5000\n",
      "Epoch 00006: val_accuracy did not improve from 0.55769\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.7206 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7131 - accuracy: 0.4340\n",
      "Epoch 00007: val_accuracy improved from 0.55769 to 0.65385, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.7131 - accuracy: 0.4340 - val_loss: 0.6897 - val_accuracy: 0.6538\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6971 - accuracy: 0.4717\n",
      "Epoch 00008: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.6971 - accuracy: 0.4717 - val_loss: 0.6972 - val_accuracy: 0.5000\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.5094\n",
      "Epoch 00009: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6957 - accuracy: 0.5094 - val_loss: 0.6970 - val_accuracy: 0.5000\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.5094\n",
      "Epoch 00010: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6950 - accuracy: 0.5094 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.5189\n",
      "Epoch 00011: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6945 - accuracy: 0.5189 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.5283\n",
      "Epoch 00012: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.6901 - accuracy: 0.5283 - val_loss: 0.6919 - val_accuracy: 0.6250\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5472\n",
      "Epoch 00013: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.6912 - accuracy: 0.5472 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.5660\n",
      "Epoch 00014: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6868 - accuracy: 0.5660 - val_loss: 0.6909 - val_accuracy: 0.5000\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6990 - accuracy: 0.5000\n",
      "Epoch 00015: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6990 - accuracy: 0.5000 - val_loss: 0.6898 - val_accuracy: 0.5192\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6965 - accuracy: 0.4811\n",
      "Epoch 00016: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.6965 - accuracy: 0.4811 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5094\n",
      "Epoch 00017: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6921 - accuracy: 0.5094 - val_loss: 0.6901 - val_accuracy: 0.5000\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6922 - accuracy: 0.5377\n",
      "Epoch 00018: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6922 - accuracy: 0.5377 - val_loss: 0.6884 - val_accuracy: 0.6250\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.5755\n",
      "Epoch 00019: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6880 - accuracy: 0.5755 - val_loss: 0.6870 - val_accuracy: 0.5288\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5472\n",
      "Epoch 00020: val_accuracy did not improve from 0.65385\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.6818 - accuracy: 0.5472 - val_loss: 0.6823 - val_accuracy: 0.5769\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6914 - accuracy: 0.5472\n",
      "Epoch 00021: val_accuracy improved from 0.65385 to 0.66346, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6914 - accuracy: 0.5472 - val_loss: 0.6710 - val_accuracy: 0.6635\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.5472\n",
      "Epoch 00022: val_accuracy did not improve from 0.66346\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6828 - accuracy: 0.5472 - val_loss: 0.6572 - val_accuracy: 0.6635\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.5660\n",
      "Epoch 00023: val_accuracy did not improve from 0.66346\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6630 - accuracy: 0.5660 - val_loss: 0.6910 - val_accuracy: 0.5096\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7056 - accuracy: 0.5283\n",
      "Epoch 00024: val_accuracy did not improve from 0.66346\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.7056 - accuracy: 0.5283 - val_loss: 0.6751 - val_accuracy: 0.5192\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.6038\n",
      "Epoch 00025: val_accuracy improved from 0.66346 to 0.67308, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6782 - accuracy: 0.6038 - val_loss: 0.6580 - val_accuracy: 0.6731\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6388 - accuracy: 0.6604\n",
      "Epoch 00026: val_accuracy improved from 0.67308 to 0.70192, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6388 - accuracy: 0.6604 - val_loss: 0.5883 - val_accuracy: 0.7019\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7635 - accuracy: 0.5283\n",
      "Epoch 00027: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.7635 - accuracy: 0.5283 - val_loss: 0.6671 - val_accuracy: 0.4904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.4906\n",
      "Epoch 00028: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6959 - accuracy: 0.4906 - val_loss: 0.6850 - val_accuracy: 0.5192\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7080 - accuracy: 0.5283\n",
      "Epoch 00029: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.7080 - accuracy: 0.5283 - val_loss: 0.6830 - val_accuracy: 0.6538\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.5566\n",
      "Epoch 00030: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6877 - accuracy: 0.5566 - val_loss: 0.6927 - val_accuracy: 0.5000\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.5189\n",
      "Epoch 00031: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6909 - accuracy: 0.5189 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.5472\n",
      "Epoch 00032: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6822 - accuracy: 0.5472 - val_loss: 0.6867 - val_accuracy: 0.5000\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.6038\n",
      "Epoch 00033: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6868 - accuracy: 0.6038 - val_loss: 0.6849 - val_accuracy: 0.6731\n",
      "Epoch 34/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6880 - accuracy: 0.5104\n",
      "Epoch 00034: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6865 - accuracy: 0.5189 - val_loss: 0.6825 - val_accuracy: 0.6923\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.5660\n",
      "Epoch 00035: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6784 - accuracy: 0.5660 - val_loss: 0.6740 - val_accuracy: 0.6250\n",
      "Epoch 36/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6830 - accuracy: 0.5208\n",
      "Epoch 00036: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.6845 - accuracy: 0.5189 - val_loss: 0.6639 - val_accuracy: 0.6827\n",
      "Epoch 37/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6838 - accuracy: 0.5521\n",
      "Epoch 00037: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6787 - accuracy: 0.5566 - val_loss: 0.6614 - val_accuracy: 0.6346\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6613 - accuracy: 0.6226\n",
      "Epoch 00038: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.6613 - accuracy: 0.6226 - val_loss: 0.6717 - val_accuracy: 0.4904\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6526 - accuracy: 0.5849\n",
      "Epoch 00039: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6526 - accuracy: 0.5849 - val_loss: 0.6460 - val_accuracy: 0.6538\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6270 - accuracy: 0.7075\n",
      "Epoch 00040: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6270 - accuracy: 0.7075 - val_loss: 0.6934 - val_accuracy: 0.5577\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5871 - accuracy: 0.7075\n",
      "Epoch 00041: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.5871 - accuracy: 0.7075 - val_loss: 1.1957 - val_accuracy: 0.5577\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8275 - accuracy: 0.5566\n",
      "Epoch 00042: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.8275 - accuracy: 0.5566 - val_loss: 0.6954 - val_accuracy: 0.5192\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.4811\n",
      "Epoch 00043: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.7057 - accuracy: 0.4811 - val_loss: 0.6986 - val_accuracy: 0.5000\n",
      "Epoch 44/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6867 - accuracy: 0.5521\n",
      "Epoch 00044: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.7035 - accuracy: 0.5094 - val_loss: 0.6990 - val_accuracy: 0.5000\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6905 - accuracy: 0.5189\n",
      "Epoch 00045: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6905 - accuracy: 0.5189 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.5189\n",
      "Epoch 00046: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.6942 - accuracy: 0.5189 - val_loss: 0.6898 - val_accuracy: 0.5577\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.4811\n",
      "Epoch 00047: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.6929 - accuracy: 0.4811 - val_loss: 0.6879 - val_accuracy: 0.5962\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5189\n",
      "Epoch 00048: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6931 - accuracy: 0.5189 - val_loss: 0.6847 - val_accuracy: 0.4615\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6896 - accuracy: 0.5849\n",
      "Epoch 00049: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6896 - accuracy: 0.5849 - val_loss: 0.6828 - val_accuracy: 0.5865\n",
      "Epoch 50/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7047 - accuracy: 0.5312\n",
      "Epoch 00050: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6989 - accuracy: 0.5566 - val_loss: 0.6753 - val_accuracy: 0.5673\n",
      "Epoch 51/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6658 - accuracy: 0.6042\n",
      "Epoch 00051: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.6709 - accuracy: 0.5849 - val_loss: 0.6790 - val_accuracy: 0.5865\n",
      "Epoch 52/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6418 - accuracy: 0.5833\n",
      "Epoch 00052: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6438 - accuracy: 0.5943 - val_loss: 0.6845 - val_accuracy: 0.5192\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7093 - accuracy: 0.4717\n",
      "Epoch 00053: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.7093 - accuracy: 0.4717 - val_loss: 0.6830 - val_accuracy: 0.5288\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6804 - accuracy: 0.5660\n",
      "Epoch 00054: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.6804 - accuracy: 0.5660 - val_loss: 0.6747 - val_accuracy: 0.5962\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.5283\n",
      "Epoch 00055: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6893 - accuracy: 0.5283 - val_loss: 0.6737 - val_accuracy: 0.5865\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5472\n",
      "Epoch 00056: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6912 - accuracy: 0.5472 - val_loss: 0.6995 - val_accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.5755\n",
      "Epoch 00057: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6821 - accuracy: 0.5755 - val_loss: 0.6651 - val_accuracy: 0.6731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7312 - accuracy: 0.4906\n",
      "Epoch 00058: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.7312 - accuracy: 0.4906 - val_loss: 0.6653 - val_accuracy: 0.6731\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6585 - accuracy: 0.6226\n",
      "Epoch 00059: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6585 - accuracy: 0.6226 - val_loss: 0.6971 - val_accuracy: 0.5000\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6758 - accuracy: 0.5283\n",
      "Epoch 00060: val_accuracy did not improve from 0.70192\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6758 - accuracy: 0.5283 - val_loss: 0.6683 - val_accuracy: 0.5577\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6667 - accuracy: 0.6226\n",
      "Epoch 00061: val_accuracy improved from 0.70192 to 0.72115, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6667 - accuracy: 0.6226 - val_loss: 0.6267 - val_accuracy: 0.7212\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6387 - accuracy: 0.6509\n",
      "Epoch 00062: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6387 - accuracy: 0.6509 - val_loss: 0.6659 - val_accuracy: 0.6250\n",
      "Epoch 63/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6926 - accuracy: 0.5625\n",
      "Epoch 00063: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6894 - accuracy: 0.5660 - val_loss: 0.7967 - val_accuracy: 0.5000\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6683 - accuracy: 0.5849\n",
      "Epoch 00064: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.6683 - accuracy: 0.5849 - val_loss: 0.6284 - val_accuracy: 0.7019\n",
      "Epoch 65/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6223 - accuracy: 0.6354\n",
      "Epoch 00065: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 0.6138 - accuracy: 0.6509 - val_loss: 0.6497 - val_accuracy: 0.6442\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.6321\n",
      "Epoch 00066: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.5728 - accuracy: 0.6321 - val_loss: 0.7926 - val_accuracy: 0.6154\n",
      "Epoch 67/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6805 - accuracy: 0.6146\n",
      "Epoch 00067: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.6942 - accuracy: 0.5943 - val_loss: 0.6377 - val_accuracy: 0.6154\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7044 - accuracy: 0.4906\n",
      "Epoch 00068: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.7044 - accuracy: 0.4906 - val_loss: 0.6778 - val_accuracy: 0.5769\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.5660\n",
      "Epoch 00069: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6895 - accuracy: 0.5660 - val_loss: 0.7139 - val_accuracy: 0.5000\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7056 - accuracy: 0.5189\n",
      "Epoch 00070: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.7056 - accuracy: 0.5189 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 71/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6866 - accuracy: 0.5417\n",
      "Epoch 00071: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.6863 - accuracy: 0.5472 - val_loss: 0.6872 - val_accuracy: 0.6923\n",
      "Epoch 72/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6895 - accuracy: 0.5000\n",
      "Epoch 00072: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6892 - accuracy: 0.5000 - val_loss: 0.6838 - val_accuracy: 0.5769\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.5377\n",
      "Epoch 00073: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.6880 - accuracy: 0.5377 - val_loss: 0.6780 - val_accuracy: 0.5962\n",
      "Epoch 74/200\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6686 - accuracy: 0.6562\n",
      "Epoch 00074: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.6636 - accuracy: 0.6604 - val_loss: 0.6664 - val_accuracy: 0.6923\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.6321\n",
      "Epoch 00075: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.6537 - accuracy: 0.6321 - val_loss: 0.6414 - val_accuracy: 0.6538\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.5094\n",
      "Epoch 00076: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6919 - accuracy: 0.5094 - val_loss: 0.6767 - val_accuracy: 0.5288\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6402 - accuracy: 0.6132\n",
      "Epoch 00077: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6402 - accuracy: 0.6132 - val_loss: 0.6169 - val_accuracy: 0.6731\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.6321\n",
      "Epoch 00078: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.6484 - accuracy: 0.6321 - val_loss: 0.6663 - val_accuracy: 0.5192\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6455 - accuracy: 0.6038\n",
      "Epoch 00079: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6455 - accuracy: 0.6038 - val_loss: 0.6192 - val_accuracy: 0.6731\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.6509\n",
      "Epoch 00080: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6396 - accuracy: 0.6509 - val_loss: 0.6533 - val_accuracy: 0.5673\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.6981 ETA: 0s - loss: 0.6240 - accuracy: 0.67\n",
      "Epoch 00081: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.6093 - accuracy: 0.6981 - val_loss: 0.6258 - val_accuracy: 0.6538\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.6509\n",
      "Epoch 00082: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.6771 - accuracy: 0.6509 - val_loss: 0.6720 - val_accuracy: 0.5962\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.6415\n",
      "Epoch 00083: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6356 - accuracy: 0.6415 - val_loss: 0.7865 - val_accuracy: 0.4808\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.6226\n",
      "Epoch 00084: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.6166 - accuracy: 0.6226 - val_loss: 0.6289 - val_accuracy: 0.6250\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6478 - accuracy: 0.6509\n",
      "Epoch 00085: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6478 - accuracy: 0.6509 - val_loss: 0.6211 - val_accuracy: 0.6731\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6331 - accuracy: 0.6509\n",
      "Epoch 00086: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.6331 - accuracy: 0.6509 - val_loss: 0.6829 - val_accuracy: 0.6250\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.6981\n",
      "Epoch 00087: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5682 - accuracy: 0.6981 - val_loss: 0.8629 - val_accuracy: 0.5769\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.7642\n",
      "Epoch 00088: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.5130 - accuracy: 0.7642 - val_loss: 0.9851 - val_accuracy: 0.6058\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.7358\n",
      "Epoch 00089: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5074 - accuracy: 0.7358 - val_loss: 1.3396 - val_accuracy: 0.6154\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.7830\n",
      "Epoch 00090: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.4998 - accuracy: 0.7830 - val_loss: 1.4185 - val_accuracy: 0.6154\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4796 - accuracy: 0.7642\n",
      "Epoch 00091: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4796 - accuracy: 0.7642 - val_loss: 0.8082 - val_accuracy: 0.6442\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6805 - accuracy: 0.5566\n",
      "Epoch 00092: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6805 - accuracy: 0.5566 - val_loss: 0.8658 - val_accuracy: 0.6154\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5899 - accuracy: 0.6132\n",
      "Epoch 00093: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.5899 - accuracy: 0.6132 - val_loss: 1.0001 - val_accuracy: 0.6154\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.7453\n",
      "Epoch 00094: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5084 - accuracy: 0.7453 - val_loss: 1.4738 - val_accuracy: 0.6250\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5802 - accuracy: 0.6981\n",
      "Epoch 00095: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5802 - accuracy: 0.6981 - val_loss: 0.6974 - val_accuracy: 0.6346\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.5283\n",
      "Epoch 00096: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6751 - accuracy: 0.5283 - val_loss: 0.6177 - val_accuracy: 0.6250\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7108 - accuracy: 0.4906\n",
      "Epoch 00097: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.7108 - accuracy: 0.4906 - val_loss: 0.6165 - val_accuracy: 0.6058\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6724 - accuracy: 0.5849\n",
      "Epoch 00098: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6724 - accuracy: 0.5849 - val_loss: 0.6604 - val_accuracy: 0.6827\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6685 - accuracy: 0.5849\n",
      "Epoch 00099: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6685 - accuracy: 0.5849 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.5660 ETA: 0s - loss: 0.6532 - accuracy: 0.55\n",
      "Epoch 00100: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.6420 - accuracy: 0.5660 - val_loss: 0.8048 - val_accuracy: 0.5000\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.5943\n",
      "Epoch 00101: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6213 - accuracy: 0.5943 - val_loss: 0.7620 - val_accuracy: 0.7115\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6308 - accuracy: 0.6415\n",
      "Epoch 00102: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.6308 - accuracy: 0.6415 - val_loss: 0.6095 - val_accuracy: 0.6154\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.5000\n",
      "Epoch 00103: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.7090 - accuracy: 0.5000 - val_loss: 0.6464 - val_accuracy: 0.5673\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7060 - accuracy: 0.5000\n",
      "Epoch 00104: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.7060 - accuracy: 0.5000 - val_loss: 0.6681 - val_accuracy: 0.5769\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.4811\n",
      "Epoch 00105: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6897 - accuracy: 0.4811 - val_loss: 0.6770 - val_accuracy: 0.6442\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5189\n",
      "Epoch 00106: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.6935 - accuracy: 0.5189 - val_loss: 0.6752 - val_accuracy: 0.6827\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6963 - accuracy: 0.5189\n",
      "Epoch 00107: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6963 - accuracy: 0.5189 - val_loss: 0.6729 - val_accuracy: 0.6154\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.6132\n",
      "Epoch 00108: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6822 - accuracy: 0.6132 - val_loss: 0.6864 - val_accuracy: 0.5000\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.5094\n",
      "Epoch 00109: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6857 - accuracy: 0.5094 - val_loss: 0.6712 - val_accuracy: 0.6442\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6700 - accuracy: 0.5943\n",
      "Epoch 00110: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6700 - accuracy: 0.5943 - val_loss: 0.6642 - val_accuracy: 0.6346\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.5472\n",
      "Epoch 00111: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6878 - accuracy: 0.5472 - val_loss: 0.6660 - val_accuracy: 0.6154\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.7264\n",
      "Epoch 00112: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6481 - accuracy: 0.7264 - val_loss: 0.6386 - val_accuracy: 0.6635\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6666 - accuracy: 0.5943\n",
      "Epoch 00113: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6666 - accuracy: 0.5943 - val_loss: 0.6217 - val_accuracy: 0.6250\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6546 - accuracy: 0.6415\n",
      "Epoch 00114: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.6546 - accuracy: 0.6415 - val_loss: 0.6369 - val_accuracy: 0.5769\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.6604\n",
      "Epoch 00115: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6288 - accuracy: 0.6604 - val_loss: 0.5916 - val_accuracy: 0.6635\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.7453\n",
      "Epoch 00116: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.5824 - accuracy: 0.7453 - val_loss: 0.5908 - val_accuracy: 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.7170\n",
      "Epoch 00117: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5568 - accuracy: 0.7170 - val_loss: 0.6169 - val_accuracy: 0.6827\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.7925\n",
      "Epoch 00118: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.5055 - accuracy: 0.7925 - val_loss: 0.8491 - val_accuracy: 0.6250\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5616 - accuracy: 0.7264\n",
      "Epoch 00119: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.5616 - accuracy: 0.7264 - val_loss: 0.6244 - val_accuracy: 0.6827\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5115 - accuracy: 0.7547\n",
      "Epoch 00120: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5115 - accuracy: 0.7547 - val_loss: 0.7608 - val_accuracy: 0.6827\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5489 - accuracy: 0.7264\n",
      "Epoch 00121: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.5489 - accuracy: 0.7264 - val_loss: 0.7537 - val_accuracy: 0.6250\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4867 - accuracy: 0.7170\n",
      "Epoch 00122: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.4867 - accuracy: 0.7170 - val_loss: 0.8899 - val_accuracy: 0.6346\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4311 - accuracy: 0.7925\n",
      "Epoch 00123: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.4311 - accuracy: 0.7925 - val_loss: 0.7908 - val_accuracy: 0.6635\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.8019\n",
      "Epoch 00124: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4489 - accuracy: 0.8019 - val_loss: 1.8628 - val_accuracy: 0.5288\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.7642\n",
      "Epoch 00125: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.6135 - accuracy: 0.7642 - val_loss: 0.9198 - val_accuracy: 0.5865\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4649 - accuracy: 0.8208\n",
      "Epoch 00126: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.4649 - accuracy: 0.8208 - val_loss: 0.7714 - val_accuracy: 0.5481\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4562 - accuracy: 0.8208\n",
      "Epoch 00127: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4562 - accuracy: 0.8208 - val_loss: 0.7847 - val_accuracy: 0.5385\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.8302\n",
      "Epoch 00128: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4134 - accuracy: 0.8302 - val_loss: 0.8650 - val_accuracy: 0.5769\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8208\n",
      "Epoch 00129: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.3974 - accuracy: 0.8208 - val_loss: 1.0719 - val_accuracy: 0.6154\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.8491\n",
      "Epoch 00130: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.3525 - accuracy: 0.8491 - val_loss: 0.8175 - val_accuracy: 0.6442\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8208\n",
      "Epoch 00131: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.3968 - accuracy: 0.8208 - val_loss: 0.9247 - val_accuracy: 0.6538\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.8679\n",
      "Epoch 00132: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.3463 - accuracy: 0.8679 - val_loss: 1.4217 - val_accuracy: 0.6058\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8774\n",
      "Epoch 00133: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.3102 - accuracy: 0.8774 - val_loss: 1.2656 - val_accuracy: 0.5192\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4981 - accuracy: 0.8396\n",
      "Epoch 00134: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.4981 - accuracy: 0.8396 - val_loss: 0.7266 - val_accuracy: 0.5192\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.7830\n",
      "Epoch 00135: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.4883 - accuracy: 0.7830 - val_loss: 0.6412 - val_accuracy: 0.6250\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5116 - accuracy: 0.7642\n",
      "Epoch 00136: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5116 - accuracy: 0.7642 - val_loss: 0.6780 - val_accuracy: 0.5673\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.7925\n",
      "Epoch 00137: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.5084 - accuracy: 0.7925 - val_loss: 0.7557 - val_accuracy: 0.6154\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.5472\n",
      "Epoch 00138: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.7341 - accuracy: 0.5472 - val_loss: 0.6175 - val_accuracy: 0.6538\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6266 - accuracy: 0.6887\n",
      "Epoch 00139: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.6266 - accuracy: 0.6887 - val_loss: 0.6670 - val_accuracy: 0.5000\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6749 - accuracy: 0.5472\n",
      "Epoch 00140: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6749 - accuracy: 0.5472 - val_loss: 0.6722 - val_accuracy: 0.5000\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.6226\n",
      "Epoch 00141: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.6406 - accuracy: 0.6226 - val_loss: 0.6521 - val_accuracy: 0.5096\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.6981\n",
      "Epoch 00142: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6079 - accuracy: 0.6981 - val_loss: 0.6527 - val_accuracy: 0.6442\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5989 - accuracy: 0.6792\n",
      "Epoch 00143: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.5989 - accuracy: 0.6792 - val_loss: 0.6774 - val_accuracy: 0.6538\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.6792\n",
      "Epoch 00144: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.5502 - accuracy: 0.6792 - val_loss: 0.6829 - val_accuracy: 0.6538\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5544 - accuracy: 0.7642\n",
      "Epoch 00145: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5544 - accuracy: 0.7642 - val_loss: 0.7253 - val_accuracy: 0.6346\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.7830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00146: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.4451 - accuracy: 0.7830 - val_loss: 0.6648 - val_accuracy: 0.6538\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.7642\n",
      "Epoch 00147: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4684 - accuracy: 0.7642 - val_loss: 0.7613 - val_accuracy: 0.6250\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.7170\n",
      "Epoch 00148: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4337 - accuracy: 0.7170 - val_loss: 0.8952 - val_accuracy: 0.6442\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.8302\n",
      "Epoch 00149: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.3666 - accuracy: 0.8302 - val_loss: 1.3581 - val_accuracy: 0.6058\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.7547\n",
      "Epoch 00150: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.5691 - accuracy: 0.7547 - val_loss: 0.9959 - val_accuracy: 0.6538\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6021 - accuracy: 0.7170\n",
      "Epoch 00151: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.6021 - accuracy: 0.7170 - val_loss: 0.9651 - val_accuracy: 0.7019\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4530 - accuracy: 0.7358\n",
      "Epoch 00152: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4530 - accuracy: 0.7358 - val_loss: 0.9418 - val_accuracy: 0.5865\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.6792\n",
      "Epoch 00153: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5048 - accuracy: 0.6792 - val_loss: 0.9008 - val_accuracy: 0.5000\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.6698\n",
      "Epoch 00154: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.4783 - accuracy: 0.6698 - val_loss: 0.9782 - val_accuracy: 0.5769\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.7264\n",
      "Epoch 00155: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4494 - accuracy: 0.7264 - val_loss: 1.1888 - val_accuracy: 0.6250\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.8302\n",
      "Epoch 00156: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.3829 - accuracy: 0.8302 - val_loss: 1.4059 - val_accuracy: 0.5962\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3538 - accuracy: 0.8396\n",
      "Epoch 00157: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.3538 - accuracy: 0.8396 - val_loss: 1.8750 - val_accuracy: 0.6154\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.8491\n",
      "Epoch 00158: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.3310 - accuracy: 0.8491 - val_loss: 1.5095 - val_accuracy: 0.6154\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.8585\n",
      "Epoch 00159: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.3062 - accuracy: 0.8585 - val_loss: 1.9385 - val_accuracy: 0.5865\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.8774\n",
      "Epoch 00160: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.3697 - accuracy: 0.8774 - val_loss: 1.2705 - val_accuracy: 0.6442\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.7736\n",
      "Epoch 00161: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.4451 - accuracy: 0.7736 - val_loss: 0.8354 - val_accuracy: 0.6923\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.7075\n",
      "Epoch 00162: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5005 - accuracy: 0.7075 - val_loss: 0.8952 - val_accuracy: 0.6346\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4547 - accuracy: 0.7642\n",
      "Epoch 00163: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.4547 - accuracy: 0.7642 - val_loss: 1.3096 - val_accuracy: 0.6346\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.7264\n",
      "Epoch 00164: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.5441 - accuracy: 0.7264 - val_loss: 1.5643 - val_accuracy: 0.6250\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.7925\n",
      "Epoch 00165: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4353 - accuracy: 0.7925 - val_loss: 1.1921 - val_accuracy: 0.6058\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.7925\n",
      "Epoch 00166: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.4084 - accuracy: 0.7925 - val_loss: 1.3192 - val_accuracy: 0.6731\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5312 - accuracy: 0.7547\n",
      "Epoch 00167: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.5312 - accuracy: 0.7547 - val_loss: 1.5201 - val_accuracy: 0.6635\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.8113\n",
      "Epoch 00168: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.5301 - accuracy: 0.8113 - val_loss: 1.0251 - val_accuracy: 0.6442\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4304 - accuracy: 0.7925\n",
      "Epoch 00169: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.4304 - accuracy: 0.7925 - val_loss: 0.7671 - val_accuracy: 0.6346\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3930 - accuracy: 0.8019\n",
      "Epoch 00170: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.3930 - accuracy: 0.8019 - val_loss: 0.8263 - val_accuracy: 0.6538\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8774\n",
      "Epoch 00171: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.3234 - accuracy: 0.8774 - val_loss: 1.0655 - val_accuracy: 0.5962\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.9057\n",
      "Epoch 00172: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.2974 - accuracy: 0.9057 - val_loss: 1.3413 - val_accuracy: 0.6442\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.8679\n",
      "Epoch 00173: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.3139 - accuracy: 0.8679 - val_loss: 1.7526 - val_accuracy: 0.6250\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8774\n",
      "Epoch 00174: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.2856 - accuracy: 0.8774 - val_loss: 2.1206 - val_accuracy: 0.5865\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.8679\n",
      "Epoch 00175: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.2970 - accuracy: 0.8679 - val_loss: 1.8657 - val_accuracy: 0.6346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.7358\n",
      "Epoch 00176: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.5712 - accuracy: 0.7358 - val_loss: 1.4871 - val_accuracy: 0.6058\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5586 - accuracy: 0.7170\n",
      "Epoch 00177: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 0.5586 - accuracy: 0.7170 - val_loss: 1.9124 - val_accuracy: 0.6154\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5649 - accuracy: 0.6981\n",
      "Epoch 00178: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.5649 - accuracy: 0.6981 - val_loss: 1.3076 - val_accuracy: 0.5769\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.7547\n",
      "Epoch 00179: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.4568 - accuracy: 0.7547 - val_loss: 0.8292 - val_accuracy: 0.5769\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.7453\n",
      "Epoch 00180: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.4989 - accuracy: 0.7453 - val_loss: 0.7654 - val_accuracy: 0.5481\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.7264\n",
      "Epoch 00181: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 0.5640 - accuracy: 0.7264 - val_loss: 0.7896 - val_accuracy: 0.5192\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6740 - accuracy: 0.6887\n",
      "Epoch 00182: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.6740 - accuracy: 0.6887 - val_loss: 0.8283 - val_accuracy: 0.5192\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.6415\n",
      "Epoch 00183: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.6590 - accuracy: 0.6415 - val_loss: 0.8392 - val_accuracy: 0.5385\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.6038\n",
      "Epoch 00184: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.6227 - accuracy: 0.6038 - val_loss: 1.1048 - val_accuracy: 0.5192\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.8302\n",
      "Epoch 00185: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.5288 - accuracy: 0.8302 - val_loss: 1.0875 - val_accuracy: 0.6827\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.7453\n",
      "Epoch 00186: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.4808 - accuracy: 0.7453 - val_loss: 0.9944 - val_accuracy: 0.6346\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4889 - accuracy: 0.7170\n",
      "Epoch 00187: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.4889 - accuracy: 0.7170 - val_loss: 0.9553 - val_accuracy: 0.6058\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.7547\n",
      "Epoch 00188: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.4599 - accuracy: 0.7547 - val_loss: 0.8732 - val_accuracy: 0.6058\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4399 - accuracy: 0.7830\n",
      "Epoch 00189: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.4399 - accuracy: 0.7830 - val_loss: 0.8802 - val_accuracy: 0.4712\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4114 - accuracy: 0.8113\n",
      "Epoch 00190: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 0.4114 - accuracy: 0.8113 - val_loss: 1.0033 - val_accuracy: 0.4615\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.7547\n",
      "Epoch 00191: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.4381 - accuracy: 0.7547 - val_loss: 1.1896 - val_accuracy: 0.4615\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8019\n",
      "Epoch 00192: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.4513 - accuracy: 0.8019 - val_loss: 1.4244 - val_accuracy: 0.4712\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.8113\n",
      "Epoch 00193: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.4192 - accuracy: 0.8113 - val_loss: 1.5526 - val_accuracy: 0.4712\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.8679\n",
      "Epoch 00194: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.3071 - accuracy: 0.8679 - val_loss: 1.6999 - val_accuracy: 0.4519\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.8774\n",
      "Epoch 00195: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.3391 - accuracy: 0.8774 - val_loss: 1.9094 - val_accuracy: 0.4712\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8585\n",
      "Epoch 00196: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.3476 - accuracy: 0.8585 - val_loss: 2.0578 - val_accuracy: 0.4423\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8962\n",
      "Epoch 00197: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.3355 - accuracy: 0.8962 - val_loss: 2.2596 - val_accuracy: 0.4327\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.8774\n",
      "Epoch 00198: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.2960 - accuracy: 0.8774 - val_loss: 2.3895 - val_accuracy: 0.4231\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.8774\n",
      "Epoch 00199: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.3173 - accuracy: 0.8774 - val_loss: 1.8064 - val_accuracy: 0.4808\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8868\n",
      "Epoch 00200: val_accuracy did not improve from 0.72115\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.3969 - accuracy: 0.8868 - val_loss: 1.4807 - val_accuracy: 0.5288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x297f977ad60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "chk = ModelCheckpoint('best_model', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(train, train_target, epochs=200, batch_size=32, callbacks=[chk], validation_data=(validation,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-17f8931156b5>:5: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5961538461538461"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the model and checking accuracy on the test data\n",
    "model = load_model('best_model')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_preds = model.predict_classes(test)\n",
    "accuracy_score(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 29],\n",
       "       [13, 41]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"TRY1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY1_1: Building time series classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop=EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(\n",
    "    tf.keras.layers.Bidirectional(\n",
    "      tf.keras.layers.LSTM(\n",
    "          units=256, \n",
    "          input_shape=(seq_len, 4), kernel_regularizer=l2(0.1)\n",
    "      )\n",
    "    )\n",
    ")\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# opt=Adam(lr=0.1)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(seq_len, 4), return_sequences=True))\n",
    "# model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "# model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(64, return_sequences=False))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(loss=tf.losses.MeanSquaredError(), optimizer=opt,metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 2.2625 - acc: 0.4906 - val_loss: 2.1623 - val_acc: 0.6442\n",
      "Epoch 2/600\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 2.0959 - acc: 0.8491 - val_loss: 2.0446 - val_acc: 0.5288\n",
      "Epoch 3/600\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 1.9215 - acc: 0.8679 - val_loss: 1.9701 - val_acc: 0.5096\n",
      "Epoch 4/600\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.7540 - acc: 0.7925 - val_loss: 2.0226 - val_acc: 0.5192\n",
      "Epoch 5/600\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.5689 - acc: 0.8113 - val_loss: 2.2668 - val_acc: 0.6058\n",
      "Epoch 6/600\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 1.4492 - acc: 0.8491 - val_loss: 3.0252 - val_acc: 0.5673\n",
      "Epoch 7/600\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 1.3682 - acc: 0.8585 - val_loss: 2.6153 - val_acc: 0.5673\n",
      "Epoch 8/600\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 1.2001 - acc: 0.8679 - val_loss: 2.2338 - val_acc: 0.6250\n",
      "Epoch 9/600\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.1636 - acc: 0.8491 - val_loss: 2.3911 - val_acc: 0.6058\n",
      "Epoch 10/600\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 1.0785 - acc: 0.8774 - val_loss: 2.6253 - val_acc: 0.5962\n",
      "Epoch 11/600\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 0.9987 - acc: 0.9057 - val_loss: 3.0816 - val_acc: 0.5673\n",
      "Epoch 12/600\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.9189 - acc: 0.9057 - val_loss: 3.0730 - val_acc: 0.5769\n",
      "Epoch 13/600\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.8414 - acc: 0.9340 - val_loss: 3.1735 - val_acc: 0.5673\n",
      "Epoch 14/600\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.7729 - acc: 0.9340 - val_loss: 5.2383 - val_acc: 0.5865\n",
      "Epoch 15/600\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.8559 - acc: 0.8962 - val_loss: 4.7632 - val_acc: 0.5481\n",
      "Epoch 16/600\n",
      "2/2 [==============================] - 0s 188ms/step - loss: 0.8422 - acc: 0.8585 - val_loss: 2.9071 - val_acc: 0.5865\n",
      "Epoch 17/600\n",
      "2/2 [==============================] - 0s 195ms/step - loss: 0.7264 - acc: 0.8585 - val_loss: 1.7004 - val_acc: 0.5962\n",
      "Epoch 18/600\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.7100 - acc: 0.9151 - val_loss: 1.7401 - val_acc: 0.5673\n",
      "Epoch 19/600\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 0.7050 - acc: 0.8962 - val_loss: 1.7583 - val_acc: 0.5769\n",
      "Epoch 20/600\n",
      "2/2 [==============================] - 0s 219ms/step - loss: 0.7029 - acc: 0.8585 - val_loss: 1.5389 - val_acc: 0.5865\n",
      "Epoch 21/600\n",
      "2/2 [==============================] - 0s 212ms/step - loss: 0.6403 - acc: 0.9434 - val_loss: 1.3955 - val_acc: 0.5962\n",
      "Epoch 22/600\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 0.6107 - acc: 0.9434 - val_loss: 1.3410 - val_acc: 0.6250\n",
      "Epoch 23/600\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.5804 - acc: 0.9717 - val_loss: 1.4736 - val_acc: 0.6154\n",
      "Epoch 24/600\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.5510 - acc: 0.9434 - val_loss: 1.8614 - val_acc: 0.5769\n",
      "Epoch 25/600\n",
      "2/2 [==============================] - 0s 203ms/step - loss: 0.5107 - acc: 0.9434 - val_loss: 1.8042 - val_acc: 0.5769\n",
      "Epoch 26/600\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.4504 - acc: 0.9434 - val_loss: 2.4371 - val_acc: 0.5481\n",
      "Epoch 27/600\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.4506 - acc: 0.9340 - val_loss: 2.4919 - val_acc: 0.5481\n",
      "Epoch 28/600\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.3853 - acc: 0.9528 - val_loss: 2.2048 - val_acc: 0.5673\n",
      "Epoch 29/600\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.4070 - acc: 0.9151 - val_loss: 2.2604 - val_acc: 0.5865\n",
      "Epoch 30/600\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.3561 - acc: 0.9528 - val_loss: 2.3679 - val_acc: 0.5865\n",
      "Epoch 31/600\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.3393 - acc: 0.9528 - val_loss: 2.3960 - val_acc: 0.5865\n",
      "Epoch 32/600\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.3123 - acc: 0.9528 - val_loss: 2.1905 - val_acc: 0.5865\n",
      "Epoch 33/600\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.3041 - acc: 0.9623 - val_loss: 2.2998 - val_acc: 0.5769\n",
      "Epoch 34/600\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.2735 - acc: 0.9811 - val_loss: 2.5082 - val_acc: 0.5769\n",
      "Epoch 35/600\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.2602 - acc: 0.9623 - val_loss: 2.1676 - val_acc: 0.5769\n",
      "Epoch 36/600\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 0.2995 - acc: 0.9434 - val_loss: 2.7990 - val_acc: 0.5769\n",
      "Epoch 37/600\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.2985 - acc: 0.9717 - val_loss: 2.1385 - val_acc: 0.5769\n",
      "Epoch 38/600\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.2515 - acc: 1.0000 - val_loss: 1.9786 - val_acc: 0.5865\n",
      "Epoch 39/600\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.2945 - acc: 0.9528 - val_loss: 2.0540 - val_acc: 0.6058\n",
      "Epoch 40/600\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 0.3062 - acc: 0.9245 - val_loss: 1.7691 - val_acc: 0.5865\n",
      "Epoch 41/600\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.2603 - acc: 0.9811 - val_loss: 1.7455 - val_acc: 0.5673\n",
      "Epoch 42/600\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 0.2497 - acc: 0.9811 - val_loss: 1.9477 - val_acc: 0.5481\n",
      "Epoch 43/600\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 0.2251 - acc: 0.9906 - val_loss: 2.1930 - val_acc: 0.5481\n",
      "Epoch 44/600\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 0.1971 - acc: 0.9906 - val_loss: 2.5376 - val_acc: 0.5385\n",
      "Epoch 45/600\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.1936 - acc: 0.9811 - val_loss: 2.7430 - val_acc: 0.5577\n",
      "Epoch 46/600\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 0.2119 - acc: 0.9717 - val_loss: 2.1757 - val_acc: 0.5577\n",
      "Epoch 47/600\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.2246 - acc: 0.9717 - val_loss: 1.7648 - val_acc: 0.5865\n",
      "Epoch 00047: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x297ff5d08b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, train_target, epochs=600, batch_size=64, callbacks=[early_stop], validation_data=(validation,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MODEL1_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses=pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 29],\n",
       "       [13, 41]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               267264    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 293,065\n",
      "Trainable params: 293,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len=60\n",
    "n_features=4\n",
    "n_outputs=1\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.7064 - accuracy: 0.6132 - val_loss: 1.1210 - val_accuracy: 0.5000\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.7879 - accuracy: 0.5566 - val_loss: 0.6982 - val_accuracy: 0.5769\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 1.1349 - accuracy: 0.4528 - val_loss: 0.7748 - val_accuracy: 0.5385\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.7444 - accuracy: 0.5849 - val_loss: 0.8462 - val_accuracy: 0.5000\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.8611 - accuracy: 0.5377 - val_loss: 0.8959 - val_accuracy: 0.4904\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.8434 - accuracy: 0.4811 - val_loss: 0.7392 - val_accuracy: 0.5096\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.7261 - accuracy: 0.5283 - val_loss: 0.8168 - val_accuracy: 0.5000\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.7042 - accuracy: 0.5283 - val_loss: 0.7395 - val_accuracy: 0.4808\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.7524 - accuracy: 0.4811 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.7150 - accuracy: 0.5660 - val_loss: 0.7576 - val_accuracy: 0.5000\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.7132 - accuracy: 0.5000 - val_loss: 0.6913 - val_accuracy: 0.5192\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.7015 - accuracy: 0.4906 - val_loss: 0.6865 - val_accuracy: 0.5577\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6811 - accuracy: 0.5566 - val_loss: 0.7311 - val_accuracy: 0.5481\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.7356 - accuracy: 0.4717 - val_loss: 0.7203 - val_accuracy: 0.5000\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6900 - val_accuracy: 0.5096\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6938 - accuracy: 0.4811 - val_loss: 0.7021 - val_accuracy: 0.4423\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6917 - accuracy: 0.4811 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.6729 - accuracy: 0.5849 - val_loss: 0.6843 - val_accuracy: 0.6442\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6617 - accuracy: 0.6321 - val_loss: 0.6868 - val_accuracy: 0.5577\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6601 - accuracy: 0.5189 - val_loss: 0.6971 - val_accuracy: 0.5481\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6384 - accuracy: 0.5943 - val_loss: 0.7817 - val_accuracy: 0.5096\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6289 - accuracy: 0.5283 - val_loss: 0.7824 - val_accuracy: 0.5192\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6220 - accuracy: 0.5943 - val_loss: 0.8470 - val_accuracy: 0.5000\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6457 - accuracy: 0.5377 - val_loss: 0.9578 - val_accuracy: 0.5000\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6154 - accuracy: 0.6887 - val_loss: 0.6696 - val_accuracy: 0.5865\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5907 - accuracy: 0.6887 - val_loss: 0.6941 - val_accuracy: 0.5962\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6074 - accuracy: 0.6132 - val_loss: 0.6939 - val_accuracy: 0.6154\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5943 - accuracy: 0.6604 - val_loss: 0.7069 - val_accuracy: 0.6154\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6152 - accuracy: 0.6321 - val_loss: 0.7266 - val_accuracy: 0.5962\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6140 - accuracy: 0.6132 - val_loss: 0.7457 - val_accuracy: 0.5865\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.8983 - accuracy: 0.6698 - val_loss: 0.9569 - val_accuracy: 0.5096\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6255 - accuracy: 0.5943 - val_loss: 0.6853 - val_accuracy: 0.5673\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6479 - accuracy: 0.6132 - val_loss: 0.7465 - val_accuracy: 0.5769\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6366 - accuracy: 0.7075 - val_loss: 0.7802 - val_accuracy: 0.5096\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5899 - accuracy: 0.6604 - val_loss: 0.8001 - val_accuracy: 0.5096\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5589 - accuracy: 0.6792 - val_loss: 0.7924 - val_accuracy: 0.5096\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4818 - accuracy: 0.7736 - val_loss: 0.9009 - val_accuracy: 0.5096\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4576 - accuracy: 0.7925 - val_loss: 1.1633 - val_accuracy: 0.5096\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4892 - accuracy: 0.7453 - val_loss: 1.1764 - val_accuracy: 0.5192\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4434 - accuracy: 0.7925 - val_loss: 0.9210 - val_accuracy: 0.5865\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5097 - accuracy: 0.7170 - val_loss: 1.6297 - val_accuracy: 0.5192\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5407 - accuracy: 0.7264 - val_loss: 0.9704 - val_accuracy: 0.5577\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4727 - accuracy: 0.7736 - val_loss: 1.5430 - val_accuracy: 0.5192\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4834 - accuracy: 0.7736 - val_loss: 0.9352 - val_accuracy: 0.5673\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5655 - accuracy: 0.7075 - val_loss: 1.5232 - val_accuracy: 0.5192\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.4906 - accuracy: 0.7358 - val_loss: 1.0567 - val_accuracy: 0.5577\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4315 - accuracy: 0.7642 - val_loss: 1.0135 - val_accuracy: 0.5673\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4052 - accuracy: 0.7642 - val_loss: 1.5069 - val_accuracy: 0.5192\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4573 - accuracy: 0.7547 - val_loss: 1.4389 - val_accuracy: 0.5192\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4472 - accuracy: 0.7547 - val_loss: 0.7783 - val_accuracy: 0.5865\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4598 - accuracy: 0.7358 - val_loss: 1.3072 - val_accuracy: 0.5192\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5095 - accuracy: 0.6887 - val_loss: 1.1111 - val_accuracy: 0.6250\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4458 - accuracy: 0.7453 - val_loss: 1.4816 - val_accuracy: 0.5673\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4126 - accuracy: 0.7358 - val_loss: 1.2507 - val_accuracy: 0.5000\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4246 - accuracy: 0.7736 - val_loss: 1.9909 - val_accuracy: 0.5577\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4756 - accuracy: 0.7642 - val_loss: 1.8500 - val_accuracy: 0.5192\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4539 - accuracy: 0.7358 - val_loss: 0.9103 - val_accuracy: 0.5962\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4366 - accuracy: 0.7453 - val_loss: 0.9487 - val_accuracy: 0.5769\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3967 - accuracy: 0.8019 - val_loss: 1.2005 - val_accuracy: 0.5673\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.3659 - accuracy: 0.8208 - val_loss: 1.4311 - val_accuracy: 0.5577\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3713 - accuracy: 0.7830 - val_loss: 1.3858 - val_accuracy: 0.5769\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3964 - accuracy: 0.8396 - val_loss: 1.8539 - val_accuracy: 0.5288\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4225 - accuracy: 0.8491 - val_loss: 1.6780 - val_accuracy: 0.5385\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.3891 - accuracy: 0.8585 - val_loss: 1.4935 - val_accuracy: 0.5577\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3317 - accuracy: 0.8396 - val_loss: 1.6245 - val_accuracy: 0.5577\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.2856 - accuracy: 0.9057 - val_loss: 1.6561 - val_accuracy: 0.5577\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.3322 - accuracy: 0.8962 - val_loss: 1.6073 - val_accuracy: 0.5769\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.3428 - accuracy: 0.8679 - val_loss: 1.7706 - val_accuracy: 0.5385\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.3426 - accuracy: 0.8679 - val_loss: 1.6969 - val_accuracy: 0.5577\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.3692 - accuracy: 0.8585 - val_loss: 1.5540 - val_accuracy: 0.5385\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.3849 - accuracy: 0.8208 - val_loss: 1.5398 - val_accuracy: 0.5481\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.3725 - accuracy: 0.8491 - val_loss: 1.2132 - val_accuracy: 0.5192\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.4137 - accuracy: 0.8208 - val_loss: 1.3566 - val_accuracy: 0.5577\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3392 - accuracy: 0.8868 - val_loss: 1.6445 - val_accuracy: 0.5288\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3126 - accuracy: 0.8491 - val_loss: 1.3023 - val_accuracy: 0.5481\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3188 - accuracy: 0.8491 - val_loss: 1.2677 - val_accuracy: 0.5577\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3167 - accuracy: 0.8585 - val_loss: 1.3240 - val_accuracy: 0.5769\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3376 - accuracy: 0.8396 - val_loss: 1.6297 - val_accuracy: 0.5577\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3885 - accuracy: 0.8208 - val_loss: 1.7133 - val_accuracy: 0.5577\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3667 - accuracy: 0.8679 - val_loss: 1.7574 - val_accuracy: 0.5577\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2974 - accuracy: 0.8585 - val_loss: 1.9273 - val_accuracy: 0.5673\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3682 - accuracy: 0.8396 - val_loss: 1.6369 - val_accuracy: 0.5096\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4730 - accuracy: 0.7925 - val_loss: 1.3684 - val_accuracy: 0.5385\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3542 - accuracy: 0.8396 - val_loss: 1.9698 - val_accuracy: 0.5000\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3796 - accuracy: 0.8491 - val_loss: 1.1360 - val_accuracy: 0.5577\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.3277 - accuracy: 0.8585 - val_loss: 1.1236 - val_accuracy: 0.5385\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3272 - accuracy: 0.8585 - val_loss: 1.4098 - val_accuracy: 0.5481\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.3418 - accuracy: 0.8396 - val_loss: 1.5701 - val_accuracy: 0.5481\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3029 - accuracy: 0.8585 - val_loss: 1.5522 - val_accuracy: 0.5192\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.3255 - accuracy: 0.8585 - val_loss: 1.6816 - val_accuracy: 0.5673\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.3137 - accuracy: 0.8585 - val_loss: 1.9673 - val_accuracy: 0.5577\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.3028 - accuracy: 0.8774 - val_loss: 1.9718 - val_accuracy: 0.5769\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2996 - accuracy: 0.8868 - val_loss: 1.9879 - val_accuracy: 0.5577\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2897 - accuracy: 0.8868 - val_loss: 2.0791 - val_accuracy: 0.5577\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2970 - accuracy: 0.8774 - val_loss: 1.8405 - val_accuracy: 0.5481\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.3651 - accuracy: 0.8585 - val_loss: 1.4080 - val_accuracy: 0.5481\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2957 - accuracy: 0.8774 - val_loss: 1.5959 - val_accuracy: 0.5577\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3244 - accuracy: 0.8774 - val_loss: 1.5648 - val_accuracy: 0.5577\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3023 - accuracy: 0.8774 - val_loss: 1.2663 - val_accuracy: 0.5769\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3130 - accuracy: 0.8868 - val_loss: 1.4243 - val_accuracy: 0.5385\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3137 - accuracy: 0.8491 - val_loss: 1.7166 - val_accuracy: 0.5481\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2940 - accuracy: 0.8302 - val_loss: 1.9801 - val_accuracy: 0.5481\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2694 - accuracy: 0.8585 - val_loss: 2.1500 - val_accuracy: 0.5481\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2922 - accuracy: 0.8585 - val_loss: 2.4287 - val_accuracy: 0.5577\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2750 - accuracy: 0.8774 - val_loss: 2.5865 - val_accuracy: 0.5577\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3179 - accuracy: 0.7925 - val_loss: 2.7395 - val_accuracy: 0.5481\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3239 - accuracy: 0.8774 - val_loss: 2.7840 - val_accuracy: 0.5481\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.2870 - accuracy: 0.8396 - val_loss: 2.5670 - val_accuracy: 0.5385\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2746 - accuracy: 0.8491 - val_loss: 2.5313 - val_accuracy: 0.5577\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.2753 - accuracy: 0.8679 - val_loss: 2.4964 - val_accuracy: 0.5673\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.2746 - accuracy: 0.8774 - val_loss: 2.5803 - val_accuracy: 0.5577\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.2475 - accuracy: 0.8868 - val_loss: 2.6152 - val_accuracy: 0.5673\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2497 - accuracy: 0.8774 - val_loss: 2.6621 - val_accuracy: 0.5577\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.2533 - accuracy: 0.8868 - val_loss: 2.6736 - val_accuracy: 0.5577\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2654 - accuracy: 0.8774 - val_loss: 2.5707 - val_accuracy: 0.5673\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2606 - accuracy: 0.8679 - val_loss: 2.5197 - val_accuracy: 0.5769\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3082 - accuracy: 0.8679 - val_loss: 2.7995 - val_accuracy: 0.5481\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2484 - accuracy: 0.8868 - val_loss: 3.0434 - val_accuracy: 0.5481\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3356 - accuracy: 0.8302 - val_loss: 3.1064 - val_accuracy: 0.5481\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2913 - accuracy: 0.8396 - val_loss: 2.5809 - val_accuracy: 0.6058\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.3493 - accuracy: 0.8679 - val_loss: 2.6532 - val_accuracy: 0.5673\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4387 - accuracy: 0.7925 - val_loss: 1.6634 - val_accuracy: 0.5096\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3719 - accuracy: 0.8396 - val_loss: 0.8976 - val_accuracy: 0.5577\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3990 - accuracy: 0.7925 - val_loss: 1.0211 - val_accuracy: 0.6154\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.3251 - accuracy: 0.8302 - val_loss: 1.5779 - val_accuracy: 0.5288\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3949 - accuracy: 0.8491 - val_loss: 1.0725 - val_accuracy: 0.5481\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3367 - accuracy: 0.8208 - val_loss: 1.1684 - val_accuracy: 0.5385\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2693 - accuracy: 0.8679 - val_loss: 1.2688 - val_accuracy: 0.5577\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2628 - accuracy: 0.8585 - val_loss: 1.4914 - val_accuracy: 0.5385\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.2596 - accuracy: 0.8679 - val_loss: 1.4279 - val_accuracy: 0.5577\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2579 - accuracy: 0.8868 - val_loss: 1.7014 - val_accuracy: 0.5481\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2302 - accuracy: 0.8868 - val_loss: 1.6856 - val_accuracy: 0.5865\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2539 - accuracy: 0.8679 - val_loss: 1.4519 - val_accuracy: 0.6346\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2488 - accuracy: 0.8679 - val_loss: 1.7367 - val_accuracy: 0.5577\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4203 - accuracy: 0.7453 - val_loss: 1.6235 - val_accuracy: 0.5385\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3791 - accuracy: 0.7453 - val_loss: 1.5410 - val_accuracy: 0.5769\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3586 - accuracy: 0.7642 - val_loss: 1.6440 - val_accuracy: 0.5673\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3124 - accuracy: 0.8491 - val_loss: 1.7521 - val_accuracy: 0.5962\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2711 - accuracy: 0.8774 - val_loss: 2.0849 - val_accuracy: 0.5385\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3313 - accuracy: 0.8585 - val_loss: 2.2544 - val_accuracy: 0.5481\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2525 - accuracy: 0.8868 - val_loss: 2.3740 - val_accuracy: 0.5481\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2511 - accuracy: 0.8679 - val_loss: 2.4778 - val_accuracy: 0.5481\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.3292 - accuracy: 0.8679 - val_loss: 2.6146 - val_accuracy: 0.5385\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.3146 - accuracy: 0.8585 - val_loss: 1.9074 - val_accuracy: 0.5385\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.3075 - accuracy: 0.8491 - val_loss: 2.2561 - val_accuracy: 0.5288\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.3701 - accuracy: 0.8868 - val_loss: 1.3118 - val_accuracy: 0.5192\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2710 - accuracy: 0.8774 - val_loss: 1.0685 - val_accuracy: 0.5577\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3040 - accuracy: 0.8302 - val_loss: 1.3248 - val_accuracy: 0.5288\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2300 - accuracy: 0.8962 - val_loss: 1.2925 - val_accuracy: 0.5577\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2302 - accuracy: 0.8962 - val_loss: 1.3843 - val_accuracy: 0.5481\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2358 - accuracy: 0.8868 - val_loss: 1.5083 - val_accuracy: 0.5577\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2162 - accuracy: 0.9057 - val_loss: 1.5849 - val_accuracy: 0.6346\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2711 - accuracy: 0.8491 - val_loss: 1.9578 - val_accuracy: 0.5962\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2372 - accuracy: 0.8868 - val_loss: 2.2555 - val_accuracy: 0.5673\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2969 - accuracy: 0.8868 - val_loss: 2.0877 - val_accuracy: 0.5288\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.3779 - accuracy: 0.8396 - val_loss: 1.2894 - val_accuracy: 0.5385\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.2614 - accuracy: 0.8679 - val_loss: 1.5760 - val_accuracy: 0.5000\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3157 - accuracy: 0.8491 - val_loss: 1.4083 - val_accuracy: 0.5192\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2378 - accuracy: 0.8774 - val_loss: 1.2162 - val_accuracy: 0.5481\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.2523 - accuracy: 0.8679 - val_loss: 1.2548 - val_accuracy: 0.5385\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2204 - accuracy: 0.9151 - val_loss: 1.3116 - val_accuracy: 0.5673\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2438 - accuracy: 0.8868 - val_loss: 1.3506 - val_accuracy: 0.5769\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1781 - accuracy: 0.9057 - val_loss: 1.4733 - val_accuracy: 0.5673\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1821 - accuracy: 0.9151 - val_loss: 1.7022 - val_accuracy: 0.5481\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1876 - accuracy: 0.9151 - val_loss: 1.7907 - val_accuracy: 0.5288\n",
      "Epoch 166/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2373 - accuracy: 0.8774 - val_loss: 1.5905 - val_accuracy: 0.5577\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2079 - accuracy: 0.9340 - val_loss: 1.5593 - val_accuracy: 0.6442\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2240 - accuracy: 0.9057 - val_loss: 1.3501 - val_accuracy: 0.6154\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5663 - accuracy: 0.6792 - val_loss: 0.7581 - val_accuracy: 0.5288\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.6753 - accuracy: 0.5189 - val_loss: 0.7027 - val_accuracy: 0.5000\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6917 - accuracy: 0.5189 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6763 - accuracy: 0.6038 - val_loss: 0.6992 - val_accuracy: 0.4808\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6693 - accuracy: 0.5283 - val_loss: 0.7772 - val_accuracy: 0.5288\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6277 - accuracy: 0.5755 - val_loss: 0.8235 - val_accuracy: 0.4808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5799 - accuracy: 0.6792 - val_loss: 0.8952 - val_accuracy: 0.5000\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6073 - accuracy: 0.6132 - val_loss: 0.8977 - val_accuracy: 0.4712\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5676 - accuracy: 0.6509 - val_loss: 0.9786 - val_accuracy: 0.5385\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5820 - accuracy: 0.6698 - val_loss: 0.9030 - val_accuracy: 0.5481\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5973 - accuracy: 0.7925 - val_loss: 0.9745 - val_accuracy: 0.5096\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5203 - accuracy: 0.7358 - val_loss: 1.1667 - val_accuracy: 0.5673\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5180 - accuracy: 0.7358 - val_loss: 0.8842 - val_accuracy: 0.5385\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3967 - accuracy: 0.8302 - val_loss: 0.9801 - val_accuracy: 0.5096\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3737 - accuracy: 0.8491 - val_loss: 0.8712 - val_accuracy: 0.5288\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4498 - accuracy: 0.8019 - val_loss: 1.1752 - val_accuracy: 0.5096\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3927 - accuracy: 0.8491 - val_loss: 1.2440 - val_accuracy: 0.5000\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4245 - accuracy: 0.7925 - val_loss: 0.8651 - val_accuracy: 0.6346\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2776 - accuracy: 0.9057 - val_loss: 1.2769 - val_accuracy: 0.5481\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3806 - accuracy: 0.8396 - val_loss: 1.1672 - val_accuracy: 0.5673\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3356 - accuracy: 0.8962 - val_loss: 1.0232 - val_accuracy: 0.5769\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3373 - accuracy: 0.8868 - val_loss: 1.0173 - val_accuracy: 0.5769\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4021 - accuracy: 0.8302 - val_loss: 1.4423 - val_accuracy: 0.5192\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3657 - accuracy: 0.8302 - val_loss: 1.1295 - val_accuracy: 0.5385\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4794 - accuracy: 0.7736 - val_loss: 1.0496 - val_accuracy: 0.5577\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.5357 - accuracy: 0.7547 - val_loss: 1.1507 - val_accuracy: 0.5096\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4689 - accuracy: 0.7264 - val_loss: 1.0197 - val_accuracy: 0.5385\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3701 - accuracy: 0.8302 - val_loss: 1.3376 - val_accuracy: 0.5288\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2413 - accuracy: 0.9151 - val_loss: 1.4020 - val_accuracy: 0.5385\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2439 - accuracy: 0.8679 - val_loss: 1.3775 - val_accuracy: 0.5481\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4564 - accuracy: 0.8019 - val_loss: 0.9705 - val_accuracy: 0.5385\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3597 - accuracy: 0.8396 - val_loss: 1.2435 - val_accuracy: 0.5192\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2801 - accuracy: 0.8774 - val_loss: 1.5015 - val_accuracy: 0.5577\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2819 - accuracy: 0.8774 - val_loss: 1.3030 - val_accuracy: 0.5577\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3484 - accuracy: 0.8585 - val_loss: 1.4956 - val_accuracy: 0.5288\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3446 - accuracy: 0.8302 - val_loss: 1.5602 - val_accuracy: 0.5288\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.2298 - accuracy: 0.8868 - val_loss: 1.4670 - val_accuracy: 0.5192\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.82 - 0s 75ms/step - loss: 0.3139 - accuracy: 0.8208 - val_loss: 1.1983 - val_accuracy: 0.5673\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4663 - accuracy: 0.8019 - val_loss: 1.4758 - val_accuracy: 0.5000\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5104 - accuracy: 0.7453 - val_loss: 1.1178 - val_accuracy: 0.5000\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3986 - accuracy: 0.8019 - val_loss: 1.0420 - val_accuracy: 0.5192\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.3813 - accuracy: 0.8019 - val_loss: 1.1814 - val_accuracy: 0.5577\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2996 - accuracy: 0.8679 - val_loss: 1.2383 - val_accuracy: 0.5673\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3042 - accuracy: 0.9057 - val_loss: 1.6648 - val_accuracy: 0.5192\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4722 - accuracy: 0.8585 - val_loss: 1.3463 - val_accuracy: 0.5192\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.2392 - accuracy: 0.8962 - val_loss: 1.0989 - val_accuracy: 0.5481\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2868 - accuracy: 0.8774 - val_loss: 1.2209 - val_accuracy: 0.5865\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1554 - accuracy: 0.9623 - val_loss: 1.5945 - val_accuracy: 0.5577\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.1642 - accuracy: 0.8868 - val_loss: 1.9988 - val_accuracy: 0.5865\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4111 - accuracy: 0.8868 - val_loss: 1.7407 - val_accuracy: 0.5577\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2867 - accuracy: 0.9340 - val_loss: 1.2761 - val_accuracy: 0.5192\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5481 - accuracy: 0.8019 - val_loss: 0.9811 - val_accuracy: 0.5192\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.4177 - accuracy: 0.8113 - val_loss: 0.8464 - val_accuracy: 0.5577\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.3654 - accuracy: 0.8302 - val_loss: 0.9583 - val_accuracy: 0.5096\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3692 - accuracy: 0.8679 - val_loss: 1.0323 - val_accuracy: 0.5096\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2952 - accuracy: 0.8868 - val_loss: 1.1075 - val_accuracy: 0.5577\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2023 - accuracy: 0.9151 - val_loss: 1.2824 - val_accuracy: 0.5481\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2311 - accuracy: 0.8774 - val_loss: 1.2585 - val_accuracy: 0.5577\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2300 - accuracy: 0.8962 - val_loss: 1.6908 - val_accuracy: 0.5096\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1769 - accuracy: 0.9434 - val_loss: 1.8974 - val_accuracy: 0.5096\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.2280 - accuracy: 0.9057 - val_loss: 1.6377 - val_accuracy: 0.5385\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1640 - accuracy: 0.9528 - val_loss: 1.9203 - val_accuracy: 0.5385\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.1734 - accuracy: 0.9245 - val_loss: 1.9547 - val_accuracy: 0.5288\n",
      "Epoch 232/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0772 - accuracy: 0.9623 - val_loss: 2.0069 - val_accuracy: 0.5385\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1258 - accuracy: 0.9528 - val_loss: 2.2215 - val_accuracy: 0.5288\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0619 - accuracy: 0.9906 - val_loss: 2.4487 - val_accuracy: 0.5192\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.6374 - val_accuracy: 0.5385\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0443 - accuracy: 0.9811 - val_loss: 2.8513 - val_accuracy: 0.5288\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1288 - accuracy: 0.9717 - val_loss: 2.8785 - val_accuracy: 0.5288\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0570 - accuracy: 0.9906 - val_loss: 2.9562 - val_accuracy: 0.5096\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0562 - accuracy: 0.9623 - val_loss: 3.1450 - val_accuracy: 0.5192\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0407 - accuracy: 0.9811 - val_loss: 3.2230 - val_accuracy: 0.5192\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0399 - accuracy: 0.9811 - val_loss: 3.2647 - val_accuracy: 0.5288\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1813 - accuracy: 0.9623 - val_loss: 2.4633 - val_accuracy: 0.5769\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5379 - accuracy: 0.8774 - val_loss: 1.4173 - val_accuracy: 0.5577\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2312 - accuracy: 0.8962 - val_loss: 1.7820 - val_accuracy: 0.5288\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3224 - accuracy: 0.8679 - val_loss: 1.2677 - val_accuracy: 0.5288\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2453 - accuracy: 0.9057 - val_loss: 1.0138 - val_accuracy: 0.5192\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1931 - accuracy: 0.9245 - val_loss: 1.3788 - val_accuracy: 0.5192\n",
      "Epoch 248/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1043 - accuracy: 0.9717 - val_loss: 1.6339 - val_accuracy: 0.5385\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1111 - accuracy: 0.9528 - val_loss: 1.7950 - val_accuracy: 0.5288\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0598 - accuracy: 0.9811 - val_loss: 1.8500 - val_accuracy: 0.5577\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 2.0388 - val_accuracy: 0.5865\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0483 - accuracy: 0.9906 - val_loss: 2.4887 - val_accuracy: 0.5673\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1078 - accuracy: 0.9623 - val_loss: 2.6194 - val_accuracy: 0.5385\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0768 - accuracy: 0.9906 - val_loss: 2.5332 - val_accuracy: 0.5288\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0491 - accuracy: 0.9906 - val_loss: 2.6336 - val_accuracy: 0.5288\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0786 - accuracy: 0.9717 - val_loss: 2.6370 - val_accuracy: 0.5288\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.2404 - accuracy: 0.9528 - val_loss: 2.6129 - val_accuracy: 0.5096\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1805 - accuracy: 0.9434 - val_loss: 2.3948 - val_accuracy: 0.5481\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2731 - accuracy: 0.9151 - val_loss: 2.1578 - val_accuracy: 0.5288\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1347 - accuracy: 0.9623 - val_loss: 1.7667 - val_accuracy: 0.5385\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1252 - accuracy: 0.9623 - val_loss: 2.0207 - val_accuracy: 0.5288\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1065 - accuracy: 0.9623 - val_loss: 2.2152 - val_accuracy: 0.5577\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0558 - accuracy: 0.9811 - val_loss: 2.4111 - val_accuracy: 0.5481\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 2.5103 - val_accuracy: 0.5481\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0504 - accuracy: 0.9811 - val_loss: 2.8477 - val_accuracy: 0.5385\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0455 - accuracy: 0.9906 - val_loss: 3.0068 - val_accuracy: 0.5481\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0800 - accuracy: 0.9811 - val_loss: 3.0788 - val_accuracy: 0.5385\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0260 - accuracy: 0.9906 - val_loss: 3.0336 - val_accuracy: 0.5385\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0280 - accuracy: 0.9811 - val_loss: 3.0073 - val_accuracy: 0.5385\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 3.0569 - val_accuracy: 0.5385\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0152 - accuracy: 0.9906 - val_loss: 3.0944 - val_accuracy: 0.5385\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 3.2529 - val_accuracy: 0.5288\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.3545 - val_accuracy: 0.5288\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0920 - accuracy: 0.9906 - val_loss: 3.4213 - val_accuracy: 0.5385\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.5364 - val_accuracy: 0.5288\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 3.6115 - val_accuracy: 0.5288\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0162 - accuracy: 0.9906 - val_loss: 3.5756 - val_accuracy: 0.5481\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.5785 - val_accuracy: 0.5481\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.7064 - val_accuracy: 0.5481\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 3.7540 - val_accuracy: 0.5673\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 3.7688 - val_accuracy: 0.5577\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.7887 - val_accuracy: 0.5577\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.8575 - val_accuracy: 0.5577\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.9015 - val_accuracy: 0.5673\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9790 - val_accuracy: 0.5577\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.0119 - val_accuracy: 0.5481\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0558 - val_accuracy: 0.5481\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 4.1045 - val_accuracy: 0.5481\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.1485 - val_accuracy: 0.5481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.1207 - val_accuracy: 0.5577\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 4.0823 - val_accuracy: 0.5673\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9908 - val_accuracy: 0.5577\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 3.9184e-04 - accuracy: 1.0000 - val_loss: 3.9486 - val_accuracy: 0.5865\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9466 - val_accuracy: 0.5865\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.9404 - val_accuracy: 0.5865\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.9542 - val_accuracy: 0.5865\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9747 - val_accuracy: 0.5865\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 6.5636e-04 - accuracy: 1.0000 - val_loss: 3.9962 - val_accuracy: 0.5962\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0141 - val_accuracy: 0.5865\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.2577 - val_accuracy: 0.5481\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.5101 - val_accuracy: 0.5385\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6124 - val_accuracy: 0.5385\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6569 - val_accuracy: 0.5385\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 3.7782e-04 - accuracy: 1.0000 - val_loss: 4.6780 - val_accuracy: 0.5385\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 4.7360 - val_accuracy: 0.5288\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 6.8931e-04 - accuracy: 1.0000 - val_loss: 4.7880 - val_accuracy: 0.5288\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.7558 - val_accuracy: 0.5385\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7738 - val_accuracy: 0.5385\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.7846 - val_accuracy: 0.5385\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 7.4456e-04 - accuracy: 1.0000 - val_loss: 4.7838 - val_accuracy: 0.5481\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.7986 - val_accuracy: 0.5481\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 4.8915 - val_accuracy: 0.5385\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.9131 - val_accuracy: 0.5385\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 4.8789 - val_accuracy: 0.5385\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 4.7716e-04 - accuracy: 1.0000 - val_loss: 4.8583 - val_accuracy: 0.5385\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.8852 - val_accuracy: 0.5385\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.9198 - val_accuracy: 0.5385\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 3.9085e-04 - accuracy: 1.0000 - val_loss: 4.9453 - val_accuracy: 0.5385\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 8.8095e-04 - accuracy: 1.0000 - val_loss: 4.9663 - val_accuracy: 0.5385\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.9922 - val_accuracy: 0.5385\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 5.5378e-04 - accuracy: 1.0000 - val_loss: 5.0221 - val_accuracy: 0.5385\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 4.6374e-04 - accuracy: 1.0000 - val_loss: 5.0403 - val_accuracy: 0.5385\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 7.8246e-04 - accuracy: 1.0000 - val_loss: 5.0565 - val_accuracy: 0.5385\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.8066e-04 - accuracy: 1.0000 - val_loss: 5.0702 - val_accuracy: 0.5385\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 5.1196 - val_accuracy: 0.5385\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 3.0581e-04 - accuracy: 1.0000 - val_loss: 5.1509 - val_accuracy: 0.5385\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.1679 - val_accuracy: 0.5385\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 7.9190e-04 - accuracy: 1.0000 - val_loss: 5.1846 - val_accuracy: 0.5385\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 3.5729e-04 - accuracy: 1.0000 - val_loss: 5.1947 - val_accuracy: 0.5385\n",
      "Epoch 330/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 5.1548 - val_accuracy: 0.5481\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 5.0920 - val_accuracy: 0.5481\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 5.0486 - val_accuracy: 0.5577\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 5.0624 - val_accuracy: 0.5577\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1535 - accuracy: 0.9811 - val_loss: 4.9466 - val_accuracy: 0.5096\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 1.2948 - accuracy: 0.7547 - val_loss: 2.8931 - val_accuracy: 0.5000\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4863 - accuracy: 0.8396 - val_loss: 2.3323 - val_accuracy: 0.5096\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.5548 - accuracy: 0.7547 - val_loss: 1.1225 - val_accuracy: 0.5385\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4918 - accuracy: 0.8396 - val_loss: 0.9628 - val_accuracy: 0.5192\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4476 - accuracy: 0.8208 - val_loss: 0.9015 - val_accuracy: 0.5385\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3877 - accuracy: 0.8679 - val_loss: 0.9108 - val_accuracy: 0.6058\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3275 - accuracy: 0.8962 - val_loss: 1.0107 - val_accuracy: 0.6154\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3365 - accuracy: 0.8585 - val_loss: 1.2395 - val_accuracy: 0.5385\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3662 - accuracy: 0.8868 - val_loss: 1.2102 - val_accuracy: 0.5385\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3173 - accuracy: 0.8585 - val_loss: 1.2333 - val_accuracy: 0.5769\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2268 - accuracy: 0.8962 - val_loss: 1.3941 - val_accuracy: 0.5769\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1994 - accuracy: 0.9245 - val_loss: 1.5401 - val_accuracy: 0.6058\n",
      "Epoch 347/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2597 - accuracy: 0.9434 - val_loss: 1.5325 - val_accuracy: 0.6154\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2634 - accuracy: 0.9245 - val_loss: 1.4777 - val_accuracy: 0.6250\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1912 - accuracy: 0.9434 - val_loss: 1.4229 - val_accuracy: 0.6058\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1750 - accuracy: 0.9245 - val_loss: 1.6779 - val_accuracy: 0.5673\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2001 - accuracy: 0.9245 - val_loss: 1.7429 - val_accuracy: 0.5769\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1588 - accuracy: 0.9623 - val_loss: 1.7120 - val_accuracy: 0.5673\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1520 - accuracy: 0.9528 - val_loss: 1.8938 - val_accuracy: 0.5481\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2367 - accuracy: 0.9340 - val_loss: 1.7644 - val_accuracy: 0.5481\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1893 - accuracy: 0.9528 - val_loss: 1.4079 - val_accuracy: 0.5673\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0864 - accuracy: 0.9623 - val_loss: 1.5787 - val_accuracy: 0.5673\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0836 - accuracy: 0.9623 - val_loss: 1.8898 - val_accuracy: 0.5673\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0701 - accuracy: 0.9623 - val_loss: 2.0278 - val_accuracy: 0.5673\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0866 - accuracy: 0.9717 - val_loss: 2.2206 - val_accuracy: 0.5673\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0752 - accuracy: 0.9717 - val_loss: 2.3810 - val_accuracy: 0.5481\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0498 - accuracy: 0.9811 - val_loss: 2.4336 - val_accuracy: 0.5577\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 2.4838 - val_accuracy: 0.5481\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0504 - accuracy: 0.9811 - val_loss: 2.6626 - val_accuracy: 0.5577\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0320 - accuracy: 0.9906 - val_loss: 2.6078 - val_accuracy: 0.5769\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0391 - accuracy: 0.9906 - val_loss: 2.5897 - val_accuracy: 0.5865\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0561 - accuracy: 0.9623 - val_loss: 2.6222 - val_accuracy: 0.5865\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0300 - accuracy: 0.9811 - val_loss: 2.6246 - val_accuracy: 0.5769\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0318 - accuracy: 0.9906 - val_loss: 2.6975 - val_accuracy: 0.5673\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0324 - accuracy: 0.9811 - val_loss: 2.7845 - val_accuracy: 0.5673\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 2.9321 - val_accuracy: 0.5577\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.4016 - accuracy: 0.9245 - val_loss: 2.8588 - val_accuracy: 0.5481\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.4243 - accuracy: 0.8962 - val_loss: 1.8144 - val_accuracy: 0.6058\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2479 - accuracy: 0.8962 - val_loss: 1.4517 - val_accuracy: 0.5769\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.1466 - accuracy: 0.9528 - val_loss: 1.6162 - val_accuracy: 0.5481\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.2280 - accuracy: 0.9057 - val_loss: 1.6217 - val_accuracy: 0.5673\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.2172 - accuracy: 0.9340 - val_loss: 1.4018 - val_accuracy: 0.5481\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.1607 - accuracy: 0.9340 - val_loss: 1.2381 - val_accuracy: 0.5769\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.1974 - accuracy: 0.9245 - val_loss: 1.6323 - val_accuracy: 0.5673\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.2736 - accuracy: 0.9340 - val_loss: 1.7223 - val_accuracy: 0.5865\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.1198 - accuracy: 0.9528 - val_loss: 1.8764 - val_accuracy: 0.5481\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.1801 - accuracy: 0.9528 - val_loss: 1.7908 - val_accuracy: 0.5481\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.2366 - accuracy: 0.9245 - val_loss: 1.6805 - val_accuracy: 0.5481\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.2079 - accuracy: 0.9340 - val_loss: 1.7705 - val_accuracy: 0.5577\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.1977 - accuracy: 0.9245 - val_loss: 1.8211 - val_accuracy: 0.5481\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.1455 - accuracy: 0.9434 - val_loss: 1.8237 - val_accuracy: 0.5481\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.1445 - accuracy: 0.9717 - val_loss: 1.8260 - val_accuracy: 0.5481\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.1305 - accuracy: 0.9528 - val_loss: 1.9027 - val_accuracy: 0.5481\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.1061 - accuracy: 0.9623 - val_loss: 2.1222 - val_accuracy: 0.5481\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0654 - accuracy: 0.9623 - val_loss: 2.2744 - val_accuracy: 0.5769\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1117 - accuracy: 0.9528 - val_loss: 2.0859 - val_accuracy: 0.5865\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.1391 - accuracy: 0.9340 - val_loss: 2.0063 - val_accuracy: 0.6346\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.4656 - accuracy: 0.8868 - val_loss: 2.2980 - val_accuracy: 0.5481\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.3396 - accuracy: 0.8679 - val_loss: 1.8421 - val_accuracy: 0.5288\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.3329 - accuracy: 0.8585 - val_loss: 1.5957 - val_accuracy: 0.5481\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.1287 - accuracy: 0.9717 - val_loss: 1.4843 - val_accuracy: 0.5481\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0974 - accuracy: 0.9717 - val_loss: 1.3927 - val_accuracy: 0.6058\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0694 - accuracy: 0.9811 - val_loss: 1.6891 - val_accuracy: 0.5577\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 2.1106 - val_accuracy: 0.5481\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0630 - accuracy: 0.9811 - val_loss: 2.2934 - val_accuracy: 0.5288\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0436 - accuracy: 0.9906 - val_loss: 2.5226 - val_accuracy: 0.5385\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0488 - accuracy: 0.9906 - val_loss: 2.5329 - val_accuracy: 0.5385\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 2.7149 - val_accuracy: 0.5481\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 2.8071 - val_accuracy: 0.5481\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 3.0474 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 3.3235 - val_accuracy: 0.5385\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.4537 - val_accuracy: 0.5385\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.4902 - val_accuracy: 0.5385\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5080 - val_accuracy: 0.5385\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.4318 - val_accuracy: 0.5385\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.3757 - val_accuracy: 0.5385\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.4077 - val_accuracy: 0.5385\n",
      "Epoch 412/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.4895 - val_accuracy: 0.5385\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.7428 - val_accuracy: 0.5385\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.8849 - val_accuracy: 0.5385\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.9036 - val_accuracy: 0.5385\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.9789 - val_accuracy: 0.5385\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.1314 - val_accuracy: 0.5385\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 4.1465 - val_accuracy: 0.5385\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1030 - val_accuracy: 0.5385\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.1983 - val_accuracy: 0.5385\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.2795 - val_accuracy: 0.5385\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 7.6663e-04 - accuracy: 1.0000 - val_loss: 4.3112 - val_accuracy: 0.5385\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.3929 - val_accuracy: 0.5385\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 4.3580e-04 - accuracy: 1.0000 - val_loss: 4.4785 - val_accuracy: 0.5385\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.4975 - val_accuracy: 0.5385\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.7979e-04 - accuracy: 1.0000 - val_loss: 4.5034 - val_accuracy: 0.5385\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.5456 - val_accuracy: 0.5385\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 5.6686e-04 - accuracy: 1.0000 - val_loss: 4.5845 - val_accuracy: 0.5385\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 4.9212 - val_accuracy: 0.5385\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0188 - accuracy: 0.9906 - val_loss: 5.1915 - val_accuracy: 0.5385\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 5.3250 - val_accuracy: 0.5385\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.3241 - val_accuracy: 0.5385\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 5.3111 - val_accuracy: 0.5385\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 5.1801 - val_accuracy: 0.5385\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 5.0590 - val_accuracy: 0.5385\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 6.4338e-04 - accuracy: 1.0000 - val_loss: 4.9623 - val_accuracy: 0.5385\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 3.6598e-04 - accuracy: 1.0000 - val_loss: 4.8693 - val_accuracy: 0.5481\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 2.6061e-04 - accuracy: 1.0000 - val_loss: 4.8523 - val_accuracy: 0.5481\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 5.2506 - val_accuracy: 0.5385\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 5.4377 - val_accuracy: 0.5385\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 5.3056 - val_accuracy: 0.5385\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.2931 - val_accuracy: 0.5385\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 5.0000e-04 - accuracy: 1.0000 - val_loss: 5.3092 - val_accuracy: 0.5385\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 9.6165e-04 - accuracy: 1.0000 - val_loss: 5.2687 - val_accuracy: 0.5385\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 5.2978 - val_accuracy: 0.5385\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.2720e-04 - accuracy: 1.0000 - val_loss: 5.3142 - val_accuracy: 0.5385\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 5.3441 - val_accuracy: 0.5385\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 5.3963 - val_accuracy: 0.5385\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 1.5517e-04 - accuracy: 1.0000 - val_loss: 5.4313 - val_accuracy: 0.5385\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 7.6762e-04 - accuracy: 1.0000 - val_loss: 5.4873 - val_accuracy: 0.5385\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 1.1672e-04 - accuracy: 1.0000 - val_loss: 5.5306 - val_accuracy: 0.5385\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 5.7235 - val_accuracy: 0.5385\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.5174 - val_accuracy: 0.5673\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0489 - accuracy: 0.9906 - val_loss: 5.4818 - val_accuracy: 0.5481\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0725 - accuracy: 0.9717 - val_loss: 5.1913 - val_accuracy: 0.5481\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0289 - accuracy: 0.9811 - val_loss: 5.1632 - val_accuracy: 0.5288\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0505 - accuracy: 0.9906 - val_loss: 4.4831 - val_accuracy: 0.5481\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.2727 - accuracy: 0.9340 - val_loss: 3.3909 - val_accuracy: 0.5192\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0420 - accuracy: 0.9717 - val_loss: 3.1524 - val_accuracy: 0.5288\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0919 - accuracy: 0.9717 - val_loss: 2.9479 - val_accuracy: 0.5577\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0308 - accuracy: 0.9906 - val_loss: 3.0197 - val_accuracy: 0.5481\n",
      "Epoch 462/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0250 - accuracy: 0.9906 - val_loss: 3.3425 - val_accuracy: 0.5577\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0494 - accuracy: 0.9717 - val_loss: 3.2950 - val_accuracy: 0.5481\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 3.2842 - val_accuracy: 0.5481\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0148 - accuracy: 0.9906 - val_loss: 3.3547 - val_accuracy: 0.5481\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.5074 - val_accuracy: 0.5481\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.6739 - val_accuracy: 0.5481\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.8138 - val_accuracy: 0.5481\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0142 - accuracy: 0.9906 - val_loss: 3.9363 - val_accuracy: 0.5481\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.0639 - val_accuracy: 0.5481\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 4.1492 - val_accuracy: 0.5481\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 4.2087 - val_accuracy: 0.5481\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.2477 - val_accuracy: 0.5481\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.2945 - val_accuracy: 0.5481\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.3363 - val_accuracy: 0.5481\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 4.4116 - val_accuracy: 0.5385\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 6.3624e-04 - accuracy: 1.0000 - val_loss: 4.4721 - val_accuracy: 0.5385\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 7.9551e-04 - accuracy: 1.0000 - val_loss: 4.5105 - val_accuracy: 0.5385\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.5437 - val_accuracy: 0.5385\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 4.5711 - val_accuracy: 0.5385\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6439 - val_accuracy: 0.5288\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.6758 - val_accuracy: 0.5288\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 2.3844e-04 - accuracy: 1.0000 - val_loss: 4.6994 - val_accuracy: 0.5288\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7190 - val_accuracy: 0.5288\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.7529 - val_accuracy: 0.5288\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 3.8923e-04 - accuracy: 1.0000 - val_loss: 4.7887 - val_accuracy: 0.5288\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 7.4315e-04 - accuracy: 1.0000 - val_loss: 4.8156 - val_accuracy: 0.5288\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 6.7713e-04 - accuracy: 1.0000 - val_loss: 4.8391 - val_accuracy: 0.5288\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 4.8887 - val_accuracy: 0.5288\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 5.3094e-04 - accuracy: 1.0000 - val_loss: 4.9310 - val_accuracy: 0.5288\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0107 - accuracy: 0.9906 - val_loss: 5.0416 - val_accuracy: 0.5288\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 9.4230e-04 - accuracy: 1.0000 - val_loss: 5.1603 - val_accuracy: 0.5288\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 6.5753e-04 - accuracy: 1.0000 - val_loss: 5.1730 - val_accuracy: 0.5288\n",
      "Epoch 494/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 5.1973 - val_accuracy: 0.5385\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 5.2542 - val_accuracy: 0.5385\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 6.8636e-04 - accuracy: 1.0000 - val_loss: 5.2867 - val_accuracy: 0.5385\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 6.9939e-04 - accuracy: 1.0000 - val_loss: 5.3162 - val_accuracy: 0.5385\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 1.7819e-04 - accuracy: 1.0000 - val_loss: 5.3349 - val_accuracy: 0.5385\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 5.7600e-04 - accuracy: 1.0000 - val_loss: 5.3495 - val_accuracy: 0.5385\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 3.0910e-04 - accuracy: 1.0000 - val_loss: 5.3638 - val_accuracy: 0.5385\n",
      "0.8846153616905212\n"
     ]
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 1, 500, 20\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(train, train_target, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_data=(validation, validation_target))\n",
    "# evaluate model\n",
    "_,accuracy = model.evaluate(test, test_target, batch_size=batch_size, verbose=0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY_3: USE LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(seq_len, n_features)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100)               42000     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,101\n",
      "Trainable params: 42,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 3.5808 - val_loss: 1.3428\n",
      "Epoch 2/600\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7896 - val_loss: 0.7402\n",
      "Epoch 3/600\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6555 - val_loss: 0.6775\n",
      "Epoch 4/600\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.8587 - val_loss: 0.9747\n",
      "Epoch 5/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.7540 - val_loss: 0.8294\n",
      "Epoch 6/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6689 - val_loss: 0.7248\n",
      "Epoch 7/600\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6385 - val_loss: 0.7772\n",
      "Epoch 8/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6132 - val_loss: 0.7053\n",
      "Epoch 9/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5862 - val_loss: 1.1157\n",
      "Epoch 10/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6128 - val_loss: 0.9219\n",
      "Epoch 11/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5677 - val_loss: 1.0617\n",
      "Epoch 12/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.4341 - val_loss: 0.9447\n",
      "Epoch 13/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.4197 - val_loss: 0.6481\n",
      "Epoch 14/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 4.9112 - val_loss: 4.2713\n",
      "Epoch 15/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.9593 - val_loss: 4.3648\n",
      "Epoch 16/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.4555 - val_loss: 2.6200\n",
      "Epoch 17/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6746 - val_loss: 1.8767\n",
      "Epoch 18/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5960 - val_loss: 2.8470\n",
      "Epoch 19/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5600 - val_loss: 3.3821\n",
      "Epoch 20/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5340 - val_loss: 2.5086\n",
      "Epoch 21/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5139 - val_loss: 3.7302\n",
      "Epoch 22/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5107 - val_loss: 3.0743\n",
      "Epoch 23/600\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.8937 - val_loss: 6.7408\n",
      "Epoch 24/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.4387 - val_loss: 2.7941\n",
      "Epoch 25/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.5445 - val_loss: 3.6366\n",
      "Epoch 26/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.7357 - val_loss: 4.3341\n",
      "Epoch 27/600\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 4.7820 - val_loss: 4.3338\n",
      "Epoch 28/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 4.7771 - val_loss: 4.3279\n",
      "Epoch 29/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.5833 - val_loss: 2.6761\n",
      "Epoch 30/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6456 - val_loss: 3.0032\n",
      "Epoch 31/600\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.5123 - val_loss: 2.6961\n",
      "Epoch 32/600\n",
      "22/22 [==============================] - 0s 23ms/step - loss: 0.8179 - val_loss: 3.0767\n",
      "Epoch 33/600\n",
      "22/22 [==============================] - 0s 23ms/step - loss: 2.4460 - val_loss: 3.0066\n",
      "Epoch 34/600\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 3.9253 - val_loss: 4.1142\n",
      "Epoch 35/600\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 2.7477 - val_loss: 3.0245\n",
      "Epoch 36/600\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 2.9698 - val_loss: 3.0259\n",
      "Epoch 37/600\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 3.5343 - val_loss: 3.1618\n",
      "Epoch 38/600\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.7663 - val_loss: 3.0884\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2978ae73a60>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, \n",
    "          train_target, \n",
    "          epochs=600, \n",
    "          batch_size=5, \n",
    "          callbacks=[early_stop], \n",
    "          validation_data=(validation,validation_target),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 17],\n",
       "       [19, 35]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 60, 4), (104, 60, 4))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(seq_len, 4)))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100)               42000     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,201\n",
      "Trainable params: 52,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.7036 - accuracy: 0.4906 - val_loss: 0.7019 - val_accuracy: 0.5288\n",
      "Epoch 2/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6514 - accuracy: 0.5566 - val_loss: 0.9807 - val_accuracy: 0.5385\n",
      "Epoch 3/600\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5658 - accuracy: 0.6887 - val_loss: 2.3561 - val_accuracy: 0.6154\n",
      "Epoch 4/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.7006 - accuracy: 0.7547 - val_loss: 0.8485 - val_accuracy: 0.5962\n",
      "Epoch 5/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5856 - accuracy: 0.7453 - val_loss: 1.1787 - val_accuracy: 0.5385\n",
      "Epoch 6/600\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5634 - accuracy: 0.7358 - val_loss: 0.6203 - val_accuracy: 0.7019\n",
      "Epoch 7/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5598 - accuracy: 0.7547 - val_loss: 0.6719 - val_accuracy: 0.5962\n",
      "Epoch 8/600\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4952 - accuracy: 0.7453 - val_loss: 1.2557 - val_accuracy: 0.5673\n",
      "Epoch 9/600\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5093 - accuracy: 0.7830 - val_loss: 0.6445 - val_accuracy: 0.6154\n",
      "Epoch 10/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5273 - accuracy: 0.7170 - val_loss: 0.8221 - val_accuracy: 0.5481\n",
      "Epoch 11/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4724 - accuracy: 0.7547 - val_loss: 0.9478 - val_accuracy: 0.6346\n",
      "Epoch 12/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3792 - accuracy: 0.8679 - val_loss: 0.7797 - val_accuracy: 0.6635\n",
      "Epoch 13/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5797 - accuracy: 0.7075 - val_loss: 0.7111 - val_accuracy: 0.5865\n",
      "Epoch 14/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6766 - accuracy: 0.5660 - val_loss: 0.6624 - val_accuracy: 0.5769\n",
      "Epoch 15/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.7172 - accuracy: 0.6038 - val_loss: 0.6297 - val_accuracy: 0.6154\n",
      "Epoch 16/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6009 - accuracy: 0.6509 - val_loss: 0.7201 - val_accuracy: 0.5000\n",
      "Epoch 17/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5609 - accuracy: 0.6132 - val_loss: 1.0624 - val_accuracy: 0.4904\n",
      "Epoch 18/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6739 - accuracy: 0.6981 - val_loss: 0.7294 - val_accuracy: 0.5192\n",
      "Epoch 19/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6151 - accuracy: 0.6321 - val_loss: 0.7966 - val_accuracy: 0.6058\n",
      "Epoch 20/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6755 - accuracy: 0.6981 - val_loss: 0.9686 - val_accuracy: 0.6058\n",
      "Epoch 21/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5491 - accuracy: 0.7358 - val_loss: 0.6016 - val_accuracy: 0.6058\n",
      "Epoch 22/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6111 - accuracy: 0.6038 - val_loss: 0.6333 - val_accuracy: 0.6154\n",
      "Epoch 23/600\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5780 - accuracy: 0.6321 - val_loss: 0.7346 - val_accuracy: 0.5962\n",
      "Epoch 24/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5290 - accuracy: 0.7075 - val_loss: 0.7694 - val_accuracy: 0.6731\n",
      "Epoch 25/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4901 - accuracy: 0.7264 - val_loss: 0.6751 - val_accuracy: 0.6538\n",
      "Epoch 26/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5067 - accuracy: 0.8019 - val_loss: 0.8699 - val_accuracy: 0.6442\n",
      "Epoch 27/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4564 - accuracy: 0.7830 - val_loss: 0.9793 - val_accuracy: 0.6442\n",
      "Epoch 28/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.7610 - accuracy: 0.6509 - val_loss: 0.6291 - val_accuracy: 0.6731\n",
      "Epoch 29/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6787 - accuracy: 0.6038 - val_loss: 0.6485 - val_accuracy: 0.6058\n",
      "Epoch 30/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5166 - accuracy: 0.7830 - val_loss: 1.0059 - val_accuracy: 0.5962\n",
      "Epoch 31/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4632 - accuracy: 0.8302 - val_loss: 0.9403 - val_accuracy: 0.5865\n",
      "Epoch 32/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4074 - accuracy: 0.8585 - val_loss: 0.9584 - val_accuracy: 0.6154\n",
      "Epoch 33/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3111 - accuracy: 0.9057 - val_loss: 1.4437 - val_accuracy: 0.6250\n",
      "Epoch 34/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2930 - accuracy: 0.8962 - val_loss: 1.7929 - val_accuracy: 0.6058\n",
      "Epoch 35/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3187 - accuracy: 0.8774 - val_loss: 1.3909 - val_accuracy: 0.6442\n",
      "Epoch 36/600\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2753 - accuracy: 0.8962 - val_loss: 1.1884 - val_accuracy: 0.5288\n",
      "Epoch 37/600\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 1.0704 - accuracy: 0.5849 - val_loss: 1.1532 - val_accuracy: 0.5769\n",
      "Epoch 38/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6034 - accuracy: 0.7075 - val_loss: 0.8282 - val_accuracy: 0.5865\n",
      "Epoch 39/600\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5274 - accuracy: 0.6698 - val_loss: 0.7260 - val_accuracy: 0.6250\n",
      "Epoch 40/600\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5091 - accuracy: 0.7264 - val_loss: 0.9817 - val_accuracy: 0.5865\n",
      "Epoch 41/600\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4926 - accuracy: 0.7264 - val_loss: 1.0163 - val_accuracy: 0.5962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29794fac3d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "chk = ModelCheckpoint('best_model', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(train, train_target, epochs=600, batch_size=32, callbacks=[early_stop], validation_data=(validation,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAABsI0lEQVR4nO2dd3hT5fuH75Mm3XsvWvZuSykbGYKCMkQQF7gQBw6cOHCi4viK4oKfCgqIgoIgAm4RZMsotFB2KaMt0L3SmXF+f5wmtNC90qbvfV29oMkZT06TT57zrFeSZRmBQCAQtHxUljZAIBAIBA2DEHSBQCCwEoSgCwQCgZUgBF0gEAisBCHoAoFAYCWoLXVib29vuW3btpY6vUAgELRIoqOj02VZ9qnoOYsJetu2bdm/f7+lTi8QCAQtEkmSzlX2nAi5CAQCgZUgBF0gEAisBCHoAoFAYCVYLIYusD50Oh1JSUkUFRVZ2hQBYG9vT3BwMBqNxtKmCJoIIeiCBiMpKQkXFxfatm2LJEmWNqdVI8syGRkZJCUl0a5dO0ubI2giRMhF0GAUFRXh5eUlxLwZIEkSXl5e4m6plSEEXdCgCDFvPoi/Reuj1Qn6Lwm/oC3RWtoMgUAgaHBalaBf0F5g9vbZ/Hn2T0ubImgknJ2dLW2CQGAxWpWg5xTnAJBXkmdhSwQCgaDhaVWCrtVpy/0rsF5kWea5556jZ8+ehIWFsWrVKgAuXrzI0KFD6dWrFz179mT79u0YDAbuu+8+87YfffSRha0XCOpGqypbNHnm+bp8C1ti/byx8QhHL+Q26DG7B7ry+vgeNdr2p59+IiYmhtjYWNLT0+nbty9Dhw5l5cqVjB49mpdffhmDwUBBQQExMTEkJycTFxcHQHZ2doPaLRA0Fa3SQxeCbv3s2LGDO++8ExsbG/z8/Bg2bBj79u2jb9++LF26lDlz5nD48GFcXFxo3749CQkJzJw5kz/++ANXV1dLmy8Q1IlW6aGLkEvjU1NPurGobPHzoUOHsm3bNn799VfuvvtunnvuOe655x5iY2P5888/WbhwIatXr2bJkiVNbLFAUH9al4deWq5YoCuwsCWCxmbo0KGsWrUKg8FAWloa27Zto1+/fpw7dw5fX18efPBBpk+fzoEDB0hPT8doNHLLLbfw1ltvceDAAUubLxDUiVbloYukaOth4sSJ7N69m4iICCRJ4v3338ff359vvvmGefPmodFocHZ2Zvny5SQnJzNt2jSMRiMA7777roWtFwjqRqsSdJEUtX60WuXLWpIk5s2bx7x588o9f++993LvvfdetZ/wygXWQOsKuYikqEAgsGJapaCLkItAILBGWpegl0mKVlYFIRAIBC2VVinoBtlAkUGMFRUIBNZFqxL0PN3lGS4iji4QCKyNViXo2hItHnYegBB0gUBgfbQaQTcYDRToC/B38gdEYlQgEFgfrUbQTQLu5+gHiG5RQf3Q6/WWNkEguIrWJ+hOiqCLVYusl5tvvpmoqCh69OjBokWLAPjjjz/o3bs3ERERjBw5ElCakKZNm0ZYWBjh4eGsXbsWKL9Ixpo1a7jvvvsAuO+++3jmmWe49tpreeGFF9i7dy+DBg0iMjKSQYMGceLECQAMBgOzZs0yH/ezzz7jn3/+YeLEiebj/v3330yaNKkpLoegFdFqOkVNAm4KueTrRQy9Ufn9Rbh0uGGP6R8GN75X7WZLlizB09OTwsJC+vbty4QJE3jwwQfZtm0b7dq1IzMzE4C33noLNzc3Dh9W7MzKyqr22CdPnmTTpk3Y2NiQm5vLtm3bUKvVbNq0iZdeeom1a9eyaNEizpw5w8GDB1Gr1WRmZuLh4cFjjz1GWloaPj4+LF26lGnTptXveggEV9BqBN3U9m8KueSXCEG3Vj799FPWrVsHQGJiIosWLWLo0KG0a9cOAE9PTwA2bdrEDz/8YN7Pw8Oj2mPfeuut2NjYAJCTk8O9997LqVOnkCQJnU5nPu6MGTNQq9Xlznf33Xfz3XffMW3aNHbv3s3y5csb6BULBAqtRtBNVS0iKdpE1MCTbgz+/fdfNm3axO7du3F0dGT48OFERESYwyFlkWUZSZKuerzsY0VF5fsVnJyczP9/9dVXufbaa1m3bh1nz55l+PDhVR532rRpjB8/Hnt7e2699Vaz4AsEDUWriaGbatC9HbxRSSpRtmil5OTk4OHhgaOjI8ePH+e///6juLiYrVu3cubMGQBzyGXUqFEsWLDAvK8p5OLn58exY8cwGo1mT7+ycwUFBQGwbNky8+OjRo3iiy++MCdOTecLDAwkMDCQuXPnmuPyAkFD0moE3RRDd7F1wUnjJATdSrnhhhvQ6/WEh4fz6quvMmDAAHx8fFi0aBGTJk0iIiKC22+/HYBXXnmFrKwsevbsSUREBFu2bAHgvffeY9y4cYwYMYKAgIBKz/X8888ze/ZsBg8ejMFgMD/+wAMPEBISQnh4OBEREaxcudL83NSpU2nTpg3du3dvpCsgaM1Ilppp0qdPH3n//v1Ndr6vDn/FJwc+Yf9d+xm3bhz9/fsz95q5TXb+1sCxY8fo1q2bpc1o1jz++ONERkYyffr0Jjmf+JtYH5IkRcuy3Kei51pNEC+vJA+1So2tyhZnjbPw0AVNTlRUFE5OTnz44YeWNkVgpbQaQdeWaHHRuCBJEo4aR5EUFTQ50dHRljZBYOW0mhh6ni4PZ1ulYcRZ4yw6RQUCgdXRagRdW6LFWaMIupPGSXjoAoHA6mg9gq7T4mLrAiCqXAQCgVXSagQ9ryTP7KGLpKhAILBGWo2g5+vyzTF0R40j+bp8sQydQCCwKqoVdEmS2kiStEWSpGOSJB2RJOnJCraRJEn6VJKkeEmSDkmS1LtxzK072pLLIRdnjTMyMoX6QgtbJbAkZacqXsnZs2fp2bNnE1ojENSfmnjoeuBZWZa7AQOAxyRJurLN7UagU+nPQ8DnDWplPTHKRrS68klREPNcBAKBdVFtHbosyxeBi6X/z5Mk6RgQBBwts9kEYLmsxDD+kyTJXZKkgNJ9LU6BrgAZuVxSFMQydI3J//b+j+OZxxv0mF09u/JCvxcqff6FF14gNDSURx99FIA5c+YgSRLbtm0jKysLnU7H3LlzmTBhQq3OW1RUxCOPPML+/ftRq9XMnz+fa6+9liNHjjBt2jRKSkowGo2sXbuWwMBAbrvtNpKSkjAYDLz66qvmUQMCQWNTq8YiSZLaApHAniueCgISy/yeVPpYOUGXJOkhFA+ekJCQWppad0yeeNmkKAhBtzbuuOMOnnrqKbOgr169mj/++IOnn34aV1dX0tPTGTBgADfddFOF0xArY+HChQAcPnyY48ePM2rUKE6ePMkXX3zBk08+ydSpUykpKcFgMPDbb78RGBjIr7/+CigDvASCpqLGgi5JkjOwFnhKluXcK5+uYJerMo6yLC8CFoEyy6UWdtYL0yx0J1vFM3fUOAIi5NKYVOVJNxaRkZGkpqZy4cIF0tLS8PDwICAggKeffppt27ahUqlITk4mJSUFf3//Gh93x44dzJw5E4CuXbsSGhrKyZMnGThwIG+//TZJSUlMmjSJTp06ERYWxqxZs3jhhRcYN24cQ4YMaayXKxBcRY2qXCRJ0qCI+QpZln+qYJMkoE2Z34OBC/U3r2EwCbeL5nJSFISHbo1MnjyZNWvWsGrVKu644w5WrFhBWloa0dHRxMTE4Ofnd9WM8+qorBpqypQpbNiwAQcHB0aPHs3mzZvp3Lkz0dHRhIWFMXv2bN58882GeFkCQY2oSZWLBHwNHJNleX4lm20A7imtdhkA5DSX+Dlc9tDLtv6DEHRr5I477uCHH35gzZo1TJ48mZycHHx9fdFoNGzZsoVz587V+phDhw5lxYoVgLIE3fnz5+nSpQsJCQm0b9+eJ554gptuuolDhw5x4cIFHB0dueuuu5g1axYHDhxo6JcoEFRKTUIug4G7gcOSJMWUPvYSEAIgy/IXwG/AGCAeKACa1WKJ5lnopR66KeQiBN366NGjB3l5eQQFBREQEMDUqVMZP348ffr0oVevXnTt2rXWx3z00UeZMWMGYWFhqNVqli1bhp2dHatWreK7775Do9Hg7+/Pa6+9xr59+3juuedQqVRoNBo+/7xZFXwJrJyaVLnsoOIYedltZOCxhjKqoTEnRU0euq3w0K0Z06LPAN7e3uzevbvC7bTaynMobdu2JS4uDgB7e/tyKxKZmD17NrNnzy732OjRoxk9enQdrBYI6k+r6BQ1h1xKQy22KlvUktrsuQsEAoE10Crmoefr8rGRbHBQOwDKIsBOtmJAl0Dx5u++++5yj9nZ2bFnz5WVuQJB86dVCHpeiTILvWztsRjQJQAICwsjJibG0mYIBA1Cqwi5lG37N2Ea0CUQCATWQusQ9DKDuUwID10gEFgbrULQ83R55vktJsS6ogKBwNpoFYJuWiC6LMJDFwgE1kbrEHSd1lx7bkIIuqCqeegCQUukVQh62eXnTIikqKC5oNfrLW2CwEqw+rJFWZbJ1+VXmBQt0BdgMBqwUdlYyDrr5dI771B8rGHnodt164r/Sy9V+nxDzkPXarVMmDChwv2WL1/OBx98gCRJhIeH8+2335KSksKMGTNISEgA4PPPPycwMJBx48aZO04/+OADtFotc+bMYfjw4QwaNIidO3dy00030blzZ+bOnUtJSQleXl6sWLECPz8/tFotM2fOZP/+/UiSxOuvv052djZxcXF89NFHACxevJhjx44xf35lo5YErQWrF/RCfSEG2XBVyMWUJC3QF1wl9oKWSUPOQ7e3t2fdunVX7Xf06FHefvttdu7cibe3N5mZmQA88cQTDBs2jHXr1mEwGNBqtWRlZVV5juzsbLZu3QpAVlYW//33H5Ik8dVXX/H+++/z4Ycf8tZbb+Hm5mYeZ5CVlYWtrS3h4eG8//77aDQali5dypdfflnfyyewAqxe0K9c3MJE2VWLhKA3PFV50o1FQ85Dl2WZl1566ar9Nm/ezOTJk/H29gbA09MTgM2bN7N8+XIAbGxscHNzq1bQy65klJSUxO23387FixcpKSmhXbt2AGzatIkffvjBvJ2HhwcAI0aM4JdffqFbt27odDrCwsJqebUE1oj1C7pp0mIFIRcQA7qsDdM89EuXLl01D12j0dC2bdsazUOvbD9Zlmu82pFarcZoNJp/v/K8Tk6XS2lnzpzJM888w0033cS///7LnDlzACo93wMPPMA777xD165dmTatWQ03FVgQq0+K5unKD+YyIUboWicNNQ+9sv1GjhzJ6tWrycjIADCHXEaOHGkelWswGMjNzcXPz4/U1FQyMjIoLi7ml19+qfJ8QUFBAHzzzTfmx0eNGsWCBQvMv5u8/v79+5OYmMjKlSu58847a3p5WhTnc88z8seRJGQnWNqUFoPVC3p1HrpoLrIuKpqHvn//fvr06cOKFStqPA+9sv169OjByy+/zLBhw4iIiOCZZ54B4JNPPmHLli2EhYURFRXFkSNH0Gg0vPbaa/Tv359x48ZVee45c+Zw6623MmTIEHM4B+CVV14hKyuLnj17EhERwZYtW8zP3XbbbQwePNgchrE2tidvJ7UglV0XdlnalJaDLMsW+YmKipKbgt/P/C73XNZTPpl5stzjxzOOyz2X9ZT/OvtXk9jRGjh69KilTWhVjB07Vt60aVOV27Tkv8msf2fJPZf1lGdvm21pU5oVwH65El1ttR562aSoQNCSyM7OpnPnzjg4ODBy5EhLm9NoHEw9CMDRjKMWtqTl0GqSolfG0EVSVAAtcx66u7s7J0+etLQZjcql/EukFKTg7eDNmdwzFOgKzHkvQeVYvaDn6fKQkK56MwgPvXGQa1EF0hyw5nnoyt15y8Tknd/W+Tb+L/b/OJF1gkjfSAtb1fxpFSEXZ40zKqn8S9XYaLBV2YqkaANib29PRkZGixYSa0GWZTIyMrC3t7e0KXXiYOpBHNQO3NzxZkCEXWqK1XvoFQ3mMuGkcSK/RHjoDUVwcDBJSUmkpaVZ2hQByhdscHCwpc2oEzGpMYT7hOPv5I+XvZcQ9Bpi/YJeUo2g64WgNxQajcbc4SgQ1JUCXQEns07yQNgDSJJEd6/uQtBriPWHXHRXz0I34WzrLDx0gaCZcSj9EAbZYI6Zd/fqTkJOAgW6Agtb1vyxekE3LRBdEY5qR+GhCwTNjIOpB5GQCPcJBxRBN8pGTmZZd2VPQ2D1gl7RAtEmnG2dzWWNAoGgeRCbGktHj47m3pHuXt0BOJJxxJJmtQisX9ArWCDahJPaSZQtCgTNCIPRQGxaLJE+l0sU/Rz98LT3FHH0GmDVgi7LcoULRJtwshWCLhA0J+Kz49HqtPTy7WV+TCRGa45VC3qxoRi9UV+phy7WFRUImhcxqTEAVzURmRKjhfpCC1jVcrBqQa9scQsTjhpHigxF6I1iTUeBoDkQkxaDt4M3Qc5B5R43JUZPZJ6wkGUtA6sW9LyS0lnolVS5iHkuAkHz4mDqQSJ9I68aH9HDqwcgOkarw6oF3TxpsZI6dDHPRSBoPqQVpJGsTaaXT6+rnhOJ0Zph3YJuCrlU0SkKQtAFguaAaSBX2YSoCUmS6ObVjaOZQtCronUIemV16CLkIhA0G2LSYrCzsaObZ7cKn+/u2Z2E7ASK9NWvCdtasW5Br2RxCxPCQxcImg8xqTH09O6JxkZT4fM9vHpgkA2cyBKJ0cqwakGvLilqEnQxQlcgsCyF+kKOZRyrMH5uwtQxKuLolWPVgm4Said1JY1FwkMXCJoFcelx6GV9lYtY+Dv542HnIQS9Cqxa0PNK8nBUO2KjsqnweSHoAkHzIDYtFqg4IWpCdIxWj1ULelWLW4AIuQgEzYWDqQdp79YeNzu3Krfr7tWd09mnRWK0EqoVdEmSlkiSlCpJUlwlzw+XJClHkqSY0p/XGt7MuqEtqXwWOoBapcbexl7MWRYILIhRNhKTGlOld26iu1d3DLJBjNKthJp46MuAG6rZZrssy71Kf96sv1kNQ56u8lnoJpw0TsJDFwgsyJmcM+SW5FaZEDVhSoweyzjWyFa1TKoVdFmWtwGZTWBLg1PV8nMmxLqiAoFlqWwgV0UEOAXgbucuGowqoaFi6AMlSYqVJOl3SZJ6NNAx601Vy8+ZEOuKCgSW5WDqQTzsPAh1Da12W5EYrZqGEPQDQKgsyxHAZ8DPlW0oSdJDkiTtlyRpf1OsDF8TD12sWiQQWJaYtBgifCOuGshVGd29uhOfFU+xobiRLWt51FvQZVnOlWVZW/r/3wCNJEnelWy7SJblPrIs9/Hx8anvqaulRh662okCvUiKCgSWIKMwg3O552oUbjHR3as7elnPqaxTjWhZy6Tegi5Jkr9U+tUqSVK/0mNm1Pe49UVn0FFsKK4+hm7rJDx0gcBCmOvPa5AQNSE6RitHXd0GkiR9DwwHvCVJSgJeBzQAsix/AUwGHpEkSQ8UAnfIsiw3msU1JE9X2vZfyWAuE2JdUYHAcsSkxqBRaejhXfPUW6BTIG52bkLQK6BaQZdl+c5qnl8ALGgwixoIk9ddEw9dCLpAYBli0mLo7tUdOxu7Gu8jSRLdPUVitCKstlO0ph66s8aZEmMJOoOuKcwSCASlyLLMicwT5hBKbeju1Z1T2acoMZQ0gmUtF6sV9OpG55oQ81wEAstwKf8SBfoCOrp3rPW+3b26ozeKxOiVWL2gVxtDF/NcBAKLcDrnNADt3drXel+TV38k40iD2tTSsVpBN4dcatApCsJDFwiamtPZiqB3cO9Q632DnINwtXUVcfQrsFpBr26BaBNC0AUCy5CQk4CnvSce9h613tfUMSo89PJYr6CbFrewrXhxCxOmkIwIuQgETUt8dnydvHMTgwIHcTzzOAk5CQ1oVcvGegW9RIuD2gGNquL1CU2YPHQxQlcgaDpkWSYhO6FO8XMT4zuMRy2p+enkTw1oWcvGegVdp602IQoiKSoQWILUglS0Om29PHRvB2+uDbmWDac3iPLFUqxW0PNKqp+FDiKGLhBYAlOFS11KFssyudNksoqz2Hx+c0OY1eKxWkGvyWAuAEe1IyAEXSBoSkwVLvUJuQAMCBxAkHMQa06taQizWjzWK+glWrP3XRU2Khsc1Y4i5CIQNCGns0/jbueOp71nvY6jklRM7DiRPRf3kJib2EDWtVysVtBrsvycCSeNk0iKCgRNSEKOkhCt6Qz0qri5482oJBVrT61tAMtaNlYr6NoSbbVt/ybEuqICQdMhyzKns0/XO35uws/Jj6HBQ/k5/md0xtY9k8l6Bb2GVS5QugydiKELBE1CemE6uSW5tHevX/y8LJM7TSajKINtidsa7JgtEasUdJ1RR6G+sMYhF2eNsxB0gaCJMFW41Kdk8UoGBw3G19GXH0/92GDHbIlYpaCb4uE1qXIBEXIRCJoS8wwXt4YTdLVKzaROk9iVvIsL2gsNdtyWhlUKel5JzQZzmRBJUYGg6UjITsDV1hVvhwqXHq4zEztOBGBd/LoGPW5LwioF3eRttxQPvdhQzG0bb2PXhV0Ws0EgaCpMM1waosKlLIHOgQwKGsRPp35Cb9Q36LFbClYp6HXx0PN1+VhqKdSzOWc5lnmMPRf3WOT8AkFTIcsyp3NO17uhqDJu7XQrqQWp7Eze2SjHb+5YpaDXdD1RE862zuiNekqMlpkHkZSXVO5fgcBaySzKJKc4p0ETomUZ2mYoXvZerbZz1DoFXVez1YpMmAd0lVgm7JKYp3S4JWuTLXJ+gaCpMI26bSxB16g03NzxZrYnbSclP6VRztGcsUpBN4dcainolkqMmgQ9SSs8dIF10xgVLldyS6dbMMgGfo7/udHO0VyxSkE3J0Vr0Sladr+mxiToOcU55i8jgcAaic+Ox1njjK+jb6Odo41rG/oH9Gdd/DqMsrHRztMcsU5BL9Fiq7LF1sa2RttbeoRuYl4iDmoHQIRdBNZNQk4C7d0bZoZLVUzuNJlkbTI/nvgRg9HQqOdqTliloNdmMBdcDs1UJ+irT6zmQMqBetl2JTqjjov5F4nyiwIgOU8IusB6acgZLlUxImQEnTw6MXfPXMb/PJ4Vx1a0im5wqxT0/JL8GodboGYhlwJdAe/ueZfvjn1Xb/vKckl7CYNsYEDAAEDE0QXWS1ZRFplFmY1WslgWWxtbVo9bzYfDPsTL3ov39r7HdT9ex/v73rfqajK1pQ1oDPJ0eTVOiELNQi6H0g+hl/Wczz1fb/vKcj5POV4Prx64aFys+s0maN2YE6KNVOFyJWqVmlFtRzGq7SgOpx3m22Pf8v2x71lxbAUj2oxgWs9phPuEN4ktTYVVeujaEm2Dh1yiU6IBRYAbsgHJlBBt49KGYJdg4aELrBZzyWIjVrhURphPGO8PfZ/fb/mdaT2msffSXu79416rK220TkGv4fJzJhzUDkhINRL0Qn0haYVp9bbRRGJeInY2dvg4+hDsEiySogKr5XT2aRzVjvg7+VvMBn8nf56KeoqvRn2F3qhn76W9FrOlMbBKQa/pAtEmJEmqciZ6iaGEQ2mHzJ5FQ4ZdEvMSCXYORiWpCHIOIjkvudWVWglaB6dzTjfKDJe60NmjMy62LmZHzVqwSkGvzeIWJqoa0HUk4wjFhmImdlKmuZni3g1BYl4ibVzaABDsHEyJsYS0goa7AxAImgunsxtvhkttsVHZEOUbxf6U/ZY2pUGxOkE3GA3k6/Jr5aFD1asWmb7Fx7Yfi1ql5lzuuXrbCcqgomRtMm1cFUEPcgkCRC26wPrIKc4hvTC9yRKiNaGPfx/O5Z4jtSDV0qY0GFYn6Pl6RZRr66FXtWrR/pT9dHTviLeDN8HOwQ0WckkvTKdQX1jOQwdRuiiwPhp7hktd6OPXB4D9l6zHS7c6QTcN2KpNHTqAo8axQkHXG/XEpMaYG39CXEMaLORStsIFlHnOEpJoLhJYHU1dslgTunh2wUnjZFVhF6sT9NoO5jJRmYd+IusE+br8y4LuEkJiXmKDlC6avhhMgm5rY4uvo6/w0AVWx+ns0zioHQhwCrC0KQAU6QzEJuYR6RspBL05YxLlusTQK0qKmm7HTIIe6hpKob6wQeJuiXmJqCQVgU6B5seCnINEcxFwJP0I60613qXErI3T2adp59YOldQ8JOenA8lM/mIXXdx6cSbnDOmF6ZY2qUFoHle3Aant8nMmKkuKRqdEE+ISYp4OF+ISAjRMpUtiXiIBTgFobDTmx0RzkcIXh77gzd1virVerYTTOU0zw6WmJGUVIMvgQhcAqylftDpBr+3ycyYqWobOKBs5kHrA7J2DEkOHhqlFT8pLItgluNxjwc7BpBWkUWworvfxWypG2UhMagx6Wc+h9EOWNkdQT/JK8kgtSG02JYsAKbnK56tA64+D2sFqEqNWJ+h1TYo6aZwwykaKDEXmx05nnyanOKecoAc4BaBRaTiXV//SxbI16CaCXYKRkbmgvVDv47dUzuacJbs4G7Aez6k10xwToql5yuf8dEqhVcXRqxV0SZKWSJKUKklSXCXPS5IkfSpJUrwkSYckSerd8GbWnDxd3ZOiUH6ei0lMygq6jcqGYJf6ly7mleSRXZxtDuGYMHnsrTmOfiBVGVHsae/Z4OOKBU2PJWe4VEZKriLoJ1K09PHrQ3x2PFlFWU1y7pNZJxvNYauJh74MuKGK528EOpX+PAR8Xn+z6o62RItapcbOxq5W+znZXr2uaHRKNH6OfgQ5B5XbNsSl/qWLV5YsmjCdqzU3Fx1MPYinvSc3truR2LRYdAadpU1q1eTr8skuyq7z/qezT2NvY0+gc2D1GzcRqXlKyOV0qpZIH8UHbUznwWA08M/5f5j+53Ru2XALS+KWNMp5qhV0WZa3AZlVbDIBWC4r/Ae4S5JksdokU9t/bedFOKlLR+iWNibJskx0SjRRflFXHSvENYTE3MR6zVypTNC9Hbyxs7Fr3R56ygF6+/Ymyi+KYkMxRzKOWNqkVs2bu99k9NrRbD6/uU77n85RKlxsVDYNbFndKNIZyC7Q0d7biRKDESepPfY29o0SdsktyeWbI98wdt1YntryFOfzzvNU76d4vNfjDX4uaJgYehCQWOb3pNLHrkKSpIckSdovSdL+tLTGmVeSV1K7WegmTEnU/BJF0BPzEkkrTCsXbjER6hJKkaGoXqWLJkG/MimqklQEOge2Wg89rSCNJG0Skb6R9PZVPCcRR7ccsizz38X/KDIU8eSWJ1l0aFGtezBOZ5+mvXvzSYimlXrn13TyBuBsWhERPhHsu7Svwc6RkJPA3P/mct2P1/HB/g/wc/Tjw2Ef8vuk35keNh13e/cGO1dZGkLQK3KFK/yLy7K8SJblPrIs9/Hx8WmAU19NXklerROioHSKwuUYuklETO3BZTFVuphEuS4k5iXiae9pXlyjLMHOrbd08WDqQQAifSPxcvCirWtbc0xdUDn7Lu1j3r55DTqrH5QxFJlFmczqM4sx7cbw2cHPeH7b8xTqC2u0v7ZEy6X8S80yfj6ogxeSBCdS8ojyj+Jk1klyinPqffwfjv/AhJ8n8NOpnxgVOorV41bzzY3fMKrtKNSqxl1TqCEEPQkoGzcIBixSomGUjcSlx9XJGzB59aY69v0p+/Gw86CdW7urtjUJen2GdFVU4WLC1FzU0B/OlsDB1IPY29jT1asroCSkD6YcbNCFftedWsevCb822PEsTWpBKs/++yzLjy5v8Du7mNQYAPr59+O9Ie/xVO+n+PPsn9z7+71cyr9U7f5ncs4AzavCxVSyGOrlRKinIydT8ujj1wcZuUHi6D8c/4GeXj35e/LfzL1mLt28utX7mDWlIQR9A3BPabXLACBHluWLDXDcWnMk/QhZxVkMCRpS632vXIausvg5gL+jPxqVpl6VLlUJerBLMFqdltyS3Dofv6VyIPUA4T7haFRKs1WUXxR5ujzis+Mb5PgZhRm8vedtPo+1aO6+wTAYDczePtv8XolJi2nQ48emxeKkcaKje0ckSWJ62HQ+G/EZ5/POc/svt5sF/0ou5V/ij7N/8HXc10DzEnRTyaKfqz2d/Vw4cSmPcJ9wbFW29Y6jn805y+mc04zrMA4vB6+GMLdW1KRs8XtgN9BFkqQkSZKmS5I0Q5KkGaWb/AYkAPHAYuDRRrO2GrYnb0dCYnDg4FrvW1bQL+VfIlmbTB//q8MtoJQutnFpU+dKlxJDCSn5KZULegNOXUwrSOOdPe+0iNbmfF0+xzOPE+kbaX7MlMNoqITVyuMrKTYUcy73XL0qN5oLX8d9zd5Le3l1wKs4aZwqFdi6cijtEGHeYeUSmsPaDGPFmBU4a5yZ9uc01p5cS1x6HN8d/Y5ZW2dx/ZrruX7N9Ty39Tl2Ju/k2jbXmt/TzYGU3GI0NhIejhq6+LtwNqMA2agm3Ce83u+zLYlbABjRZkRDmFprqg3oyLJ8ZzXPy8BjDWZRPdiRvIMwn7A6JRzsbeyxkWzI1+Wb/6gVJURNhLiE1DnkkqRNQkau0kMHpRa9h1ePOp3DxNz/5rI5cTM2kg0v9HuhXsdqbA6lHcIoG83JUFAmUAY4BRCdEs3UblPrdfwCXQE/HP9BWRlKm8yh9EMMDR5aX7MtxsHUg/xfzP9xY7sbmdRpEn+e/bNBBb1AV8CJrBM8GPbgVc91cO/AyrErmbV1FnN2zzE/7u/kTy+fXvTq0YtePr3o7NnZfLfVXEjNLcLXxR5Jkujs54LBKJOQlk8f/z4sOrSoznk4gH/O/0M3z24EOFum0M9qOkUzizKJS4+rU7gFlGXoTCN0o1OicdG40Mm9U6Xbh7gqUxfrUrpoKkmsKoYO9a9F33RuE5sTN+Pt4M2ak2vILKqq+tTyHEw9iEpSXbUSe2+/3hxIOVDvnMKak2vILcnlzUFvopJUHEpruWMFcopzeGHbCwQ4BfDagNeQJIlI30hOZZ8q10tRH+LS4zDKRnr59qrweTc7Nz6/7nPeHPQmHwz7gL8n/83fk/9m3rB5TO02lR7ePZqdmAOk5BXh66r0qXTxV4TbFEc3ykZzYr62pBWkcSjtECNCLOOdgxUJ+s7kncjIdRZ0UBKjWp2W6JRoIv0iq6ybDXUNpdhQXKfSxcpq0M122Drjbuder1r03JJc3tnzDl08uvDl9V9SbCjmu6Pf1fl4TcGB1AN08ehy1RyeKL8oMooy6pWE1hl0LD+6nL7+fekX0I/OHp1brKDLsszru14nrSCNecPmma9XL99eGGVjg82/McXjw7zDKt1GrVIzsdNERrcdbdHFn2tDSm4xfi72ALT1ckJjI3EiRYmjq1XqOodd/k36FxlZCHpDsD15O572nvXKKDtpnEjMS+RMzpkqwy1QvyFd53PP46h2xNPes9Jtgp2D6yXoH0d/TEZRBm8MeoPOHp25LvQ6fjj+g3l4WXNDZ9RxKO1Qufi5iShf5W9Rn/LFX8/8SkpBCvf3vB9QROpw+uEWuSD36hOr+ef8PzzZ+0l6evc0Px7uE45KUjVY2CU2LZb2bu1xs3NrkONdRX4G/PESrJkOf78OexfDiT/gUhwUZkMjVXml5hbhV+qh26pVtPd25uSlPBzUDoR5hxF9qW59D5vPbybYObjKO/vGxioE3WA0sOvCLq4JuqZe85adNE7EpsUCVcfP4fIY3boM6TJVuFTVzRrkElTnkEt0SjQ/nvzRfNsLMD1sOnm6PFadWFWnYzY2JzNPUqgvrFDQ27m1w8POo84NRkbZyNK4pXTx6GJOmIf7hKPVac1ldS2FE5kneH/f+wwOGsw9Pe4p95yTxonOHp3rHDIoiyzLxKbFVhpuqRcGvSLen/WGPV9A0j7YvRB+mwXf3w5fDIb/hcK7beD/BsH6x+HgCshMqF7k89Ph2C/w58uwbBwk/Fvu6cISA7lFenxd7c2PdfZ34USK4uj08evDkYwjtR7brC3RsufiHkaGjKx1l3pD0rhV7k3E4fTD5BTn1CvcAkrIxSgbcVA70N2ze5Xb+jv5Y6uyrZOHnpiXWO1s6GDnYP45/w8Go6FWLdMlhhLe2P0GgU6B5dqLe3j1YHDgYL49+i1Tu03FQe1Qa7sbE5P3XZGASJJElF9UnQV9a+JWEnISeG/Ie+YPmylOfyjtULMqqauKAl0Bz297Hlc7V94e/HaFzkuETwQbT2+s9fvmSs7mniWnOIcIn4j6mFzBgXfA7y9AShy0GwY3vg++XcFohPxUyEmCnMTSf5Mg4zQc2wgHv1X2d/aDkAEQMlD5sXOBxD1wbhec/w8yTinbmWY5HVgO7YebT28qWfR1uTzrqYufMxtjL5BfrKePXx8WH15MTGoMg4IG1fhl7Ujegc6os2i4BaxE0Lcnb0clqRgYOLBexzF1i4b7hJdbdKIiVJJKKV2spaAbjAaStclc2+baKrcLcglCb9STWpBaq4z54sOLOZNzhs+v+9z8ekw8EPYA0/6cxrpT65jSbYryoCyDoQTUtRtm1tAcTD1IkHNQpXHY3n692XR+E5fyL9U6VrskbglBzkGMbjva/Fhb17a42LpwKP0QEztNrJftTcX7+97nTM4Zvrz+y0prnCN9I1l1YhWnsk/R1bNrnc9lulNtMEHPSYa/X4W4teDWBm5bDt1uApM3q1KBi7/yE3xFubDRCOknLov2+d1wdH35bezdFIGPnKr8GxipePYJW5T3eOl5TE1FfmU89E5+SmL0VKqWXv69sJFs2Jeyr1aCvvn8ZjztPRv+C7CWWIWg70jeQYRPRL1jfaZu0erCLSbqsmB0akEqOqPuqhkuV1K2Fr2mgh6fFc9Xh79ibPuxXBN0zVXPR/lFEekbybIjy7i1y61oJDX8MBUy4mHGDlDb1uq1NBSyLHMw9SADAgZUuo3pb3Ig5QBj2o+p8bEPpBwgJi2Gl/q/VK7tWiWpCPcObzGJ0aMZR1l7ai3397y/SsfFdIcTkxpTb0F3sXWpsFO6VpTkK2GVbR+A0QDDXoDBT4GtY7W7mlGpwLeb8tN3uvJYdqIi7iVaaNMffLoq25Wl3VA4vBrSjiv7crntv6ygdykV9JOX8ujVpg09vHrUasGLEkMJ25K3MbrtaIsPIGvxMfT0wnSOZhytd7gFLjcXVTS/pSJMC0bXJrFWXYWLCbOg1zAxapSNzNk9B2eNM8/3fb7CbSRJ4oGwB7iYf5HfEn6DvYvgxK+K9xO7ssavoaFJyksivTC9wvi5iS4eygrttQ27fB33NR52Htzc8earngv3CSc+O77CpQebG6tPrMZB7cD0sOlVbhfoFIivg2+94+gxqTHmJGutMejg5F+w9gGY1xH+eRM6jIDH98K1L9VOzCvDvQ2E3wp9poFf96vFHBRBBzizzfyQaWyuKSkK0MbTEXuNyhxHj/KPIi4jrsbzavZd2ke+Lt9izURlafGCviN5B0CFHmltCXAKwEXjUmWZVllCXEMoNhSTkp9S43OYBN1UJVMZ/s7+qCRVjbtFV51YRWxaLM/1fa7K6pkhQUPo4tGFr2L+D8Nfr0KnURAUBds+BH1JjV9HQ2KKn5dtKLoSG5UNvXx71UrQT2WdYlvSNqZ0m1JhziDcJxyjbORIevMez5tbkstvZ35jTLsxuNq6VrmtJElE+EaYQyZ1Ia8kj9PZp+nl06vmOxmNisf8yzPwQWdYeSuc+hvCb4P7/4I7VoBH2zrbVCc8QsE9tLyg5xZhq1bh5nA5pGqjkujk68LJMolRvVFf42u4+fxmHNQODAis/A6zqbAKQfdx8KnX7aWJO7veycaJG7FX21e/MUotOtRuwejEvETUKjX+jlXHgTUqDQFOATWqdLmUf4lPDnzCwICBjG8/vsptJUnigR73cTb/Aptd3WHCQhg+G3LOW8xLP5h6EFdb12qHqkX5RnE653SNV5ZZGrcUB7UDd3atuNnZ9MXd3Nct3Xh6I4X6Qm7rcluNto/0jSRZm1zn8c6H0w4jI9csHlyUA5vfhk8iYMloiFmpJCHv/AFmnYLxn0BI/zrZ0SC0GwpntyvhHpSQi6+L3VWVKKaZLqA4FipJVaOwi1E2siVxC9cEXVPrRXUagxYt6Hqjnl0XdjE4aHCDlAppbDS1GqhjLl2sRcPL+bzzBDkH1SjWZpq6WB3/2/s/DEYDrw58tUbX4fpTuwjV6Vgc1B7ZyQc6XmdRL/1A6gEifSOrvb03x9FrUI9+UXuR38/8zi2dbqk0t+Jm50Zb17b18mYbG1mWWX1iNWHeYXT3qrryyoTJs65rPXpsWiwSUtV3qkYjHPwOPouCbfPAuxNM/BKeOwW3LoUuN1osJ1OOdsOUL51Lypd2Sm5xufi5iS7+zqTmFZOVX4KzrTPh3uGsPbWWtIKq1204nH6YtMI0i1e3mGjRgn4o7RB5JXkNEj+vC35OftjZ2NWq0iUpL6nahKiJYJfgaj30+Kx4Np3fxLSe06qNywOQ8C82/y1kunsEx/KT2XVhl1IBYPbSv6+RbQ1FZlEmZ3LO1Kjeuad3T2xVtjUacbr86HIA7u1xb5XbhfsoidHmOqp4f8p+EnISuL3L7TXep6tXV+xt7OscR49Ni6WTR6erOnbNJEfD19fD+sfAox08tAXu/gki7lDKCJsT7Uq14cx2QClbLBs/N9HZlBgtDbu8OvBVtCVaZm2dhc5Y+RKIm89vRi2pLaZBV9KiBX178nZsJJt6lyvWFXPpYg1DLrIsK01FzjUQXhQPPb0wvcrkzPKjy7G3sa80rFCOgkxY9wh4d2bc2EX4Ofqx+PBi5TmTl779gyb10k1eZFXxcxO2NraE+YRVG0fPKspi7am1jGk/ptoSx3DvcDKLMpvtClE/HP8BV1tXRrt0gG/Gw6q7lCac5GilQacCNCoNPbx71MlDN8pGDqUdqjjcok1TSgEXj1RqxSd+Cff/qZQINldc/MG7szmOnppbjK9LRR56eUHv7NGZOYPmcCD1APP3z6/08JvPb6aPf5/G66atJS1b0JO208u3V50nozUEIS4hNfbQs4uz0eq0NfOkuVzpkpyXXOGHN70wnV8SfmFCxwl42HtUfTBZho1PQn4aTFqMxsGNaT2nEZ0SrXi8Ji89u2m99IOpB80CVBOi/KI4nnm80sqUS/mXePjvhykxlDCtx7Rqj1e2wai5kVaQxubzm7k54Brsv5kAlw7DxUPw50uweAS8FwLf3ARb3lU6IksuX5NI30iOZx6vcaWGiYTsBPJ0eeUF3aCH/z5Xwiux38Ogx+Hx/YpHXlF1SXOj3VA4t4v8gkLyivXmwVxl8Xe1x8Veba50ARjbfixTu03lu2PfKVVhV5CQk8DZ3LPNJtwCLVjQU/JTOJF1wuK3OrWZuljTkkUTQU7KKunJP02D99vBid/LPb/y2Er0Rj13d7+7+oPFrIBjG2DEKxDYC4BJnSbhYefBpwc/pdhQbBEv/UDqAXp696xxQinKNwqDbCA29eq4d1x6HFN+ncL5vPN8OuJTOnpU3Y0L0MmjE/Y29hxOP1xr2xubn079hF7Wc9u+VUrj1/S/4alD8MwxmLwUIu+CwkzY+j9YPgE+jVS8aBRB18t64tLjanVOUz6hXAhs4xPwx4tKw88ju2HUXLCvutqmWdFuKOjyyTm9B8A8mKsskiTRxc+Fk5fKT6p8ts+zRPpGMmf3HE5lnSr3nGnR7OqaBJuSFivoOy/sBBqmXLE+hLiGUGIsqVHpYo0F3aCD2FUEr30YgKTibKW77vs7YcdHIMsU6ApYdWIV17a51lxtUymZCUq7ddshMGim+WEHtQNPRz1NdEo00/+cTmZxFgx7scm89CJ9EUczjlZZf34lEb4R2Eg2RKeWD7v8efZP7vvjPmxtbPn2xm9rPOdcrVLTw7tHo3voeSV5LD+ynCc2P1Gjpdv0Rj1rjq1gYFEJoXYecP8fSuIRwDUQek6CMe8rDWEvnoPbv1NCan+/Blzu8KxtwjcmLQYPOw9zwl95L/wA/WfAXWvBp3OtjtcsaKs4fYbTWwEqTIqCMtPlZGpeuXyKRqXhw2Ef4qRx4ul/ny433G7z+c309OrZrKZMtlhB35G8A19HXzp7NOAbLD8djvys1NPmJFUaoyxLqIsiplUO6SrWQsK/JJ7+C4BgyVapErgSXeHloUXrHsJTBgeVhqSoKfDAJugxETbNgZ8eYn3pbO/7et5XtYHFefDTQ6CygYlfKP+WYWKnicwfPp/jmceZ8usUEnw6QGDvJvHS49Lj0Bv1NYqfm3DSONHVs6s5ji7LMl/EfsGsrbPo7tWdFWNW0MmjdtPuwn3COZZ5jBJDw7/ec7nneHfPu1z343XM2z+P7Unbmbl5ZrXDn7b9N59LxVncjitM+wPcq+hbsHeDbuOVL+vYlXBuN252brR3a29OjD60fD+v/hyH0Vh18jc2LZZwn/DL1VJ7vlTCcYOeuNym39Jw9AT/MOyTFCewoqQoQGdfZ7ILdKSVNh+Z8HH04cNhH5Kcl8xLO17CKBtJyU/hcPrhZhVugRba+q8z6th1YRc3tL2h4SabXTgI30+BvDLrW0s2ijfkFnzFT5vSn+ByY3TNrevaNGXexPn/4PwuJe4pG0j09sTXwR77j8PAxhZcg5SON7c2YOcKcWuUGHdwP7jxfaROownaeAtJ+ReU7rrJS8C3O4Ytc1met5dwz66VN3/kpyst13sXKWVbk5cqtlfA9aHX4+fox8zNM7nr97v5KHIq/X99SfHSo6quEqkPJrGp7US/KL8o8yjguf/N5bczvzG+/XjmDJqDrU3tS+UivCNYalzKscxjDTKLQ5Zldl/czYpjK9ietB0blQ03tr2Rqd2nklWUxWP/PMYL217g42s/rrh89dBqVh9ajK+9I8Om/grOvjU78dBZcPhH+PVZeHgbvXx78c/5f7iQXcBfR5U7SJ3ByDsTw1Cprv7c5BTncCbnDDd1uEl5oCgXor9RHAm3oLpejuZBu2F47FmEHSUVJkVB8dABTqTklZvGCMosoVl9Z/He3vf4+vDX5gYvIegNQExqDPm6/IaLn8ethZ8fAydvuHsdyMbL095ykpS5EYl74cg6MJb32n3t3LALcOPc3oUQ9zdcOKDMRgFQ20NQH7jmaQgZSOLRz2lj0EPYM+WnyiX8C9oUpSHjmmcgdJDZGwp2LlO6KEkw7Dm2qPUkxa/k6XNxSBcOKHFvE9mJsOszZcqcvhC6jlOOGVz1fJpwn3BWjl3JY5seY8axr3gtuDsTt38AvaZANYPK6sqB1AN0dO9Y6wqBKL8olh9dzq0bbyVZm8yTvZ9kes/pdf5yD/MpbTBKO0QEDsr7oc/94OJX62P9fuZ3Fh1aRHx2PJ72nsyImMFtXW7D28HbvM2L/V7knT3v8GH0h1ePadj3FYl/vsDONoE82vN+1DUVcwBbJ7jhPVg1FfZ+SS+fXvx06ic2HIkBYFx4AD/sS0RtI/HWhJ5XXa+rBnIdWA4leTCwWawwWT/aDUW9ewEDNPG4OlQse6aZLicu5TGkk89Vz0/pOoVDaYf47OBnBDoHEuoaSnu3qpvhmpoWKejbk7ejVqnpH1DPDjSjEf59R2mMaDNAiUM6X/2HvLy9AbTlR3yqchJpk76F87pcZRqcfxj0vkeZ+BYQUW6KYeLBt5WYf7+r12gsOxGuLMEuwey5tAdZls0fwGU5Rwhy8GVkZiosHaN0e/r1hJ2fKMOIAMJvh8FPgk+XGl+OIOcgvh3zLc/++yyvXdzNeSmHmTErUTWCl24wKonN0e1GV7/xFZhCNJlFmXw0/COuC72uXrb4Ovrib+fJoehFcHomIEPqEeX9UAv+OfcPz297ns4enZk7eC43truxwjuGO7veydmcs3x79FvaurblNv/Byh3d6c0Qs4LVHSKxkbO5pVsNSlGvpOtYZZzDlnfode9PAGw5uxdPpx58ekckQe4OfLktAVsbG14d162cqMemxWIj2Sjr2Br0yh1e6DXNuyyxpoQMxICKkXYnKv3i93K2w9vZ1ly6eCWSJPH6wNc5lX2KU1mnmNZj2tXHkmXlzvb4r1Xb0228UiXUwLRMQU/aTm/f3pU3PtSEYi2sexiO/6JUC4ydX/0IWZUNuAYoP236mh8O3fKUslDC/esr3bVAV0B6YXrlCdFK3mTBLsEU6gvJKs7C096TmNQYYtNiebHfi9iMGQWr74G1pQOb1A7Q9wEY+LgSyqkDLrYuLLxuIe/89w5fnVrD+QMf8HbPSdjXsmHk2MVcPv/3NN0DXZkx7Op543+f/5s8XV6t4ucm3O3dmT98PqGuofXLocgynNkK2+cTrk3kkL0DDHlWuQvb+bEyi6TT9TU6VEJOAi/vfJkw7zCW3bCs8tBP6SjY51Q+JNq4887utwhOSWVQYRHYOlMcdS8/5x9khP8IfB1r4Z2bkCS48X+wcABtd36Ou507JzIPM7jDUFQqiRdv7EqJwciSnWewVat44YYuZlGKTY2ls0dnZexy3FrFabnx/drb0ByxdyVB05l+VD23p7OfCydSKl+T1VHjyMfDP+a1Xa9dPfCtIFOpCDq2Ucl52FVRCVTQOOv7tjhBP5p6jvjseB4Nf6ruB8k6Bz9MgdSjyi1q/xkgSRiMMhn5FTceVEWISwjbE7eiPRCNU69IpApqc01DtmpasmjCtGB0Ul4SnvaefHPkG1xtXZnYcSJoHOGe9bDlbSW80+8hcKr56ILK0Kg0vDbwNUJLiph/diNH197IS0PfZUhw9SGukyl5fLLpFL8evohKgo2HLhAR7M7ADpftikmN4ZUdrxDuHc71oTUTzCup636AIqrHf1Eqhi4cAGc/wruO4K+MPaQPegRvjaviYf32HDz6H2iqfj9oS7Q8teUp7GzsmD98fuVifuwX5QNfkIEamOfsy92+7jwbGMx3/d+kQ4fR/HX2d7J3bKnx3JYK8WwPQ55F+vcduoSPZrc6gWs6KiEfSZJ4bVx3SvRGvth6Glu1imeu74zBaOBw+mElfi7LsGsBeHaAzjfU3Y5mxl56codunVIoUImD0tnPhdX7zyP//BhSfhoMfgJCB5dzuEJcQ1h2w7LyO57eDD8/quSurn8TBs60SI1+i6tyWRWn1H5+/Zctfx2pvvzrKs7tgsXXKrHmqT/CgEfMf6yXfjrMNf/bwvFLubU6ZIhrCCP2lZA45S7Sv/iiwm1qW4NuouwY3cTcRP45/w+3dbnt8uIValu4/g24dnaDiLkJSZK4b9g7fGX0Ql2YxaP/PMrTW56utOQuPlXLE98fZPTH2/j3RCozR3Rk54sjCPV0ZNaPseQVKe3TZ3LO8Pjmx/Fz9OOzkZ/VeBBaObLOKk0uG2aCvrjazcuRexGWjILVd0NhFoz7GJ48REQ/ZXWnQ2mHlGs69gPIOqN46lVglI28svMVzuee54NhH1RewhbzvXI35R4CE/4PZh7A6dmTLJy0ATtbFx47/BkZJTmsOrGKtq5t6e9fz3Di4CfBox2RF46jskunZ5vLvpskKTH02/oE8+k/p1iw+RTx2fEU6AuUBPX5/5QvuoGPtozGoRqyubgLNhiU11cJXfxdGK3fihTznbK60rKx8PUopQekwsq0ImVd1G8nKh75g/8o195C163F/bVev/Ze3u23FC+7EB76NprHVx4gXVvDD3XKUaUBw8FDufAdL8deD5zPYtX+REr0Rp76IYZivaHGNoXaBXDzbiOyjQ3pCxaS/9/VbxjTkK3aCnqgc2lzkTaZ5UeXY6OyYUrXKbU6Rp2RJPqNms9PiUk86dqTHck7uOnnm1gStwSdoVSg0/N5ZlUMoz7ayqZjKcwY1oHtL4zg2VFdCHBzYP7tvbiYU8ibG4+SXpjOI5sewUay4YvrvqhyzG+l5F5QuiNzLypJu2/GK3mNmpB8QPkyTzkKN38BM6OVedoae7p6dkWtUl+uR28/HHpMgu3zlTr+SlgSt4R/zv/DM1HP0Ne/b8Ub7V0MP8+AttfAvb8oq+p4dQBJIsA5gM9GfEZ6YTr3/3k/sWmx3NbltvpXb2nsYcwHDMhTvoDTdSfKPa1SSbw7KZyJkUF88NdJFuz6GyhNiO5eoHxGIprofdYEaIv17CzpiEHSKGG2SujuUsjrmuVke/VWBo2N+QC0l+D7O+DzQRC7SukTAeV9tHgE/LcQ+j4ID/2r5M0sSIsTdJWkYly3Pmx8fAjPXt+Zv46kcP38rfx8MLnqAUuyrCxCa+uszJ/wvlyrbDDKvLY+Dj9XOxZMieT4pTzm/3Wyxjb5bjmCpxbOPTcZ27ZtSZ71HLrU8iJzOvs0LrYuta7ocNQ44mXvxZGMI6w/vZ6x7cbi41hF4rahCY5CE3YbD8T9w88j/o/+Af35KPojbt14K/O2/sZ187fyW9xFHhjSnm3PX8sLN3TF0+lyyKF3iAePDu/IjwdPc9cvD5JZlMnCkQtp41qHGH9+Biy/Gbkgg2WdPyO630dKSeiia+FiNQ00h9fA0hsxqjTMD13AvJTIcjX59mp7unp0LT9Kd/TbSoXPb89XuDjxruRdfHrgU25sd2PF3bqyrKzU89ss6DKWkqEfkbrgS7LXrKHk7Fnz+zXMJ4x3rnmHhJwE7G3sL5cN1hNDh5FcKgmnXYqRvAX/h3b79nLP26gk5k0OZ2x4AH+f3ourxoOg4iIl3NRnesMsRNFMSMktogg7Mj17lZuPfiXdY97CgRJ+bf+SUjXU70GYeRAmLVbu5Nc9pPSJ/PYcLBqurIM65Ufljq4ZXK8WJ+gmbNUqZo7sxK9PXENbbyeeWhXD9G/2cyG7ktkVh1bBuZ1w3RylPLEMP+w7T1xyLi+P7c648ECm9g9h0fYE/kvIqNYOuaSEkm9+ID5IxeHuDgR/8jFGrZYLz85C1uvJKsrixe0vsi5+HX39KvHgqiHIJYh/zv9Dob6w2umBtaHo5ElSP/yQ1I8+RjZUcUdy3esgqQja9TmfjfiMz0Z8RnZRPsvPvkBgp7V89WAAL43phrdzxUnlR65ti0/7VSTnJ/Bav3fp6d2zDsbmwHeTIPsc67vPZ060Pbds8+OT0E+VsQtLblCawq7EaFRWzFk7nXzvcG7WvcWncXYs3HKanfHp5TYN9wk3NzsBSg/CtS9B/N9KzL0MSXlJPLftOTp6dGTOwDkVVztseh02vwVht1HcZw7n7r2fjMVfcfGVVzl9w42cGjqUpKeeJvPb7xhaFMK7g97mlQGvNMigJ92lSxz9aCGaX3X8b4mRThviSHzwIdK/+LKc46O2UfHhrRHYOiciF7VF2vul8iVWUSVWC8a09Fxh0GDFCagoKXl0PZoTG1mquZ19eWU0wkatLNQxY6cy593ZX+nv6DBCGYXQeVQTvYrqaXFJ0Svp5OfCmhmDWLbrLB/8eYJRH23jrZt7MDGyTBNNYTb89QoE94XI8p5UVn4J8/48Qf92nowPV9bufHlsN3bGp/Ps6lh+f2oIrvaV12HnbNiA/uJFdt7fhvy8JOz6dcL/9de5OHs2u996ihe7KSN+Hw5/mIfCH6rTawx2DuZQ2iEGBw6udRfklehSUsj95VdyNm6k+PhxJdZnNKJLPE/g//6HpKngtboFK12I296H/jPo4NSP7FNP4uj9L4UuW3lky92EeYdxa+dbuaHdDeVWB5JlmXf2vkmR5ii6lMms3+3G2I4yUkGGUtffbmj1pZUlBbDydkiJ4+z1XzFrox1jwvzo4OPMx1vi2e75Dss9P8Pxx3sh9UVl3UqVqrRL9mE48Sungm/h5jMTcXFyZMUDEby87jAvrTvMn08NxV6jeOrhPuGsPL6S09mn6eJZalO/h+HgCvj9ReUDbOtEob6Qp/99GhmZT4Z/ctVi3BgNSnNP9FLocz9FHR7i/H33A9Bu/c9IajUF+/ZTEB1Nwf795P3xBwCdXFyw79aNC4H/oQkMRBMYUPpvIOqAAFR2VVdhGbRa8v78i5yNGynYswe1LKP1CCVmfCBLgi+xJLY7aR9/TMmZBPzfeguVrXInlV50EVmdTl5KBIYL32ITdqsypdCKMHV/qjoMhUMfK85dtzKLwRRkwq+zICCCfZqpXKio0kWlUua8d74BCjLA0avZdc+2eEEH5dZx+jXtuL6bH7PWxPLM6lhUksSEXqXdbZvnKn+Au9Zelaz44K8T5BXpeWNCD7OX5Wir5qPbezH5i93M2XCE+bf1qvC8sl5P+peLsO/Zk+I+QZzLVWKtRaMGcGRjAD1W/cOwBztw9/1f1au8zjQ/va7euUGrJe+vv8nZuIGC//aALGMfHo7fK6/geuMN5Pz8M6nzPsBYXELQR/PNH/RyDH4SDizH8PuLPKB9HaNBw/eT5+DtZmTj6Y2sPrGa13a9xrz985jQYQK3dr6V9u7tWRCzgA2nN/BIxCNocm/gvd/iiFnzN5Hxn0NxDiAptdOVNT/pS5QEZuIeiics5v5NHng7G3hnYhjujrYMbO/Fk6ti6Jv8FBtCfqTD1veU6qXhs2HtdOS0E6zxmclz8QMY1tmXj27vhaeTLe9MDGPKV3v49J9TPH+DstqVafJibFrsZUG3UcPYD2HpDbD1feTr5vDm7jc5kXmi4tCRQQc/P6J0bA5+ikL/ySTeNw3J3p6QpUuxa68sumzXoQMedygzznXJyYq479tPcXw8+bt3o09NvSrMY+PlhWRXeSesISMTubgYTUgI3o8+yhtFbYjXeDDrpgIubX+OzI7/EmgXTvr6DaScOszfM3qzu/AI8dlKI9zNxnRs9IUw4NHq3lIWp+jYMTIWL0Y2ygR9+AGSTdULxpg8dNeOA5TqsDPbygv6ny8pg87u/omOB9RsP30WvcGI2qaCIIYkXXWX31ywCkE3EeLlyPL7+zFt6T6eWR2Lk62a69wvwv6vlaTFFQmLuOQcVu49z32D2tLVv3zNaGSIB49d25FP/znFdd38GBMWcNX5cn75BV1iIn6zXyTELZatydv4/vj3fHLgE2yGGvg02Ze7fsyg3ZT6jfed2HEi7nbul0cL1ALt9h0kPfkkckEBmjZt8H7kEVzHj8Ou3eXV3L2mT0eytyflrbkkPfIowQs+Q+VwxRqcds4YR7yCzYbH6ab/i1fufYr2PkofwNRuU5nSdQrRKdGsPrGaH078wHfHvqObZzeOZR5jUqdJPBLxCHLCNkY7v0rIkbMUhQzF/rpXIH4T7P1SCWm0G6oIe/vhyofGoFdq7OM3wU2f8caZLpzJOM+KB/rj7qgI26CO3vz+5BCeXR3LyJO380FQILccX4R0bAMGWzeet3uNdUmdeG50Fx4Z1sHc8j6oozeTo4JZtC2B8RGBdAtwJdg5GA87Dw6lHSpfNhg6EDn8TmIPfMlKOZXfL+7gsV6PlS/j1JcooZn/PleWPBv5GgVO15I47X5s3NwIWbYU2zYV5w00QUG4BQXhdtPl2LlcUoIuNRVd8gV0Fy6gu3gB/cVLyPrK5wvZuLnhesNo7CMiKNYb+euNv7h7gDe9ApR69nfb9SDX7yLBbioe++UMA147S84jUYyJfIJ+vr3puPR2tht6YlcYSL9q31kNg6zTgY1NhaW+FVFw8CAZX3yJdutWJHt75KIiMrp1w/vhqu9+U3KLcdDY4OLoCCEDysfRT/6lNAMNfQ78w+jsl0SJwcjZjAI6+taj18UCWJWgA9hrbFh8bx+mLv6Px1buZ7/f/3Bx9IYRLwNgzM9X3giSitfWx+HlZMtT11XsPc8c0ZGtJ1J5ad1hokI9yk1pkw0GMr74EruuXXG+9lpCTmWhM+p4Z887DAgYwOsDX8dnmI6zt0wm+ZlnCV3+TcXhjBoQ5BTIlDY317ryoeDAQZKeeALb0FD8X38Nh169Kj2G59SpqOztufjKqyQ+9DDBn3+OjbNTuW3mpUQx1tiWuc5rcG77SrnnJEmij38f+vj3IaMwg3Xx69h4ZA3X+Q/nla73If14H9LRnwl0bcPM7Fmk6UayMrgfqpD+Sq1v9DKl9vnbm5XOxGuehpN/KiN/R7/LX3ajWLknmoeHtmdQh/LekbezHUvv68tXOxJ48Q8Vu528meGyg0dTJ5DjEML3D0bSv/3VJZ0vj+nGluOpzP7pMGsfGYSNSlJWMCqTGM0uymZjwkZ+kpKJ9/fBO2EHD3a+XQmfGY3KrJ5Dq+HoeijKVm7Dx31Mvr4bidMfQOPnR8jSJWgCrnYIqkKytcU2OBjb4JqtbnUl0eeyKNEbGdzRC28Hb3p69SRJm0TvNtcQ5ZCIuuQg/v96cM/Hhwn6+AGc5VNQnMZqzcNk/XOK7x5omHVAZYOBkoQESpKT0V+8qHw5mb+kLqJPTUXl6opj79449umDY58o7Lt3L/dZkWWZgv/+I/2LLynYswcbd3d8nnwCj6lTufj666R99hnOQ67BvnvlS/Sl5CorFUmSpDgOm+Yo1VFqe/jlKfDpqgg65Re7uFLQixMSyFi0mNw//sDpmsF4PzwDh7A65IQaCasTdABnOzXLpvVj2Wev45IRy/lhHxFi70bOxl+48NJLqOzsyGnfjY56b6bfcj0uNhVXx2hsVMy/vRdjP93Oc2sO8c20vmZBzP39D0rOniXok0+QJIn+Af3p6dWT27vezoQOE5TtXCBg7lskP/MsqR99jN/zz9X6tRTGHeHSm29SfPw4AW/PxW181YtAmyg6cZLEGTNQ+/oQ8tVi1N7V3yK633ILkp09F154gcTp02mzeBE2rsqdy88Hk/l86xk8uj9Lz4SZSmnb0Ipfj1sB3PRXLkNWpmMXUIS630BQyTD8JdSDn2BITDrPrz3Ekp1neGBIe6XJY9BMpTEq9gel9nv1PcrBhs8mtcf9vPjJdnoEuvLMqIq/fFUqiYeGdqBfOy9mrrQj5qgbfQPtePJaV9xyzlNw8OpFSOyAd7rAR5tiWPdtDmPCAxic5U3K8X/ZnfcB8cd2kXXuFJ45Bp7Mt8M7C2yK9MAKTnn9jIt3Po7uWTgG2KDpcyNSxO3QfjjaHbtImvkwtqEhhCxZgtqnCauSStkZn45aJdGvnfJFtnLsSmTky+u2DtyOzm8WiesySJwxA99Bdrh27kx4vwm8/ecpos9lEhVah7LSUopOnCBnwwZyf/kVfUqZ0dIaDRp/fzSBgTgNHIgmMABdSgqF+6PRbtkCgOTggEOvCBz79EETFETW999TFHsItY8Pvi+8gMdtt6JyUpyNgNdfp3B/NMnPP0+7NWtQ2Vfc15CaV3x54Fa70tHKZ7YpsfS8i3DbcnOneEdfZyRJmeliujMvOnqU9C8XkffXX0h2driMGIF2507ObroVp8GD8Xr4IRz79q1/uWk9kSy1lmKfPn3k/furX1W7zuRnYPwsitiSQO6XX+eHNmkY572DQ+/eqNq358Rf2wnOUWp0JVtbHMLDcejbB+ehQ3GMLD+74tvdZ3l1/RHemtCDuwe2RTYaSbjpJiRJot369dXeLl568y2yVq4k8MMPcB0zpkZ/dEN2Nqkff0z2qtXYeHmhCQyk6NAhvB6Zgc/MmVWesyQxkXNTpoJKReiKFdgG125SXu7ff5P8zLPYdepIyNdfczgPbl/0H5Ft3Pl2en9s19wNp7fAEwfKJc90Fy+S8fUSsn/8EbmkGEd/KLgIPtcF4T13iXkErCzLPLg8mm2n0nhrQg8m9Q5GUzZWaTQonnlhNsbIe7nvm/3sScjg1yeuoaNv1eErWZY5/+JLFKz/uVavuTJKHG2xDQrEMagdB4o0dCzYTKCUiTbNDm26PaoS5fOj9vPDMSoKTWgIGV99jX2nTrT5+ivUHtWsJNVITFi4E41KYs0jgyrfyKDHuONzkt/+FO35Ut9OrSbV3o1CDx/Ce3dBE6QkZE3JWU1gYKXJWd2lS+T+8gs5GzZSfPIkqNU4DxmCy6hR2LYNVZK73t6Vxrt1qakUHjhgThgXnzgBsowmKAivBx/AbeLECs+t3b6DxAcfxPPee/CbPbvCYw+ft4WeQW4smNJbCeW93x482yrlrgMfV0pUyzDig39xtLNheT8HtEsWk791GypnZzymTsXz3ntQe3pi0OaT/cP3ZCxdhiEjA4fevfGe8TBOQ4ZU+Bk35uejK71LUfv7Y9+5bnk1SZKiZVnuU+FzVivo6x+H2O+5cOcmPnvzJ+7Ztwab/gPp+MVC5m46w9JdZ9hwV0/aXoo3v4GKjh4FgwGPKXfi+/zz5m97WZa5d+k+9p7J4PsHB9D+6B4uPPU0gR9+gNvYsdWaYiwp4dyUqRTFxWHbrh1uN43Hdfz4Cm+nZaOR7LVrSftwPoa8PDymTsFn5kxUdnZcfPNNctasxWX0aALfe/fqODegT0vj7NS7MObkELriO+w6Vr9qT0Vot20jaeYTSIFBfBQygjw3Lz55cgxegT5ImQmwsD9E3A4TFlJy7hzpixeT8/N6MBpw66DDq1MGtj37k7zbi7zdsbRbvarcLXG6tpjpy/YRm5RDG08HZl7biYm9g8oLO7B05xne2HjU/GVaFbIsk/r+PDKXLsXzvvtwGjy4Rq81La+I2esO0yPQlZkj2vPT6Z8JDO3BkKiJOLn7kJhZwCMroolLzmX2QAfuC0zmL30kS6KzyDpygsjss1xvuEjb5FNIGWk4RESUu7tpanIKdUS++RePj+jEM9dXLxpyXioF67+ixKYtuospHI05QXr8ObrbFCJlpF3VIWlyMDSBgWgCArDx9CR/1y4K9igJd4eICFxvGo/rmDH1+kIz5OZScubMVSGYirj01lyyVqwgZOkSnAaWX2NYlmW6v/YnU/qH8Oq40vfg93fCid+URa4f2QW2jsgGA/rUVHQXLrB3Vxwpq34kPP00Knd3vO67F48pUyr8mxqLishes5aMr79Gf/Ei9t274zJ6NPr0dHQXlfCSPvkChpwc8z6e06bh98LzVx2rJrQ+QU/cq6xKPugJMs6HkvrBh+wLCuOb6x/gjVt6c+/Svdzetw3vTAwrt5tBm0/6woVkLl2KXaeOBH7wIfZdlA9ESm4RN36ynSxtEV9u+wRXG5nMBd8wuIsfbg7Vx8aN+fnk/v47Oes3ULBvHwAOvXvjdtN4XEaPRu3hYQ6vFB06hENUFP6vvYp9l8slfbIsk7l0Ganz5mHfowfBCxei8bs8wMmQm8u5u++hJDGR0KVLcIioX9eadvd/xD80Azvd5U5cydFRKafTaNGUJGBoM5K8bXuRVODeoRCvztloIkeVjgzujyE7m4TxN6Fyc6Xd2rXlPCxZltl8PJWPN53icPLVwn78Ui43LdjJNR29+frePtXe2aR/uYi0jz7CY+pU/F55uVa3v4u2nead347z+dTe3FgmAb7leCpPrYrBKMt8dFsvrutefqTu4aQcvvvvHOtjkykqMTDUS+K+sZGM6BFY43M3NH8eucTD30az6qEBFeYOqiO/WM81/9tMrzbuLLkrEl1KKroLZWLgpjh46e9ycTGa0BDcxt+E2/hx2IZWs4JWI2AsLOTMpFswFhbSfsP6csKbW6QjfM5fvDSmKw8NVQbFlWycR86yT9D5j0KXq1deU0oKlEk66z29WBY8mNP9rufrGUPwqqTPwoRcUkLOxo1kLFpMyblzqBwdlTuc0i8+TWCQ+YvQtm0oas+6hbRal6Ab9LBoOHJBBuk2D5L+xWJcx4wh9fEXmbosmoISA+6OGrY8OxwPp4pLwLQ7dnLhxRcx5ubi+/zzeEydgiRJpOYWsXfFz3T45A0+6z+V3wIiUUnQq407Qzv7MKiDN17Otjja2uBoq8bR1uYqjxNAd+ECOb/8Ss6G9ZTEnwaNBoewMAoPHsTGywu/52bhWhrSqYi8zZtJnvUcNi4uBP/fQhx69MBYWMj56Q9QePgwbb74HOcaeqdV8e+JVB5dtJ33B3ow1FVv/jDrL15El5SILuE4slHCvWMBXl3yUfebpJQ3+pVf8PnyLfG9+M1+8arzyLLMlhOKsB8q9dgfG96RZbvOkq4t5o+nhlbatGQi64dVXJozB9dx4wh8/381rpowoTcYuWnBTtK1xfz9zDCc7dR8sukkn26Op3uAK1/cFUWIV+WdgDkFOtYcSOLb3Wc5m1HAqO5+vH5TD4Lcr76LamxeXx/H6v1JxL4+Clt13XoHF26JZ96fJ1j/2GAi2rhXup0syxhzc1G5ulo8flx4+DBn77gT1zFjCJp3eUpkfGoe183fxid39GJ8Vy8yFi0m46uvkPV61H5+5jsN811HkPK7bUgIW89k8/C30bTxdOS76f3xd6t+9pBsNGLMz0fl7Nwo16R1CfrOT5D/eo3U4jvI/HkbbpMmEfDWm0g2NuyMT2fGt9G8Or47t/WpuvVcn5HBhZdeIn/rNpyHDyfgnbex8fDg7C2TMWi1hGzcSOxFLdtOprHtZBqHknMq6g5HYyOZxb13iAfvTAzDzVHx6GVZpvjYMXI2bES7bRtO1wzGZ+ZMbFyqL3MsOn6cxEcexZCdTeDbc8n++Wfyd+wkaP58XG+o/Yzxinj42/3sP5vF7tkjKxaGg98h/zILqfdUJanp0bbSY5nyCCHLluI0oOLyyyuFHWDpfX25tmvVY2Rzf/+d5GeexWnoENosWFDnaqLDSTlMWLiDCb2CSNcWs/1UOrf1CebNCT3NzUfVUaI38vWOM3zyz0kkJJ4Y2Ynp17Srs7DWhZEf/kuwhyPf3F/34kNtqZfeJ9SDr+6tW4ezJUhbuJD0zxYQ9NF8XG+8EYBd8elM+WoPP4br8ViyAF1SEq7jxuH73HPl7nArY09CBtO/2Y+Hk4aVDwygjadlW/xbh6AbjbBlLvK2D0lJCCdrXzoeU+7E75VXynlrBqOMTQXLb1WELMtkfbeC1HnzULm54n7LLWR88SUBb8/F/ZZbym2bmV9CTGIWeUV6CkoMyk+xngKd8m9ukZ5fDl0gyN2BRff0obNf/WrTQYmXJz7+OEWxSpmd/5tv4HFbPcauliE1t4iB723mgSHtmH1jt8o3rGRhjisxFhZyZuIkjMXFtF//c5XxZVmW+fdEGrlFusvNYZWg3b6DxEcfxSE8nJCvFleYV6gNc385ylc7lFnhb03owe19q1jLswqSsgp4Y+NR/j6aQidfZ966uScD6hD+qC2XcooY8O4/5cILdeWzf07x4d8n+WXmNfQMqv84gqZA1us5O2UqJefO0X7DejR+fvzyx17S332P/inHsO3YAf9XX8Opf+2+7GITs7lnyV4cNDZ890B/i9anW6WgG7Tay/G882fQ/7sMXdJZivV+FF/Iw/P++/F9blaD3PIUnThB8rPPUhJ/Gk1gIB3+/KNOXuD+s5nM+O4AhSV65t/ei9E96t9ebSwqIvXD+eU6DxuCBZtP8cFfJ9kyazjtvJ2q36EGmG+Jx44h6P36L5xQGBPDuWn3YxsaSujybxokCVlQoufjTae4KSKwQURs09EUXt9whOTsQib1Dqpy5k1D8NOBJJ5ZHdsgIpxbpOOa9zYzoL0Xi+6pUD+aJcVnznBm4iQco6JwiIggddFiilAR+ORM/O+7p853cMcv5XLXV3uRZZnl0/vRI9AyX3L1FnRJkm4APgFsgK9kWX7viueHA+uBM6UP/STL8ptVHbOugp63aRMXXnoZY275meWSSkbt7Y6mfVdcRozE4+67GjR+ZSwsJOOrr3HsE3VVFr02XMop4uHvoolNzObJkZ14cmSnChfsrY6CEj2xiTkcOJ9FbGI2PQLdeGR4hwa5tTcaZYa8v4VQL0dWPlj77tSqSFuwkPQFCwj6+CNcb6j74glFJ09y7u57sHFzo+2K7yxS611TCksMfLb5FIu3J2CvsWH6Ne2YNqidOfTWkDy7OpbNx1OIfuX6Or2vruTjTSf5eNMpfntiCN0DLVO1Uxeyvv+eS28oEnQu8hrmBl/Hjnn1d3gS0rTc9dUe8or13NwriKhQD6JCPQj2cGiyHEK9BF2SJBvgJHA9kATsA+6UZflomW2GA7NkWR5XU6PqKuhFx4+TvfpHJXGhzkVz6DPUTgbUdy9B6jSy1sezBEU6A6/8HMea6CSu6+bHR7dH4FLFADBZlknKKuTA+SwOnMsi+nwWxy7mYTAqf7tgDweSsgrpHuDK/NsjrhpjUFu2nkzj3iV7+ezOSMZHNGy1hqzTcXbqXejOnaPdhg01imGa9zUaKTx4kJwNG8n99VdUDg6Efr+yzt2UTU18ah7/++MEfx9NwdlOzT0DQ5l+TbtqqydqiizLDHx3M1GhHiycWvul/Soip1Dx0nsEufLlXX0a5UuoMZBlmazly7Hr2o3n49Ucu5jL5lnDG+TYSVkFvPpzHHvPZJJfokwp9XWxM4t7VKgHPQLdGi1vUl9BHwjMkWV5dOnvswFkWX63zDbDaSJBN7N/qTJn2qMdTFmlLBjQgpBlmW92neWtX4/RztuJRXdH0d7HmdwiHScv5XH8Uh4nSn+OX8olt0gpp3K0taFXG3d6hyhvnMgQd9wdbfn7aAqzfzpEbqGeZ0Z15sEh7WucK7iSapOh9cR8S9ynD0EfzEPl5lald1OckKB0HW78BV1yMpKDAy7XXYf3I4+Yh121JI5dzGXhlnh+PXwRe7UNU/qH8PDQ9pc7GUvRG4ycTNFyKCmb2KQcTqdpGR8ewNT+oRV636fTtIz8cCtvT+zJ1P4NVzr4/d7zvPKzMibj3UlhjOzmV/1OzYhbv9iFSpJY9XDd76wrwmCUOXEpj+hzmUSXOlqJmcr4blu1im4BrkQEuxEW5EZEG3c6+DjX+TNZlvoK+mTgBlmWHyj9/W6gvyzLj5fZZjiwFsWDv4Ai7lWuxlpnQTfo4c/ZyjzijtfB5CVg3zISNhWx+3QGj608QIneiKu9mgs5RebnXOzUdPZ3oYu/C90CXOkd4k4XP5eKJ8ABGdpiXl4Xxx9HLtEn1IMPb4sg1Kt28W9zMvSadsweU0UytJ6UvSU21+uaS8eC0AQEYMjKJGfDRori4kClwmnQIKVuf+RIc+t3SyY+Vcv//RvP+pgL2Kgkbu/Tht6h7hxKyuFQUg5HLuRQpFOaelzt1fi62hOfqiUyxJ13J4VddSdm6mje+tzwWv/dqyMuOYdZP8Zy/FIek3oH8fq4Hi3GWx/6/hZ6tXHn0zsjq9+4nqTmFhF9LouDidkcSsomLjkXbbHijDnZ2tAjyI2IYDeu6+ZXpx4BqL+g3wqMvkLQ+8myPLPMNq6AUZZlrSRJY4BPZFm+anC3JEkPAQ8BhISERJ07d672ryb6G2Wh3YGPK4uxqmpWTtacScoq4J3fjqGxUdHF34Wu/i508Xcl0M2+1nE5WZb5OSaZ19YfwWCUeXlsN6b0C6nxcUz1x5ufHWaeptgYyLJM/s5dFJ86dbmb7oLSqGLIzjZvZ9+9O643jcdt7NhmHSevD+czCvh8azxropPQGWTsNSp6BLoRHuxGRLA7EW3cCfV0RJJgfcwF3vrlKDmFOh4Y0p4nR3bCwVb5DDz87X7iknPZ8cK1jRLPLdEbWbD5FAv/Pd1ivHVZlun66h/cMzCUl8dWPryrsTAaZRLStcQm5pjvtI5ezOWRYR14ugZdvBXR6CGXCvY5C/SRZTm9sm3q7KEbDZCwpdx6oIKruZBdyPNrDrEjPp1hnX349M7IajtajUaZofO20MbDke8fathkaG0w5ueju3QJSa22SNehpUjNLSIjv4ROvs6V3oWBsijLu78fY/X+JEI8HZl7c08Gd/Qm8s2/uKGnP+9Pbtx1LVuSt55ToCPizb94ZWw3ZRBcM0BnMFKsN+JsV7fZiFUJek0CpPuATpIktZMkyRa4A9hwxQn8pVKXQJKkfqXHrX79trqgshFiXgMC3R1Yfn8/3pzQg12n03nku2hK9BWsWl6G7fHpJGUVMqV/3WqvGwqVkxN2HTq0KjEH8HW1p1uAa5ViDuDhZMv7kyP44aEBqG0k7lmyl3uW7CG3SM/gjo2/8ELPIDc2PH4NT4zoyPqYC1z/0Va2HK/hQt1NTEqeEsK8Mj9hSTQ2qjqLeXVUK+iyLOuBx4E/gWPAalmWj0iSNEOSpBmlm00G4iRJigU+Be6QLVXgLjCjUkncM7At700KZ9fpDF75+XCVC2l/v+c8nk62jOrRvG+jBQoD2nvx+5NDeHJkJ/adyQK4alZ8Y2GrVvHMqC6sf2wwHo62TFu2j5fXHaagpPIFOCxBaq4yh8jXpfFq/5sTNfqakGX5N+C3Kx77osz/FwALGtY0QUNxS1Qw5zIL+PSfU4R6OfHYtVdPYEzNLWLTsRTuv6YdduqWn5doLdipbXj6+s7c1CuQ85kF+DSxcPUMcmP944P58K8TfLXjDLtOZ/DR7b3oVcX8l6bEtPScXzPy0BuTphswIbAoT1/XiQm9Apn35wk2xl646vkfo5PQG2Xu6Fv1jBtB86SDjzPXdql5TX9DYq+x4eWx3VnxQH+KdQZu+XwXH286id5QdYivKTCHXFqJhy4EvZUgSRLvTw6nb1sPnv0xluhzmebnjEaZ7/eeZ2B7r0atbBFYN4M6ePP7U0MZHx7Ax5tOccsXuzmTnm9Rm1Jzi3GxU+PUSDHr5oYQ9FaEndqGRXf3IcjdgQeXR3MuQ/mw7ShNht5p4WSooOXj5qDh4zsi+ezOSM6m5zPmk+18v/fq5f+aitS8InxdW4d3DkLQWx0eTrYsva8vsiwzbdk+sgtKWLnnPB6OGkaLZKiggRgfEcifTw0lKtSD2T8dZvX+RIvYkZJbjK9L64ifgxD0VklbbycW3dOHpMxCpi3bx6ZjKUyOChbJUEGD4u9mz7JpfRnSyZuX1x1m75nM6ndqYFJyi/ATHrrA2unb1pN5t4Zz8Hw2eqPMnf1EuEXQ8KhtVCy4szdtPByZ8V00iZkFTXZuWZZJzS1uNRUuIAS9VTOhVxBzb+7Jo8M7iGSooNFwc9Tw1b190BuMPPDNfvNsk8Ymu0BHicHYrJqKGhsh6K2cuwaE8vwNXS1thsDKae/jzP9NjSI+TcuT3x80j35uTFLzWldTEQhBFwgETcQ1nbx5fXx3/jmeyvt/HG/087W2piKoYaeoQCAQNAT3DGzLyZQ8vtyWQEdfZ26tZrH2+nBZ0IWHLhAIBI3C6+N7MLijFy+vi2P/2carfLkccmk9HroQdIFA0KRobFQsnNKbQHd7Hv42mpMpebXa/3SaloVb4rlpwQ7GfrqdP+IuVTh0LiW3CFd7tXlefGtAhFwEAkGT4+5oy1f39mXi/+1k1Efb8He1JyrUg96hHvQOcS+3Jqcsyxy9mMufcZf4Pe4Sp1K1APRq406+Ts+M76LpHeLO7DHd6NvW03yO1NziVlXhAkLQBQKBhejo68xvTwxh8/FUZU3Oc1n8evgiAHZqFeHBbrT3dmZXQjqJmYWoJOjfzou7BoQyqocfAW4O6A1G1kQn8dGmk9z6xW6u6+bHCzd0oZOfCyl5raupCGqwYlFjUe9FogUCgdWRklvEgVJxP3A+i1OpWvqEenBjzwBGdvPFy7ligS4sMbBk5xm++Pc0+SV6bo1qwz/HUxnayZv5t/dq2hfRyFS1YpHw0AUCQbPBz9WeG8MCuDEsoFb7Odja8Ni1HbmzXwgLNsfz7X9n0RlkfFqZhy4EXSAQWA2eTra8Nr470wa35bs955jcO9jSJjUpQtAFAoHV0cbTkdk3drO0GU2OKFsUCAQCK0EIukAgEFgJQtAFAoHAShCCLhAIBFaCEHSBQCCwEoSgCwQCgZUgBF0gEAisBCHoAoFAYCVYbJaLJElpwLk67u4NpDegOQ1Fc7ULmq9twq7aIeyqHdZoV6gsyz4VPWExQa8PkiTtr2w4jSVprnZB87VN2FU7hF21o7XZJUIuAoFAYCUIQRcIBAIroaUK+iJLG1AJzdUuaL62Cbtqh7CrdrQqu1pkDF0gEAgEV9NSPXSBQCAQXIEQdIFAILASWpygS5J0gyRJJyRJipck6UVL22NCkqSzkiQdliQpRpIkiy2WKknSEkmSUiVJiivzmKckSX9LknSq9F+PZmLXHEmSkkuvWYwkSWMsYFcbSZK2SJJ0TJKkI5IkPVn6uEWvWRV2WfSaSZJkL0nSXkmSYkvteqP0cUtfr8rssvh7rNQOG0mSDkqS9Evp741yvVpUDF2SJBvgJHA9kATsA+6UZfmoRQ1DEXSgjyzLFm1ikCRpKKAFlsuy3LP0sfeBTFmW3yv9EvSQZfmFZmDXHEAry/IHTWnLFXYFAAGyLB+QJMkFiAZuBu7DgtesCrtuw4LXTJIkCXCSZVkrSZIG2AE8CUzCsterMrtuwMLvsVL7ngH6AK6yLI9rrM9kS/PQ+wHxsiwnyLJcAvwATLCwTc0KWZa3AZlXPDwB+Kb0/9+gCEOTUoldFkeW5YuyLB8o/X8ecAwIwsLXrAq7LIqsoC39VVP6I2P561WZXRZHkqRgYCzwVZmHG+V6tTRBDwISy/yeRDN4k5ciA39JkhQtSdJDljbmCvxkWb4IilAAvha2pyyPS5J0qDQk0+ShoLJIktQWiAT20Iyu2RV2gYWvWWn4IAZIBf6WZblZXK9K7ALLv8c+Bp4HjGUea5Tr1dIEXargsWbxLQwMlmW5N3Aj8FhpiEFQNZ8DHYBewEXgQ0sZIkmSM7AWeEqW5VxL2XElFdhl8Wsmy7JBluVeQDDQT5Kknk1tQ0VUYpdFr5ckSeOAVFmWo5vifC1N0JOANmV+DwYuWMiWcsiyfKH031RgHUp4qLmQUhqTNcVmUy1sDwCyLKeUfgiNwGIsdM1KY65rgRWyLP9U+rDFr1lFdjWXa1ZqSzbwL0qc2uLXqyK7msH1GgzcVJpj+wEYIUnSdzTS9Wppgr4P6CRJUjtJkmyBO4ANFrYJSZKcShNXSJLkBIwC4qreq0nZANxb+v97gfUWtMWM6Q1dykQscM1Kk2lfA8dkWZ5f5imLXrPK7LL0NZMkyUeSJPfS/zsA1wHHsfz1qtAuS18vWZZny7IcLMtyWxS92izL8l001vWSZblF/QBjUCpdTgMvW9qeUpvaA7GlP0csaRfwPcqtpQ7ljmY64AX8A5wq/dezmdj1LXAYOFT6Bg+wgF3XoITtDgExpT9jLH3NqrDLotcMCAcOlp4/Dnit9HFLX6/K7LL4e6yMjcOBXxrzerWoskWBQCAQVE5LC7kIBAKBoBKEoAsEAoGVIARdIBAIrAQh6AKBQGAlCEEXCAQCK0EIukAgEFgJQtAFAoHASvh/mLEXAHOAOKcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 17],\n",
       "       [28, 26]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test=test1.reshape(1,60,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(single_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
